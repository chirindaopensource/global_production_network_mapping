{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jMeEZmZ94db"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# A High-Resolution Digital Twin of the Global Production Network\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%23025596?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![NetworkX](https://img.shields.io/badge/NetworkX-blue.svg?style=flat&logo=networkx&logoColor=white)](https://networkx.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/Statsmodels-150458.svg?style=flat&logo=python-social-auth&logoColor=white)](https://www.statsmodels.org/stable/index.html)\n",
        "[![Scikit-learn](https://img.shields.io/badge/scikit--learn-%23F7931E.svg?style=flat&logo=scikit-learn&logoColor=white)](https://scikit-learn.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2508.12315-b31b1b.svg)](https://arxiv.org/abs/2508.12315)\n",
        "[![Research](https://img.shields.io/badge/Research-Supply%20Chain%20Economics-green)](https://github.com/chirindaopensource/global_production_network_mapping)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Network%20Science%20%26%20Econometrics-blue)](https://github.com/chirindaopensource/global_production_network_mapping)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Network%20Inference-orange)](https://github.com/chirindaopensource/global_production_network_mapping)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/global_production_network_mapping)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/global_production_network_mapping`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Deciphering the global production network from cross-border firm transactions\"** by:\n",
        "\n",
        "*   Neave O'Clery\n",
        "*   Ben Radcliffe-Brown\n",
        "*   Thomas Spencer\n",
        "*   Daniel Tarling-Hunter\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for transforming massive-scale, firm-level transaction data into a high-resolution, computable \"digital twin\" of the global production economy. It implements the paper's novel network inference algorithm, a full suite of network and econometric analyses, and a comprehensive set of validation and robustness checks. The goal is to provide a transparent, robust, and computationally efficient toolkit for researchers and policymakers to replicate, validate, and extend the paper's findings on global supply chain structures and economic diversification.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: main_analysis_orchestrator](#key-callable-main_analysis_orchestrator)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Deciphering the global production network from cross-border firm transactions.\" The core of this repository is the iPython Notebook `global_production_network_mapping_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final execution of a full suite of robustness checks.\n",
        "\n",
        "The study of global supply chains has been historically constrained by a lack of granular data. This project implements the paper's innovative approach, which leverages a massive dataset of 1 billion firm-to-firm transactions to infer a detailed, product-level production network.\n",
        "\n",
        "This codebase enables users to:\n",
        "-   Rigorously validate and cleanse massive-scale transaction and firm metadata.\n",
        "-   Implement the core network inference algorithm to build a weighted, directed product network.\n",
        "-   Analyze the network's structure using community detection and centrality measures.\n",
        "-   Perform multi-pronged validation of the inferred network against external benchmarks and statistical null models.\n",
        "-   Engineer network-based econometric features to predict national economic diversification.\n",
        "-   Execute the full Probit regression analysis with fixed effects.\n",
        "-   Conduct a comprehensive suite of robustness checks to test the stability of the findings.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in network science, econometrics, and economic complexity theory.\n",
        "\n",
        "**1. Network Inference from Revealed Preference:**\n",
        "The core of the methodology is to infer an input-output link from product `i` to product `j` not from direct input tables, but from the observed behavior of firms. The weight of a link, `A_ij`, is a measure of \"excess purchase\" or revealed preference. It is calculated as the ratio of the probability that a producer of `j` buys `i` to the baseline probability that any firm buys `i`.\n",
        "\n",
        "$$\n",
        "A_{i,j} = \\frac{|S_i^j|/|S_j|}{|S_i^\\dagger|/|S|}\n",
        "$$\n",
        "\n",
        "An `A_ij > 1` indicates that producers of `j` have a revealed preference for input `i`, suggesting a production linkage. This method effectively filters out ubiquitous inputs (like packaging) and highlights specific, technologically relevant connections.\n",
        "\n",
        "**2. Network Density and Economic Diversification:**\n",
        "The project implements the \"density\" metric, a concept from economic complexity that measures a country's existing capabilities relevant to a new product. The network-derived upstream and downstream densities measure the proportion of a target product's key suppliers or customers, respectively, that a country already has a comparative advantage in.\n",
        "\n",
        "$$\n",
        "d_{p,c} = \\frac{\\sum_{j \\in J_p} I(A_{p,j}) \\cdot M_{j,c}}{\\sum_{j \\in J_p} I(A_{p,j})}\n",
        "$$\n",
        "\n",
        "where `J_p` is the set of top-k downstream partners of product `p`, and `M_j,c` is an indicator of country `c`'s presence in product `j`.\n",
        "\n",
        "**3. Probit Model for Diversification:**\n",
        "The final analysis uses a Probit model to test the hypothesis that higher network density predicts the probability of a country developing a new export capability in a product. The model includes country and product fixed effects to control for unobserved heterogeneity.\n",
        "\n",
        "$$\n",
        "R_{p,c} = \\Phi(\\alpha + \\beta_d d_{p,c} + \\gamma_p + \\eta_c)\n",
        "$$\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`global_production_network_mapping_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Task-Based Architecture:** The entire pipeline is broken down into 11 distinct, modular tasks, from data validation to robustness checks.\n",
        "-   **High-Performance Data Engineering:** Utilizes vectorized `pandas` and `numpy` operations to efficiently process and transform large datasets.\n",
        "-   **Efficient Network Inference:** Implements the core `A_ij` formula and sparsification rules using performant, vectorized calculations.\n",
        "-   **State-of-the-Art Network Analysis:** Employs the `leidenalg` library for robust community detection and `networkx` for standard centrality measures.\n",
        "-   **Rigorous Statistical Validation:** Includes a parallelized Monte Carlo simulation framework for testing subgraph modularity against the configuration model.\n",
        "-   **Professional-Grade Econometrics:** Implements the Probit model with fixed effects using `statsmodels`, including correct calculation of Average Marginal Effects for interpretation.\n",
        "-   **Comprehensive Robustness Suite:** A full suite of advanced robustness checks to analyze the framework's sensitivity to parameters, temporal windows, and methodological choices.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Input Data Validation (Task 1):** Ingests and rigorously validates all raw data and configuration files.\n",
        "2.  **Data Preprocessing (Task 2):** Cleanses the transaction log and performs firm entity resolution.\n",
        "3.  **Firm Classification (Task 3):** Identifies significant producer and purchaser sets for each product.\n",
        "4.  **Network Inference (Task 4):** Computes the `A_ij` matrix and constructs the network objects.\n",
        "5.  **Structural Analysis (Task 5):** Performs community detection and topological validation.\n",
        "6.  **Centrality Calculation (Task 6):** Computes Betweenness and Hub Score centralities.\n",
        "7.  **Network Validation (Task 7):** Validates the network against external data and a statistical null model.\n",
        "8.  **Feature Engineering (Task 8):** Computes Rpop, the diversification outcome, and network density metrics.\n",
        "9.  **Econometric Analysis (Task 9):** Estimates the final Probit models.\n",
        "10. **Orchestration (Task 10):** Provides a master function to run the entire end-to-end pipeline.\n",
        "11. **Robustness Analysis (Task 11):** Provides a master function to run the full suite of robustness checks.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `global_production_network_mapping_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the 11 major tasks.\n",
        "\n",
        "## Key Callable: main_analysis_orchestrator\n",
        "\n",
        "The central function in this project is `main_analysis_orchestrator`. It orchestrates the entire analytical workflow, providing a single entry point for running the baseline study replication and the advanced robustness checks.\n",
        "\n",
        "```python\n",
        "def main_analysis_orchestrator(\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    firm_metadata_frame: pd.DataFrame,\n",
        "    # ... other data inputs\n",
        "    base_manifest: Dict[str, Any],\n",
        "    run_robustness_checks: bool = True,\n",
        "    # ... other robustness configurations\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Serves as the top-level entry point for the entire research project.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.8+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `networkx`, `statsmodels`, `scikit-learn`, `python-igraph`, `leidenalg`, `joblib`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/global_production_network_mapping.git\n",
        "    cd global_production_network_mapping\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy networkx statsmodels scikit-learn python-igraph leidenalg joblib\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires four `pandas` DataFrames and two Python dictionaries with specific structures, which are rigorously validated by the first task.\n",
        "1.  `transactions_log_frame`: Contains transaction-level data.\n",
        "2.  `firm_metadata_frame`: Contains firm-level metadata.\n",
        "3.  `comtrade_exports_frame`: Contains country-product level export data.\n",
        "4.  `country_data_frame`: Contains country-level population data.\n",
        "5.  `supply_chains_definitions_dict`: Defines product sets for validation.\n",
        "6.  `replication_manifest`: A comprehensive dictionary controlling all parameters.\n",
        "\n",
        "A fully specified example of all inputs is provided in the main notebook.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `global_production_network_mapping_draft.ipynb` notebook provides a complete, step-by-step guide. The core workflow is:\n",
        "\n",
        "1.  **Prepare Inputs:** Load your data `DataFrame`s and define your configuration dictionaries. A complete template is provided.\n",
        "2.  **Execute Pipeline:** Call the master orchestrator function.\n",
        "\n",
        "    ```python\n",
        "    # This single call runs the baseline analysis and all configured robustness checks.\n",
        "    final_results = main_analysis_orchestrator(\n",
        "        transactions_log_frame=transactions_df,\n",
        "        firm_metadata_frame=firms_df,\n",
        "        comtrade_exports_frame=comtrade_df,\n",
        "        country_data_frame=country_df,\n",
        "        supply_chains_definitions_dict=supply_chains,\n",
        "        base_manifest=replication_manifest,\n",
        "        run_robustness_checks=True,\n",
        "        parameter_grid=param_grid,\n",
        "        methods_to_test=methods_list\n",
        "    )\n",
        "    ```\n",
        "3.  **Inspect Outputs:** Programmatically access any result from the returned dictionary. For example, to view the temporal robustness results:\n",
        "    ```python\n",
        "    temporal_df = final_results['robustness_results']['temporal_robustness']\n",
        "    print(temporal_df.head())\n",
        "    ```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `main_analysis_orchestrator` function returns a single, comprehensive dictionary with two top-level keys:\n",
        "-   `baseline_results`: A dictionary containing all artifacts from the primary study replication (network objects, analysis DataFrames, econometric models, etc.).\n",
        "-   `robustness_results`: A dictionary containing the summary DataFrames from each of the executed robustness checks.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "global_production_network_mapping/\n",
        "│\n",
        "├── global_production_network_mapping_draft.ipynb  # Main implementation notebook\n",
        "├── requirements.txt                                 # Python package dependencies\n",
        "├── LICENSE                                          # MIT license file\n",
        "└── README.md                                        # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `replication_manifest` dictionary and the arguments to the `main_analysis_orchestrator`. Users can easily modify all relevant parameters for the baseline run or define custom scenarios for the robustness checks.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{oclery2025deciphering,\n",
        "  title={Deciphering the global production network from cross-border firm transactions},\n",
        "  author={O'Clery, Neave and Radcliffe-Brown, Ben and Spencer, Thomas and Tarling-Hunter, Daniel},\n",
        "  journal={arXiv preprint arXiv:2508.12315},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation of \"Deciphering the global production network from cross-border firm transactions\".\n",
        "GitHub repository: https://github.com/chirindaopensource/global_production_network_mapping\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to Neave O'Clery, Ben Radcliffe-Brown, Thomas Spencer, and Daniel Tarling-Hunter for their innovative and clearly articulated research.\n",
        "-   Thanks to the developers of the scientific Python ecosystem (`numpy`, `pandas`, `scipy`, `networkx`, `statsmodels`, etc.) for their powerful open-source tools.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `global_production_network_mapping_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "i8LjcF8hw4dV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Deciphering the global production network from cross-border firm transactions*\"\n",
        "\n",
        "Authors: Neave O'Clery, Ben Radcliffe-Brown, Thomas Spencer, Daniel Tarling-Hunter\n",
        "\n",
        "E-Journal Submission Date: 17 August 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2508.12315\n",
        "\n",
        "Abstract:\n",
        "\n",
        "Critical for policy-making and business operations, the study of global supply chains has been severely hampered by a lack of detailed data. Here we harness global firm-level transaction data covering 20m global firms, and 1 billion cross-border transactions, to infer key inputs for over 1200 products. Transforming this data to a directed network, we find that products are clustered into three large groups including textiles, chemicals and food, and machinery and metals. European industrial nations and China dominate critical intermediate products in the network such as metals, common components and tools, while industrial complexity is correlated with embeddedness in densely connected supply chains. To validate the network, we find structural similarities with two alternative product networks, one generated via LLM queries and the other derived by NAFTA to track product origins. We further detect linkages between products identified in manually mapped single sector supply chains, including electric vehicle batteries and semi-conductors. Finally, metrics derived from network structure capturing both forward and backward linkages are able to predict country-product diversification patterns with high accuracy."
      ],
      "metadata": {
        "id": "v_slZPC097_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### Summary of \"Deciphering the global production network from cross-border firm transactions\"\n",
        "\n",
        "**1. Core Problem & Novel Contribution:**\n",
        "*   **Problem:** Traditional global supply chain analysis is hampered by highly aggregated input-output (IO) tables (e.g., WIOD, WIOT) or fragmented, manually mapped sector-specific data. This limits granular insights into inter-product linkages and the propagation of shocks.\n",
        "*   **Novelty:** The authors address this by constructing a *directed, weighted product-level supply chain network* derived from *global, firm-level, cross-border transaction data*. This moves beyond aggregate sector-level or single-country firm-level analyses.\n",
        "\n",
        "**2. Data & Pre-processing (Computational & Data Engineering Aspect):**\n",
        "*   **Source:** UK Government Global Supply Chain Intelligence Programme (GSCIP) data.\n",
        "*   **Scale:** Covers 20 million global firms and 1 billion cross-border transactions from 2021-2023. This is orders of magnitude larger than typical datasets.\n",
        "*   **Attributes:** Includes detailed product codes (Harmonised System - HS 2022), transaction values/weights, firm location, industry code, ownership structure.\n",
        "*   **Key Pre-processing:**\n",
        "    *   Conversion of all transactions to HS 2022 for consistency.\n",
        "    *   **Crucially, aggregation of firms to an 'owner-country' level** using GSCIP's ownership hierarchy data. This addresses multinational structures and avoids double-counting within a conglomerate, though it's noted that local linkages (within a country) are not captured.\n",
        "    *   Filtering out firms exporting products in more than five HS Sections to mitigate noise from multi-product firms (e.g., wholesalers).\n",
        "\n",
        "**3. Methodology for Inferring Product Linkages (Econometrics & Network Science):**\n",
        "*   **\"Excess Purchase\" Metric (A_i,j):** This is the core innovation for inferring input importance. For any product *j* (the output) and potential input *i*, the metric `A_i,j` quantifies the \"excess\" purchase of *i* by firms that produce *j*, relative to the purchase of *i* by an average firm.\n",
        "    *   **Formula:** `A_i,j = (|S_j^i|/|S_j|) / (|S_i|/|S|)`\n",
        "        *   `S_j`: Set of firms producing product *j*.\n",
        "        *   `S_j^i`: Subset of firms in `S_j` that also purchase product *i*.\n",
        "        *   `S_i`: Set of all firms that purchase product *i*.\n",
        "        *   `S`: Set of all firms.\n",
        "    *   **Interpretation:** `A_i,j > 1` indicates product *i* is more prevalent in the purchasing basket of firms producing *j* than in the general firm population. This effectively down-weights ubiquitous inputs (e.g., cardboard boxes) and highlights specific inputs (e.g., aluminum panels for washing machines).\n",
        "*   **Network Construction:** A directed, weighted network of 1228 products (nodes) is formed, where `A_i,j` represents the edge weight from input *i* to output *j*. Thresholds are applied for `A_i,j > 1` and a minimum `firmcount` (number of firms providing evidence for the link) to control sparsity.\n",
        "\n",
        "**4. Key Findings & Network Structure (Network Science & Economic Geography):**\n",
        "*   **Community Structure:** Using a multi-scale community detection algorithm (based on random walkers, Delvenne et al. [2010]), the network broadly segregates into three main clusters:\n",
        "    1.  **Textiles:** Highly isolated.\n",
        "    2.  **Chemicals & Food:** Interconnected.\n",
        "    3.  **Machinery & Metals:** Interconnected.\n",
        "*   **Critical Intermediate Products (\"Choke Points\"):**\n",
        "    *   **Betweenness Centrality (BC):** Products with high BC (e.g., various machinery, common components like pumps/blades/taps/motors, plastics, chemical binders) act as key junctures in many supply paths.\n",
        "    *   **Country Dominance:** European industrial nations (Germany, Italy, Austria) and China dominate the export of products with high BC, indicating their control over critical intermediate goods.\n",
        "*   **Industrial Complexity & Embeddedness:**\n",
        "    *   **Hub Score:** A measure similar to eigencentrality, capturing \"neighbor of neighbor\" linkages and embeddedness in dense supply chains.\n",
        "    *   **Correlation:** Product Complexity Index (PCI) correlates positively with hub score (rank correlation 0.46). At the country level, Economic Complexity Index (ECI) correlates strongly with mean hub score (rank correlation 0.85). This suggests complex products/countries are deeply embedded in densely connected global supply chains.\n",
        "\n",
        "**5. Validation of Network Structure (Computer Science & Econometrics):**\n",
        "*   **Comparison with AI-generated Network:** Structural similarities were found with a product network generated via LLM queries (Fetzer et al. [2024]). While the AI network is sparser, both show similar meso-scale structures and significant in/out-degree correlations (peaking at ~0.68 for in-degree).\n",
        "*   **Comparison with NAFTA IO Network:** Similar aggregate clusters (food, textiles, chemicals, metals/machinery) were observed, despite lower direct edge correlations.\n",
        "*   **Validation against Manually Mapped Supply Chains:** The network successfully picks up dense linkages within manually constructed single-sector supply chains (e.g., electric vehicle batteries, semiconductors). A rigorous statistical test (comparing subgraph modularity to 100k random networks) yielded extremely low p-values (~1e-27 to 1e-35), indicating these observed linkages are highly unlikely to occur by chance.\n",
        "\n",
        "**6. Predictive Power for Economic Diversification (Econometrics):**\n",
        "*   **Density Metrics:** \"Downstream\" and \"Upstream\" density metrics are constructed, capturing the share of top in-degree (downstream) or out-degree (upstream) neighboring products present in a country.\n",
        "*   **Probit Regression:** A Probit model predicts country-product export diversification patterns (product appearance, defined by Rpop increase from 2016 to 2021).\n",
        "*   **Results:** Both downstream and upstream linkages show high predictive power (AUC values of 0.86-0.87), consistent with existing literature on capability overlap. Interestingly, predictive power is highest for \"weak links\" (low firmcount threshold) and for predicting smaller increases in Rpop, suggesting that even less certain connections are informative.\n",
        "\n",
        "**7. Limitations & Future Directions:**\n",
        "*   **Static Analysis:** The current network is aggregated over 2021-2023, precluding temporal analysis of supply chain evolution.\n",
        "*   **Cross-Border Focus:** Misses crucial local linkages within countries.\n",
        "*   **Data Noise:** Acknowledges inherent noise from uneven firm coverage, multi-product firms (despite mitigation efforts), and idiosyncratic purchasing patterns (e.g., semiconductor firms buying washing machines for chips during shortages).\n",
        "*   **Granularity:** Currently at HS 4-digit; higher granularity (6-digit) is computationally challenging due to exponential increase in edges.\n",
        "*   **Future Work:** Temporal analysis, regional-specific networks, higher granularity, and more sophisticated validation.\n",
        "\n",
        "**Overall Significance:**\n",
        "This paper provides an unprecedented, data-driven, and rigorously validated tool for understanding the complex global production network. Its insights into critical intermediate products, the relationship between industrial complexity and supply chain embeddedness, and its predictive power for diversification patterns have significant implications for policy-making (e.g., identifying choke points, assessing vulnerabilities, guiding industrial policy) and for advancing economic geography and network science research."
      ],
      "metadata": {
        "id": "SCx_Zkzc2WdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "8Xcr_j27DBE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  A High-Resolution Digital Twin of the Global Production Network\n",
        "#\n",
        "#  This module provides a complete, professional-grade, and end-to-end\n",
        "#  implementation of the research pipeline presented in \"Deciphering the global\n",
        "#  production network from cross-border firm transactions\" by O'Clery et al. (2025).\n",
        "#  It transforms massive-scale, firm-level transaction data into a computable,\n",
        "#  directed, and weighted product-space network. This \"digital twin\" of the\n",
        "#  global production economy enables quantitative analysis of supply chain\n",
        "#  structures, identification of critical products, and econometric prediction\n",
        "#  of national economic diversification patterns.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Network Inference from Firm Behavior: A novel statistical method to infer\n",
        "#    product-level input-output links from the \"excess purchase\" patterns of\n",
        "#    producer firms, using a dataset of 1 billion transactions.\n",
        "#  • Network Structure Analysis: Application of community detection (Leiden\n",
        "#    algorithm) and centrality measures (Betweenness, HITS) to uncover the\n",
        "#    meso- and micro-scale architecture of the global production system.\n",
        "#  • Multi-Pronged Validation: Rigorous validation of the inferred network via\n",
        "#    comparison with external benchmarks and statistical testing against a\n",
        "#    configuration model null hypothesis for known supply chains (EVs, semiconductors).\n",
        "#  • Econometric Prediction: Engineering of network-based \"density\" metrics\n",
        "#    that capture a country's proximity to new products in the network, and\n",
        "#    using these metrics in a Probit model to predict economic diversification.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • High-performance, vectorized data processing using Pandas and NumPy.\n",
        "#  • Efficient sparse matrix algebra (SciPy) for network construction and analysis.\n",
        "#  • State-of-the-art community detection using the Leiden algorithm.\n",
        "#  • Robust econometric modeling with fixed effects using Statsmodels.\n",
        "#  • A comprehensive, parallelized robustness analysis suite to test sensitivity\n",
        "#    to key methodological parameters and temporal windows.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  O'Clery, N., Radcliffe-Brown, B., Spencer, T., & Tarling-Hunter, D. (2025).\n",
        "#  Deciphering the global production network from cross-border firm transactions.\n",
        "#  arXiv preprint arXiv:2508.12315.\n",
        "#  https://arxiv.org/abs/2508.12315\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "import logging\n",
        "import warnings\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from typing import Dict, Any, List, Tuple, Set, Optional\n",
        "\n",
        "# --- Third-Party Library Imports ---\n",
        "\n",
        "# Core Data Manipulation and Numerical Computing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Network Analysis\n",
        "import networkx as nx\n",
        "import igraph as ig\n",
        "import leidenalg as la\n",
        "\n",
        "# Scientific and Statistical Computing\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Econometric Modeling\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.discrete.discrete_model import ProbitResults\n",
        "\n",
        "# Machine Learning and Model Evaluation\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# Parallel Processing\n",
        "from joblib import Parallel, delayed\n"
      ],
      "metadata": {
        "id": "5FlWp2bvDICp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "JiQSv9bzDSSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Documentation of Inputs, Processes and Outputs (IPO Analysis) of Key Pipeline Callables\n",
        "\n",
        "### **Module: Core Pipeline and Analysis**\n",
        "\n",
        "#### **Callable: `validate_and_assess_inputs`**\n",
        "*   **Inputs:** `transactions_log_frame`, `firm_metadata_frame`, `comtrade_exports_frame`, `country_data_frame`, `replication_manifest`. These are the raw, unprocessed data sources and the main configuration file.\n",
        "*   **Process:** This function is a non-mutating gatekeeper. It performs a series of checks:\n",
        "    1.  Validates the structure and values of the `replication_manifest` dictionary.\n",
        "    2.  Validates the schema (column names and dtypes) of all input DataFrames.\n",
        "    3.  Assesses data quality by checking for missing values in critical columns, verifying the completeness of ownership data, and confirming the temporal and product code ranges in the transaction data.\n",
        "*   **Outputs:** `None`. The function's purpose is to either pass silently, confirming the inputs are valid, or to raise a `DataValidationError`, halting execution before any processing begins.\n",
        "*   **Data Transformation:** No data is transformed. This is a read-only validation step.\n",
        "*   **Role in Research Pipeline:** This function implements the implicit but essential first step of any rigorous quantitative research: **Data Integrity Verification**. It ensures that the raw data conforms to the expected format and quality standards before being used in the analysis, preventing a wide range of downstream errors.\n",
        "\n",
        "#### **Callable: `preprocess_and_cleanse_data`**\n",
        "*   **Inputs:** `transactions_log_frame`, `firm_metadata_frame`, `replication_manifest`.\n",
        "*   **Process:** This function executes the primary data engineering and entity resolution steps.\n",
        "    1.  **Filtering:** It filters the transaction log to the specified date range (2021-2023) and removes records with non-positive values.\n",
        "    2.  **Entity Resolution:** It maps raw `firm_id`s to `ultimate_owner_id`s, handling missing owners by treating firms as self-owned.\n",
        "    3.  **Augmentation:** It merges the transaction log with firm metadata to attach the country of domicile to both seller and buyer.\n",
        "    4.  **Composite Key Creation:** It creates the primary analytical unit, the `owner_country_entity`, by concatenating owner ID and country.\n",
        "    5.  **Business Rule Filtering:** It removes domestic transactions and transactions from highly diversified firms (wholesalers), as specified in the \"Methods\" section (Page 14).\n",
        "*   **Outputs:** A single, cleansed `pd.DataFrame` (`cleansed_df`) where each row is a valid, cross-border transaction between two well-defined owner-country entities.\n",
        "*   **Data Transformation:** This is a heavy transformation step involving row filtering, value mapping, column joining, and string manipulation.\n",
        "*   **Role in Research Pipeline:** This function implements the **Data Preparation and Entity Resolution** stage. It transforms the raw transaction log into the specific, analysis-ready format required by the paper's novel network inference methodology.\n",
        "\n",
        "#### **Callable: `classify_firms_and_construct_sets`**\n",
        "*   **Inputs:** The `cleansed_df` from the previous step.\n",
        "*   **Process:**\n",
        "    1.  It calls `_identify_significant_entities` twice. This helper function first aggregates transaction values to the (entity, product) level. It then calculates the per-product average of these total transaction values. Finally, it identifies an entity as a \"significant\" producer (or purchaser) of a product if its total value exceeds this average.\n",
        "    2.  It then calls `_compute_intersection_metrics`, which calculates for every product pair `(i, j)` the size of the intersection set `S_i^j` (firms that produce `j` and purchase `i`) and the total value of product `i` purchased by firms in that set.\n",
        "*   **Outputs:** A tuple containing three data structures: `producer_sets` (Dict), `purchaser_sets` (Dict), and `intersection_metrics` (DataFrame).\n",
        "*   **Data Transformation:** This function transforms the transaction log into aggregated, set-based structures that form the direct inputs for the network inference formula.\n",
        "*   **Role in Research Pipeline:** This function implements the **Firm Classification** logic described in the \"Results\" section (Page 5). It operationalizes the core assumption of the paper: that the purchasing behavior of significant producers can be used to reveal input-output relationships.\n",
        "\n",
        "#### **Callable: `infer_and_construct_network`**\n",
        "*   **Inputs:** `cleansed_df`, `producer_sets`, `purchaser_sets`, `intersection_metrics`, `replication_manifest`.\n",
        "*   **Process:**\n",
        "    1.  It calculates the raw edge weight `A_ij` for every potential link using the paper's core formula.\n",
        "    2.  It applies the three sparsification filters: `firmcount` threshold, edge weight threshold, and minimum transaction value threshold.\n",
        "    3.  It constructs the final network representations from the filtered list of edges.\n",
        "*   **Outputs:** A tuple containing the final network objects: `adj_matrix` (a `csr_matrix`), `graph` (a `networkx.DiGraph`), and `product_to_idx` (a mapping dictionary).\n",
        "*   **Data Transformation:** This function transforms the set-based data into a weighted adjacency matrix and a graph object.\n",
        "*   **Role in Research Pipeline:** This function is the heart of the paper. It implements the **Network Inference Formula** from the \"Results\" section (Page 6):\n",
        "    $$\n",
        "    A_{i,j} = \\frac{|S_i^j|/|S_j|}{|S_i^\\dagger|/|S|}\n",
        "    $$\n",
        "    It also implements the **Network Sparsification** rules described in the \"Methods\" and \"Results\" sections (Pages 7 & 15) to refine the network.\n",
        "\n",
        "#### **Callable: `analyze_network_structure`**\n",
        "*   **Inputs:** `graph`, `adj_matrix`, `product_to_idx`, `replication_manifest`.\n",
        "*   **Process:**\n",
        "    1.  It performs multi-scale community detection using the Leiden algorithm, a state-of-the-art method analogous to the \"Stability\" algorithm mentioned in the paper.\n",
        "    2.  It characterizes the resulting communities with descriptive labels.\n",
        "    3.  It validates the network's topology by calculating basic statistics and comparing the intra-sector vs. inter-sector link densities.\n",
        "*   **Outputs:** A `pd.Series` (`community_labels`) mapping each product to its assigned community.\n",
        "*   **Data Transformation:** It transforms the graph structure into a node-level feature (community membership).\n",
        "*   **Role in Research Pipeline:** This function implements the **Meso-scale Structural Analysis** described in the \"Results\" section (Page 7). It replicates the finding of the three main product clusters (Machinery/Metals, Chemicals/Food, Textiles) and validates the structural properties shown in Figure 3A.\n",
        "\n",
        "#### **Callable: `calculate_centralities`**\n",
        "*   **Inputs:** `graph`, `replication_manifest`.\n",
        "*   **Process:**\n",
        "    1.  It calculates the weighted betweenness centrality for each node, correctly inverting the edge weights to represent \"distance\".\n",
        "    2.  It calculates the hub scores for each node using the HITS algorithm.\n",
        "    3.  It aggregates these scores into a single DataFrame, ranks them, and computes their correlation.\n",
        "*   **Outputs:** A `pd.DataFrame` (`centrality_df`) containing the centrality scores and ranks for each product.\n",
        "*   **Data Transformation:** It transforms the graph structure into node-level importance scores.\n",
        "*   **Role in Research Pipeline:** This function implements the **Node Centrality Analysis** described in the \"Results\" section (Page 7). It identifies the \"choke point\" products (high betweenness) and the products embedded in complex supply chains (high hub score), as shown in Figure 4.\n",
        "\n",
        "#### **Callable: `run_validation_procedures`**\n",
        "*   **Inputs:** `adj_matrix`, `product_to_idx`, `supply_chains_definitions_dict`, `replication_manifest`, and optional `external_networks`.\n",
        "*   **Process:**\n",
        "    1.  If external networks are provided, it aligns them to the same node set and computes edge-level and degree-level correlations.\n",
        "    2.  For each manually defined supply chain (EVs, semiconductors), it calculates the empirical directed modularity of the corresponding subgraph.\n",
        "    3.  It runs a large-scale Monte Carlo simulation, generating thousands of random graphs with the same degree sequence (the configuration model).\n",
        "    4.  It calculates the p-value of the empirical modularity against the null distribution from the random graphs.\n",
        "*   **Outputs:** A dictionary (`validation_results`) containing the correlation metrics and the modularity test results (empirical modularity and p-value) for each supply chain.\n",
        "*   **Data Transformation:** This function transforms the network structure into statistical evidence of its validity.\n",
        "*   **Role in Research Pipeline:** This function implements the rigorous **Network Validation** procedures described throughout the \"Results\" section (Pages 9-11). It specifically replicates the comparison to external networks (Figure 5) and the statistical significance test for manually mapped supply chains (Figure 6), including the directed modularity formula from the \"Methods\" section (Page 15):\n",
        "    $$\n",
        "    M_G = \\frac{1}{m}\\left(\\sum_{i,j \\in G} X_{i,j} - \\frac{Out_i \\cdot In_j}{m}\\right)\n",
        "    $$\n",
        "\n",
        "#### **Callable: `engineer_economic_features`**\n",
        "*   **Inputs:** `comtrade_exports_frame`, `country_data_frame`, `adj_matrix`, `product_to_idx`, `replication_manifest`.\n",
        "*   **Process:**\n",
        "    1.  It calculates the Revealed Population-Adjusted Comparative Advantage (Rpop) for each country-product-year.\n",
        "    2.  It uses Rpop to construct the binary diversification outcome variable (absent in 2016, present in 2021).\n",
        "    3.  It calculates the upstream and downstream network density metrics for each country-product pair using efficient matrix algebra.\n",
        "    4.  It assembles and filters the final long-format dataset for the regression analysis.\n",
        "*   **Outputs:** A `pd.DataFrame` (`econometric_df`) ready for modeling.\n",
        "*   **Data Transformation:** This is a major feature engineering step, transforming raw trade data and the network structure into the specific dependent and independent variables for the econometric model.\n",
        "*   **Role in Research Pipeline:** This function implements the **Economic Feature Engineering** required for the final analysis. It calculates the Rpop metric from the \"Methods\" section (Page 15) and the crucial network density metrics from the \"Results\" section (Page 11):\n",
        "    $$\n",
        "    d_{p,c} = \\frac{\\sum_{j \\in J_p} I(A_{p,j}) \\cdot M_{j,c}}{\\sum_{j \\in J_p} I(A_{p,j})}\n",
        "    $$\n",
        "\n",
        "#### **Callable: `run_econometric_analysis`**\n",
        "*   **Inputs:** `econometric_df`, `replication_manifest`.\n",
        "*   **Process:**\n",
        "    1.  It specifies and estimates the Probit models for upstream and downstream density, including country and product fixed effects.\n",
        "    2.  It evaluates the models' predictive performance using AUC and ROC curves.\n",
        "    3.  It extracts key results, including coefficients, p-values, and the correctly calculated Average Marginal Effects for interpretation.\n",
        "*   **Outputs:** A dictionary (`econometric_results`) containing the full results, performance metrics, and summary statistics for both models.\n",
        "*   **Data Transformation:** This function transforms the engineered dataset into a statistical model and a set of interpretable economic results.\n",
        "*   **Role in Research Pipeline:** This function implements the final **Econometric Analysis** described in the \"Results\" section (Page 12). It estimates the Probit model:\n",
        "    $$\n",
        "    R_{p,c} = \\Phi(\\alpha + \\beta_d d_{p,c} + \\gamma_p + \\eta_c)\n",
        "    $$\n",
        "    and produces the results shown in the tables in Figure 7.\n",
        "\n",
        "### **Module: Orchestration and Robustness**\n",
        "\n",
        "#### **Callable: `run_end_to_end_pipeline`**\n",
        "*   **Inputs:** All raw data and configuration files.\n",
        "*   **Process:** Sequentially executes the nine task-level orchestrators listed above.\n",
        "*   **Outputs:** A comprehensive dictionary containing all major artifacts from the pipeline.\n",
        "*   **Data Transformation:** Orchestrates the entire transformation from raw data to final conclusion.\n",
        "*   **Role in Research Pipeline:** Serves as the **Master Orchestrator** for a single, complete replication of the study.\n",
        "\n",
        "#### **Callable: `run_full_robustness_analysis`**\n",
        "*   **Inputs:** All raw data and configuration files, plus configurations for the robustness checks (`parameter_grid`, `methods_to_test`).\n",
        "*   **Process:** Orchestrates the execution of the three distinct robustness checks: parameter sensitivity, temporal window analysis, and alternative construction methods. It calls the appropriate specialized orchestrators for each check.\n",
        "*   **Outputs:** A dictionary containing the summary DataFrames from each completed robustness check.\n",
        "*   **Data Transformation:** Orchestrates multiple runs of the full pipeline under varying assumptions.\n",
        "*   **Role in Research Pipeline:** Serves as the **Master Robustness Orchestrator**, executing the analyses described in Task 11 to test the stability and reliability of the baseline findings.\n",
        "\n",
        "#### **Callable: `main_analysis_orchestrator`**\n",
        "*   **Inputs:** All raw data and configuration files.\n",
        "*   **Process:** Serves as the ultimate top-level entry point. It first calls `run_end_to_end_pipeline` to get the baseline results. Then, if enabled, it calls `run_full_robustness_analysis` to get the robustness results.\n",
        "*   **Outputs:** A final master dictionary containing both the baseline and robustness results.\n",
        "*   **Data Transformation:** Orchestrates the entire project.\n",
        "*   **Role in Research Pipeline:** Acts as the **Main Entry Point** for the entire project, providing a single function to replicate the paper and perform a comprehensive stability analysis.\n",
        "\n",
        "<br><br>\n",
        "### Usage Example\n",
        "\n",
        "### **Example Usage of the End-to-End Pipeline**\n",
        "\n",
        "This example demonstrates how to load the necessary data, define the configurations, and execute the entire research workflow—including the baseline replication and the full suite of robustness checks—using the master orchestrator function, `main_analysis_orchestrator`.\n",
        "\n",
        "#### **1. Preamble: Loading Data and Defining Configurations**\n",
        "\n",
        "Before calling the main function, a user must first load all required data into memory as pandas DataFrames and define the configurations for the analysis.\n",
        "\n",
        "**Step 1.1: Load Input DataFrames**\n",
        "\n",
        "In a real-world scenario, this data would be loaded from databases, CSV files, or other data sources. For this example, we will create placeholder DataFrames that conform *exactly* to the required schemas.\n",
        "\n",
        "```python\n",
        "# --- Assume this block is in a script named `run_experiment.py` ---\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# NOTE: In a real application, you would load your actual data here.\n",
        "# For example:\n",
        "# transactions_log_frame = pd.read_csv(\"path/to/transactions.csv\", parse_dates=['timestamp'])\n",
        "# firm_metadata_frame = pd.read_csv(\"path/to/firm_metadata.csv\")\n",
        "# ... and so on.\n",
        "\n",
        "# For this example, we create correctly-structured placeholder DataFrames.\n",
        "# This demonstrates the exact format the pipeline expects.\n",
        "\n",
        "# i. Transaction Log\n",
        "transactions_log_frame = pd.DataFrame({\n",
        "    'seller_firm_id': pd.Series([101, 102, 103], dtype=pd.Int64Dtype()),\n",
        "    'buyer_firm_id': pd.Series([201, 202, 201], dtype=pd.Int64Dtype()),\n",
        "    'product_hs_code': pd.Series([8507, 2836, 8541], dtype=pd.Int16Dtype()),\n",
        "    'transaction_value_usd': pd.Series([10000.0, 5000.0, 25000.0], dtype=np.float64),\n",
        "    'timestamp': pd.to_datetime(['2022-01-15', '2022-03-10', '2023-06-01'])\n",
        "})\n",
        "\n",
        "# ii. Firm Metadata\n",
        "firm_metadata_frame = pd.DataFrame({\n",
        "    'firm_id': pd.Series([101, 102, 103, 201, 202], dtype=pd.Int64Dtype()),\n",
        "    'legal_name': pd.Series(['SellerA', 'SellerB', 'SellerC', 'BuyerX', 'BuyerY'], dtype=pd.StringDtype()),\n",
        "    'country_domicile': pd.Series(['DEU', 'CHN', 'USA', 'USA', 'MEX'], dtype='category'),\n",
        "    'ultimate_owner_id': pd.Series([10, 10, 11, 20, 21], dtype=pd.Int64Dtype())\n",
        "})\n",
        "\n",
        "# iii. Comtrade Exports Data\n",
        "comtrade_exports_frame = pd.DataFrame({\n",
        "    'year': pd.Series([2016, 2021, 2016, 2021], dtype=np.int16),\n",
        "    'reporter_iso': pd.Series(['DEU', 'DEU', 'CHN', 'CHN'], dtype='category'),\n",
        "    'product_hs_code': pd.Series([8507, 8507, 2836, 2836], dtype=pd.Int16Dtype()),\n",
        "    'export_value_usd': pd.Series([1e9, 1.5e9, 2.2e9, 3e9], dtype=np.float64)\n",
        "})\n",
        "\n",
        "# iv. Country Data\n",
        "country_data_frame = pd.DataFrame({\n",
        "    'year': pd.Series([2016, 2021, 2016, 2021], dtype=np.int16),\n",
        "    'reporter_iso': pd.Series(['DEU', 'DEU', 'CHN', 'CHN'], dtype='category'),\n",
        "    'population': pd.Series([82e6, 83e6, 1.37e9, 1.41e9], dtype=np.int64)\n",
        "})\n",
        "\n",
        "# For a complete run, these placeholder frames would need to be populated\n",
        "# with sufficient data to avoid errors in the econometric stage (e.g.,\n",
        "# not enough observations to fit the model).\n",
        "```\n",
        "\n",
        "**Step 1.2: Define Configuration Dictionaries**\n",
        "\n",
        "Next, we define the non-data inputs: the manifest, the supply chain definitions, and the configurations for the robustness checks.\n",
        "\n",
        "```python\n",
        "# v. Supply Chain Definitions\n",
        "supply_chains_definitions_dict = {\n",
        "    \"EV Battery\": [8507, 2836, 2530, 7110, 2825, 2827],\n",
        "    \"Semiconductor\": [8541, 8542, 3818, 2804, 8486]\n",
        "}\n",
        "\n",
        "# vi. Base Replication Manifest\n",
        "# This is the primary configuration for the baseline study replication.\n",
        "replication_manifest = {\n",
        "    'parameters': {\n",
        "        'data_ingestion': {\n",
        "            'start_date': '2021-01-01',\n",
        "            'end_date': '2023-12-31',\n",
        "            'hs_revision_standard': 2022,\n",
        "        },\n",
        "        'network_inference': {\n",
        "            'multi_product_firm_hs2_threshold': 5,\n",
        "            'min_aggregated_link_value_usd': 1000.0,\n",
        "            'primary_edge_weight_threshold': 2.0,\n",
        "            'primary_firmcount_threshold': 2,\n",
        "        },\n",
        "        'network_analysis': {\n",
        "            'community_detection_iterations': 100,\n",
        "            'modularity_validation_simulations': 1000, # Reduced for example speed\n",
        "        },\n",
        "        'econometric_analysis': {\n",
        "            'start_year_for_diversification': 2016,\n",
        "            'end_year_for_diversification': 2021,\n",
        "            'rpop_absence_threshold': 0.05,\n",
        "            'rpop_presence_threshold': 0.1,\n",
        "            'density_metric_top_k_edges': 50,\n",
        "            'min_global_trade_for_product_inclusion_usd': 2e9,\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# --- Define Configurations for Robustness Checks ---\n",
        "\n",
        "# Configuration for Task 11.1: Parameter Sensitivity Analysis\n",
        "# We will test a grid of firmcount and edge weight thresholds.\n",
        "# Note the use of path tuples for the keys, as required by our robust implementation.\n",
        "parameter_grid_for_robustness = {\n",
        "    ('network_inference', 'primary_firmcount_threshold'): [2, 3, 4],\n",
        "    ('network_inference', 'primary_edge_weight_threshold'): [1.5, 2.0, 2.5],\n",
        "}\n",
        "\n",
        "# Configuration for Task 11.3: Construction Robustness Analysis\n",
        "# We will test the baseline 'mean' against 'median' and the 75th percentile.\n",
        "methods_to_test_for_robustness = ['mean', 'median', 0.75]\n",
        "\n",
        "# Optional: External networks would be loaded here into the required format.\n",
        "# For example:\n",
        "# ai_network_matrix = scipy.sparse.load_npz(\"path/to/ai_network.npz\")\n",
        "# ai_product_list = pd.read_csv(\"path/to/ai_products.csv\")['hs_code'].tolist()\n",
        "# external_networks_for_validation = {\n",
        "#     \"AI_Network\": {\n",
        "#         \"matrix\": ai_network_matrix,\n",
        "#         \"product_list\": ai_product_list\n",
        "#     }\n",
        "# }\n",
        "external_networks_for_validation = None\n",
        "```\n",
        "\n",
        "#### **2. Execution: Calling the Top-Level Orchestrator**\n",
        "\n",
        "With all inputs prepared, the entire analysis can be launched with a single function call.\n",
        "\n",
        "```python\n",
        "# --- Assume all pipeline functions are imported ---\n",
        "# from my_pipeline.main import main_analysis_orchestrator\n",
        "\n",
        "# --- Execute the Full Analysis ---\n",
        "\n",
        "# This single function call will:\n",
        "# 1. Run the complete baseline replication of the study.\n",
        "# 2. Run the full suite of robustness checks as configured.\n",
        "final_results = main_analysis_orchestrator(\n",
        "    # Pass all the loaded data\n",
        "    transactions_log_frame=transactions_log_frame,\n",
        "    firm_metadata_frame=firm_metadata_frame,\n",
        "    comtrade_exports_frame=comtrade_exports_frame,\n",
        "    country_data_frame=country_data_frame,\n",
        "    supply_chains_definitions_dict=supply_chains_definitions_dict,\n",
        "    # Pass the configurations\n",
        "    base_manifest=replication_manifest,\n",
        "    run_robustness_checks=True, # Explicitly enable the robustness suite\n",
        "    parameter_grid=parameter_grid_for_robustness,\n",
        "    methods_to_test=methods_to_test_for_robustness,\n",
        "    external_networks=external_networks_for_validation,\n",
        "    n_jobs=-1 # Use all available CPU cores for parallel tasks\n",
        ")\n",
        "\n",
        "# --- 3. Inspecting the Results ---\n",
        "\n",
        "# The `final_results` dictionary now contains all outputs in a structured format.\n",
        "\n",
        "# Example: Access the baseline econometric results\n",
        "baseline_econometrics = final_results['baseline_results']['econometric_results']\n",
        "print(\"\\n--- Baseline Downstream Model Summary ---\")\n",
        "print(baseline_econometrics['downstream']['statsmodels_results'].summary())\n",
        "\n",
        "# Example: Access the temporal robustness comparison table\n",
        "temporal_robustness_df = final_results['robustness_results']['temporal_robustness']\n",
        "print(\"\\n--- Temporal Robustness Results ---\")\n",
        "print(temporal_robustness_df[['downstream_auc', 'upstream_auc', 'sample_size']])\n",
        "\n",
        "# Example: Access the parameter sensitivity results\n",
        "sensitivity_df = final_results['robustness_results']['parameter_sensitivity']\n",
        "print(\"\\n--- Parameter Sensitivity Results ---\")\n",
        "print(sensitivity_df.head())\n",
        "```\n",
        "\n",
        "This example provides a complete template for using the pipeline. It separates the data loading and configuration steps from the execution step, uses placeholder data that perfectly matches the required schemas, and demonstrates how to access the structured results for further analysis.\n"
      ],
      "metadata": {
        "id": "-1yhMszCDWbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Data Validation and Quality Assurance\n",
        "\n",
        "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
        "# Custom Exception Class for Enhanced Error Reporting\n",
        "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
        "\n",
        "class DataValidationError(Exception):\n",
        "    \"\"\"\n",
        "    A custom exception class for handling specific data validation failures.\n",
        "\n",
        "    Purpose:\n",
        "    --------\n",
        "    This exception is designed to be raised when an input DataFrame, parameter,\n",
        "    or data record fails to meet the predefined structural, logical, or quality\n",
        "    constraints required by the analytical pipeline. Using a custom exception\n",
        "    type allows for specific `try...except` blocks that can catch validation\n",
        "    errors distinctly from other runtime errors (e.g., `ValueError`, `TypeError`),\n",
        "    enabling more granular error handling and clearer, more informative logging.\n",
        "\n",
        "    Inheritance:\n",
        "    ------------\n",
        "    Inherits from the base `Exception` class, making it a standard checked\n",
        "    exception in Python.\n",
        "\n",
        "    Usage:\n",
        "    ------\n",
        "    This class should be instantiated and raised within validation functions\n",
        "    when a specific, non-recoverable data integrity issue is detected.\n",
        "\n",
        "    Example:\n",
        "    --------\n",
        "    >>> if not column_set == expected_column_set:\n",
        "    ...     raise DataValidationError(\"DataFrame columns do not match schema.\")\n",
        "    \"\"\"\n",
        "    # The 'pass' statement indicates that this class inherits all the\n",
        "    # behavior from its parent class, `Exception`, without adding any new\n",
        "    # methods or attributes. Its primary purpose is to create a new, distinct\n",
        "    # exception type for semantic clarity and targeted error handling.\n",
        "    pass\n",
        "\n",
        "# =============================================================================\n",
        "# Task 1.1: Input Parameter Validation\n",
        "# =============================================================================\n",
        "\n",
        "def _validate_manifest_structure(\n",
        "    manifest: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the hierarchical structure of the replication_manifest dictionary.\n",
        "\n",
        "    This function ensures that all required primary and nested keys exist,\n",
        "    preventing downstream errors from misconfigured or incomplete parameter\n",
        "    dictionaries. It employs a recursive helper function to traverse the\n",
        "    expected schema.\n",
        "\n",
        "    Args:\n",
        "        manifest (Dict[str, Any]): The replication_manifest dictionary.\n",
        "\n",
        "    Raises:\n",
        "        DataValidationError: If a required key is missing or a value is not a\n",
        "                             dictionary where one is expected.\n",
        "    \"\"\"\n",
        "    # Define the required hierarchical schema for the manifest.\n",
        "    required_schema = {\n",
        "        'parameters': {\n",
        "            'data_ingestion': None,\n",
        "            'network_inference': None,\n",
        "            'network_analysis': None,\n",
        "            'econometric_analysis': None,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Define a recursive helper function to traverse and validate the schema.\n",
        "    def _traverse_and_check(\n",
        "        sub_manifest: Dict[str, Any],\n",
        "        sub_schema: Dict[str, Any],\n",
        "        path: str\n",
        "    ) -> None:\n",
        "        # Iterate through all keys required by the current schema level.\n",
        "        for key, nested_schema in sub_schema.items():\n",
        "            # Check if the key is present in the manifest at the current path.\n",
        "            if key not in sub_manifest:\n",
        "                # Raise a specific error if a required key is missing.\n",
        "                raise DataValidationError(\n",
        "                    f\"Missing required key '{key}' in manifest at path: '{path}'.\"\n",
        "                )\n",
        "\n",
        "            # If the schema expects a nested dictionary, recurse.\n",
        "            if nested_schema is not None:\n",
        "                # Get the nested dictionary from the manifest.\n",
        "                nested_manifest = sub_manifest[key]\n",
        "                # Ensure the value is indeed a dictionary before traversing.\n",
        "                if not isinstance(nested_manifest, dict):\n",
        "                    raise DataValidationError(\n",
        "                        f\"Expected a dictionary for key '{key}' at path '{path}', \"\n",
        "                        f\"but found type {type(nested_manifest).__name__}.\"\n",
        "                    )\n",
        "                # Recursively call the traversal function for the nested structure.\n",
        "                _traverse_and_check(\n",
        "                    nested_manifest,\n",
        "                    nested_schema,\n",
        "                    f\"{path}.{key}\"\n",
        "                )\n",
        "\n",
        "    # Start the validation from the top level of the manifest.\n",
        "    _traverse_and_check(manifest, required_schema, \"manifest\")\n",
        "\n",
        "\n",
        "def _validate_manifest_date_parameters(\n",
        "    params: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates date parameters within the manifest for format and logical order.\n",
        "\n",
        "    Args:\n",
        "        params (Dict[str, Any]): The 'parameters' sub-dictionary from the manifest.\n",
        "\n",
        "    Raises:\n",
        "        DataValidationError: If date strings are malformed or if start_date is\n",
        "                             not before end_date.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract the date strings from the data_ingestion parameters.\n",
        "        start_date_str = params['data_ingestion']['start_date']\n",
        "        end_date_str = params['data_ingestion']['end_date']\n",
        "\n",
        "        # Attempt to parse the date strings into pandas Timestamp objects.\n",
        "        # errors='raise' will throw an exception for any parsing failure.\n",
        "        start_date = pd.to_datetime(start_date_str, errors='raise')\n",
        "        end_date = pd.to_datetime(end_date_str, errors='raise')\n",
        "\n",
        "    except KeyError as e:\n",
        "        # Catch missing keys and raise a more informative error.\n",
        "        raise DataValidationError(\n",
        "            f\"Missing required date parameter in 'data_ingestion': {e}\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # Catch parsing errors and raise a specific validation error.\n",
        "        raise DataValidationError(\n",
        "            f\"Failed to parse date parameters. Ensure 'start_date' and \"\n",
        "            f\"'end_date' are valid date strings. Original error: {e}\"\n",
        "        )\n",
        "\n",
        "    # Check the logical ordering of the dates.\n",
        "    if start_date >= end_date:\n",
        "        # Raise an error if the start date is not strictly before the end date.\n",
        "        raise DataValidationError(\n",
        "            f\"Logical error: 'start_date' ({start_date_str}) must be before \"\n",
        "            f\"'end_date' ({end_date_str}).\"\n",
        "        )\n",
        "\n",
        "\n",
        "def _validate_manifest_numerical_thresholds(\n",
        "    params: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates numerical thresholds in the manifest against defined constraints.\n",
        "\n",
        "    Args:\n",
        "        params (Dict[str, Any]): The 'parameters' sub-dictionary from the manifest.\n",
        "\n",
        "    Raises:\n",
        "        DataValidationError: If any numerical parameter is not of a valid type\n",
        "                             or violates its positivity constraint.\n",
        "    \"\"\"\n",
        "    # Define a list of validation rules for numerical parameters.\n",
        "    # Each tuple contains: (path_tuple, validation_lambda, error_message).\n",
        "    validation_rules = [\n",
        "        (\n",
        "            ('network_inference', 'multi_product_firm_hs2_threshold'),\n",
        "            lambda x: x >= 1,\n",
        "            \"must be an integer ≥ 1\"\n",
        "        ),\n",
        "        (\n",
        "            ('network_inference', 'min_aggregated_link_value_usd'),\n",
        "            lambda x: x > 0,\n",
        "            \"must be a number > 0\"\n",
        "        ),\n",
        "        (\n",
        "            ('network_inference', 'primary_edge_weight_threshold'),\n",
        "            lambda x: x > 0,\n",
        "            \"must be a number > 0\"\n",
        "        ),\n",
        "        (\n",
        "            ('network_inference', 'primary_firmcount_threshold'),\n",
        "            lambda x: x >= 1,\n",
        "            \"must be an integer ≥ 1\"\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    # Iterate through each defined validation rule.\n",
        "    for path, rule, msg in validation_rules:\n",
        "        try:\n",
        "            # Safely navigate the nested dictionary to get the parameter value.\n",
        "            value = params[path[0]][path[1]]\n",
        "\n",
        "            # First, check if the value is a valid number (int or float).\n",
        "            if not isinstance(value, (int, float)):\n",
        "                # Raise a type error if it's not a number.\n",
        "                raise DataValidationError(\n",
        "                    f\"Parameter '{'.'.join(path)}' must be a number, but found \"\n",
        "                    f\"type {type(value).__name__}.\"\n",
        "                )\n",
        "\n",
        "            # Apply the specific validation rule for the parameter.\n",
        "            if not rule(value):\n",
        "                # Raise a value error if the rule is violated.\n",
        "                raise DataValidationError(\n",
        "                    f\"Parameter '{'.'.join(path)}' with value {value} is invalid. \"\n",
        "                    f\"Constraint: {msg}.\"\n",
        "                )\n",
        "        except KeyError:\n",
        "            # Catch cases where the parameter path does not exist.\n",
        "            raise DataValidationError(\n",
        "                f\"Missing required numerical parameter: '{'.'.join(path)}'.\"\n",
        "            )\n",
        "\n",
        "# =============================================================================\n",
        "# Task 1.2: DataFrame Structure Validation\n",
        "# =============================================================================\n",
        "\n",
        "def _validate_dataframe_schemas(\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    firm_metadata_frame: pd.DataFrame,\n",
        "    comtrade_exports_frame: pd.DataFrame,\n",
        "    country_data_frame: pd.DataFrame\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the schema (columns and dtypes) of all input DataFrames.\n",
        "\n",
        "    Args:\n",
        "        transactions_log_frame (pd.DataFrame): The transactions log.\n",
        "        firm_metadata_frame (pd.DataFrame): The firm metadata.\n",
        "        comtrade_exports_frame (pd.DataFrame): The Comtrade export data.\n",
        "        country_data_frame (pd.DataFrame): The country attribute data.\n",
        "\n",
        "    Raises:\n",
        "        DataValidationError: If any DataFrame has incorrect columns or dtypes.\n",
        "    \"\"\"\n",
        "    # Define the expected schema for all input DataFrames.\n",
        "    expected_schemas = {\n",
        "        \"transactions_log_frame\": {\n",
        "            'seller_firm_id': pd.Int64Dtype(),\n",
        "            'buyer_firm_id': pd.Int64Dtype(),\n",
        "            'product_hs_code': pd.Int16Dtype(),\n",
        "            'transaction_value_usd': np.dtype('float64'),\n",
        "            'timestamp': np.dtype('datetime64[ns]')\n",
        "        },\n",
        "        \"firm_metadata_frame\": {\n",
        "            'firm_id': pd.Int64Dtype(),\n",
        "            'legal_name': pd.StringDtype(),\n",
        "            'country_domicile': pd.CategoricalDtype(),\n",
        "            'ultimate_owner_id': pd.Int64Dtype()\n",
        "        },\n",
        "        \"comtrade_exports_frame\": {\n",
        "            'year': np.dtype('int16'),\n",
        "            'reporter_iso': pd.CategoricalDtype(),\n",
        "            'product_hs_code': pd.Int16Dtype(),\n",
        "            'export_value_usd': np.dtype('float64')\n",
        "        },\n",
        "        \"country_data_frame\": {\n",
        "            'year': np.dtype('int16'),\n",
        "            'reporter_iso': pd.CategoricalDtype(),\n",
        "            'population': np.dtype('int64')\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create a dictionary mapping DataFrame names to their objects.\n",
        "    data_frames = {\n",
        "        \"transactions_log_frame\": transactions_log_frame,\n",
        "        \"firm_metadata_frame\": firm_metadata_frame,\n",
        "        \"comtrade_exports_frame\": comtrade_exports_frame,\n",
        "        \"country_data_frame\": country_data_frame\n",
        "    }\n",
        "\n",
        "    # Iterate through each DataFrame and its expected schema.\n",
        "    for name, schema in expected_schemas.items():\n",
        "        # Get the actual DataFrame object.\n",
        "        df = data_frames[name]\n",
        "\n",
        "        # 1. Validate column set integrity.\n",
        "        # Get the set of expected and actual column names.\n",
        "        expected_cols = set(schema.keys())\n",
        "        actual_cols = set(df.columns)\n",
        "\n",
        "        # Check if the column sets are identical.\n",
        "        if expected_cols != actual_cols:\n",
        "            # Identify missing and unexpected columns for a precise error message.\n",
        "            missing = sorted(list(expected_cols - actual_cols))\n",
        "            extra = sorted(list(actual_cols - expected_cols))\n",
        "            raise DataValidationError(\n",
        "                f\"Schema validation failed for '{name}'.\\n\"\n",
        "                f\"Missing columns: {missing if missing else 'None'}\\n\"\n",
        "                f\"Unexpected columns: {extra if extra else 'None'}\"\n",
        "            )\n",
        "\n",
        "        # 2. Validate dtypes for each column.\n",
        "        for col, expected_dtype in schema.items():\n",
        "            # Get the actual dtype of the column.\n",
        "            actual_dtype = df[col].dtype\n",
        "            # Compare the actual dtype with the expected dtype.\n",
        "            if actual_dtype != expected_dtype:\n",
        "                raise DataValidationError(\n",
        "                    f\"Dtype validation failed for '{name}'.\\n\"\n",
        "                    f\"Column '{col}' has dtype '{actual_dtype}', but \"\n",
        "                    f\"expected '{expected_dtype}'.\"\n",
        "                )\n",
        "\n",
        "    # 3. Validate ISO code format for relevant columns.\n",
        "    # Check 'country_domicile' in firm_metadata_frame.\n",
        "    invalid_iso_domicile = firm_metadata_frame[\n",
        "        ~firm_metadata_frame['country_domicile'].astype(str).str.match(r'^[A-Z]{3}$')\n",
        "    ]\n",
        "    if not invalid_iso_domicile.empty:\n",
        "        raise DataValidationError(\n",
        "            \"Invalid ISO 3166-1 alpha-3 format found in \"\n",
        "            \"'firm_metadata_frame.country_domicile'.\"\n",
        "        )\n",
        "\n",
        "    # Check 'reporter_iso' in comtrade_exports_frame.\n",
        "    invalid_iso_comtrade = comtrade_exports_frame[\n",
        "        ~comtrade_exports_frame['reporter_iso'].astype(str).str.match(r'^[A-Z]{3}$')\n",
        "    ]\n",
        "    if not invalid_iso_comtrade.empty:\n",
        "        raise DataValidationError(\n",
        "            \"Invalid ISO 3166-1 alpha-3 format found in \"\n",
        "            \"'comtrade_exports_frame.reporter_iso'.\"\n",
        "        )\n",
        "\n",
        "# =============================================================================\n",
        "# Task 1.3: Data Completeness Assessment\n",
        "# =============================================================================\n",
        "\n",
        "def _assess_missing_values(\n",
        "    transactions_log_frame: pd.DataFrame\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Assesses missing value percentages in critical columns and issues warnings.\n",
        "\n",
        "    Args:\n",
        "        transactions_log_frame (pd.DataFrame): The transactions log.\n",
        "    \"\"\"\n",
        "    # Define critical columns and the warning threshold.\n",
        "    critical_columns = [\n",
        "        'seller_firm_id', 'buyer_firm_id',\n",
        "        'product_hs_code', 'transaction_value_usd'\n",
        "    ]\n",
        "    threshold = 0.01  # 1%\n",
        "\n",
        "    # Calculate the percentage of missing values for each column.\n",
        "    missing_pct = transactions_log_frame[critical_columns].isnull().mean()\n",
        "\n",
        "    # Iterate through the critical columns to check against the threshold.\n",
        "    for col, pct in missing_pct.items():\n",
        "        # If the percentage of missing values exceeds the threshold...\n",
        "        if pct > threshold:\n",
        "            # ...issue a user warning that does not halt execution.\n",
        "            warnings.warn(\n",
        "                f\"Column '{col}' has {pct:.2%} missing values, which \"\n",
        "                f\"exceeds the {threshold:.0%} threshold.\",\n",
        "                UserWarning\n",
        "            )\n",
        "\n",
        "\n",
        "def _assess_ownership_completeness(\n",
        "    firm_metadata_frame: pd.DataFrame\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the percentage of firms with unresolved ownership.\n",
        "\n",
        "    Args:\n",
        "        firm_metadata_frame (pd.DataFrame): The firm metadata.\n",
        "\n",
        "    Returns:\n",
        "        float: The percentage of firms where 'ultimate_owner_id' is null.\n",
        "    \"\"\"\n",
        "    # Calculate the percentage of nulls in the 'ultimate_owner_id' column.\n",
        "    unresolved_pct = firm_metadata_frame['ultimate_owner_id'].isnull().mean()\n",
        "\n",
        "    # Log the finding for informational purposes.\n",
        "    logging.info(\n",
        "        f\"Ownership resolution completeness: {1 - unresolved_pct:.2%} of firms \"\n",
        "        f\"have an 'ultimate_owner_id'. {unresolved_pct:.2%} are unresolved.\"\n",
        "    )\n",
        "\n",
        "    # Return the percentage of unresolved firms.\n",
        "    return unresolved_pct\n",
        "\n",
        "\n",
        "def _assess_temporal_and_hs_code_coverage(\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    params: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates temporal coverage and HS code range integrity.\n",
        "\n",
        "    Args:\n",
        "        transactions_log_frame (pd.DataFrame): The transactions log.\n",
        "        params (Dict[str, Any]): The 'parameters' sub-dictionary from the manifest.\n",
        "\n",
        "    Raises:\n",
        "        DataValidationError: If temporal coverage is incorrect or HS codes are\n",
        "                             outside the valid 4-digit range.\n",
        "    \"\"\"\n",
        "    # 1. Validate temporal coverage.\n",
        "    # Get expected start and end dates from parameters.\n",
        "    expected_start = pd.to_datetime(params['data_ingestion']['start_date'])\n",
        "    expected_end = pd.to_datetime(params['data_ingestion']['end_date'])\n",
        "\n",
        "    # Get actual min and max timestamps from the data.\n",
        "    actual_min_date = transactions_log_frame['timestamp'].min()\n",
        "    actual_max_date = transactions_log_frame['timestamp'].max()\n",
        "\n",
        "    # Check if the actual date range is contained within the expected range.\n",
        "    if not (actual_min_date >= expected_start and actual_max_date <= expected_end):\n",
        "        raise DataValidationError(\n",
        "            f\"Temporal coverage validation failed. Data spans from \"\n",
        "            f\"{actual_min_date.date()} to {actual_max_date.date()}, which is \"\n",
        "            f\"outside the expected range of {expected_start.date()} to \"\n",
        "            f\"{expected_end.date()}.\"\n",
        "        )\n",
        "\n",
        "    # 2. Validate HS code range.\n",
        "    # Create a boolean mask for valid HS codes (4-digit, non-zero).\n",
        "    # HS codes range from section 01 to 99, so codes are 0101 to 99xx.\n",
        "    is_valid_hs = (\n",
        "        (transactions_log_frame['product_hs_code'] >= 101) &\n",
        "        (transactions_log_frame['product_hs_code'] <= 9999)\n",
        "    )\n",
        "\n",
        "    # Count the number of invalid HS codes.\n",
        "    invalid_hs_count = (~is_valid_hs).sum()\n",
        "\n",
        "    # If any invalid codes are found, raise an error.\n",
        "    if invalid_hs_count > 0:\n",
        "        raise DataValidationError(\n",
        "            f\"Found {invalid_hs_count} records with invalid 4-digit HS codes \"\n",
        "            f\"(outside the range 0101-9999).\"\n",
        "        )\n",
        "\n",
        "# =============================================================================\n",
        "# Task 1: Orchestrator Function\n",
        "# =============================================================================\n",
        "\n",
        "def validate_and_assess_inputs(\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    firm_metadata_frame: pd.DataFrame,\n",
        "    comtrade_exports_frame: pd.DataFrame,\n",
        "    country_data_frame: pd.DataFrame,\n",
        "    replication_manifest: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of all data validation and quality assurance tasks.\n",
        "\n",
        "    This function serves as the single entry point for Task 1, sequentially\n",
        "    calling all validation and assessment functions. It will raise a\n",
        "    DataValidationError if any critical check fails, or issue UserWarnings for\n",
        "    non-critical issues.\n",
        "\n",
        "    Args:\n",
        "        transactions_log_frame (pd.DataFrame): Log of firm transactions.\n",
        "        firm_metadata_frame (pd.DataFrame): Firm ownership and location data.\n",
        "        comtrade_exports_frame (pd.DataFrame): Country-product export data.\n",
        "        country_data_frame (pd.DataFrame): Country population data.\n",
        "        replication_manifest (Dict[str, Any]): Dictionary of study parameters.\n",
        "\n",
        "    Raises:\n",
        "        DataValidationError: If any validation step fails.\n",
        "    \"\"\"\n",
        "    # Set up basic logging configuration.\n",
        "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    # Log the start of the validation process.\n",
        "    logging.info(\"Starting Task 1: Data Validation and Quality Assurance...\")\n",
        "\n",
        "    # --- Step 1.1: Input Parameter Validation ---\n",
        "    logging.info(\"Step 1.1: Validating 'replication_manifest' parameters...\")\n",
        "    # Validate the overall dictionary structure.\n",
        "    _validate_manifest_structure(manifest=replication_manifest)\n",
        "    # Extract the parameters sub-dictionary for convenience.\n",
        "    params = replication_manifest['parameters']\n",
        "    # Validate date parameters for format and logical order.\n",
        "    _validate_manifest_date_parameters(params=params)\n",
        "    # Validate numerical thresholds for type and positivity constraints.\n",
        "    _validate_manifest_numerical_thresholds(params=params)\n",
        "    logging.info(\"...Manifest parameters validated successfully.\")\n",
        "\n",
        "    # --- Step 1.2: DataFrame Structure Validation ---\n",
        "    logging.info(\"Step 1.2: Validating DataFrame schemas (columns and dtypes)...\")\n",
        "    # Validate the structure of all input DataFrames.\n",
        "    _validate_dataframe_schemas(\n",
        "        transactions_log_frame=transactions_log_frame,\n",
        "        firm_metadata_frame=firm_metadata_frame,\n",
        "        comtrade_exports_frame=comtrade_exports_frame,\n",
        "        country_data_frame=country_data_frame\n",
        "    )\n",
        "    logging.info(\"...DataFrame schemas validated successfully.\")\n",
        "\n",
        "    # --- Step 1.3: Data Completeness Assessment ---\n",
        "    logging.info(\"Step 1.3: Assessing data completeness and coverage...\")\n",
        "    # Assess missing values in critical transaction columns.\n",
        "    _assess_missing_values(transactions_log_frame=transactions_log_frame)\n",
        "    # Assess the completeness of the firm ownership data.\n",
        "    _assess_ownership_completeness(firm_metadata_frame=firm_metadata_frame)\n",
        "    # Assess temporal and HS code coverage in the transaction data.\n",
        "    _assess_temporal_and_hs_code_coverage(\n",
        "        transactions_log_frame=transactions_log_frame,\n",
        "        params=params\n",
        "    )\n",
        "    logging.info(\"...Data completeness assessment finished.\")\n",
        "\n",
        "    # Log the successful completion of the entire task.\n",
        "    logging.info(\"Task 1 successfully completed. All inputs are validated.\")\n"
      ],
      "metadata": {
        "id": "Mha-gtMlDZik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Data Preprocessing and Cleansing\n",
        "\n",
        "# =============================================================================\n",
        "# Task 2.1: Temporal and Value Filtering\n",
        "# =============================================================================\n",
        "\n",
        "def _filter_transactions_by_date_and_value(\n",
        "    transactions_df: pd.DataFrame,\n",
        "    params: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters transactions based on a specified date range and positive value.\n",
        "\n",
        "    This function applies two critical filters to the raw transaction log:\n",
        "    1. Temporal Filter: Retains only transactions within the study period\n",
        "       defined in the manifest (e.g., 2021-01-01 to 2023-12-31).\n",
        "    2. Value Filter: Removes transactions with a non-positive monetary value,\n",
        "       as these are considered invalid for economic analysis.\n",
        "\n",
        "    Args:\n",
        "        transactions_df (pd.DataFrame): The input transactions log.\n",
        "        params (Dict[str, Any]): The 'parameters' sub-dictionary from the\n",
        "                                 replication manifest.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new DataFrame containing only the filtered transactions.\n",
        "    \"\"\"\n",
        "    # Log the initial number of records for audit purposes.\n",
        "    initial_record_count = len(transactions_df)\n",
        "    logging.info(f\"Initial transaction records: {initial_record_count:,}\")\n",
        "\n",
        "    # --- Temporal Filtering ---\n",
        "    # Extract start and end dates from the parameters.\n",
        "    start_date = pd.to_datetime(params['data_ingestion']['start_date'])\n",
        "    end_date = pd.to_datetime(params['data_ingestion']['end_date'])\n",
        "\n",
        "    # Create a boolean mask for transactions within the valid date range.\n",
        "    # The comparison is inclusive of the start and end dates.\n",
        "    date_mask = (\n",
        "        (transactions_df['timestamp'] >= start_date) &\n",
        "        (transactions_df['timestamp'] <= end_date)\n",
        "    )\n",
        "\n",
        "    # --- Value Filtering ---\n",
        "    # Create a boolean mask for transactions with a positive monetary value.\n",
        "    value_mask = transactions_df['transaction_value_usd'] > 0\n",
        "\n",
        "    # Combine the masks into a single filter for efficiency.\n",
        "    combined_mask = date_mask & value_mask\n",
        "\n",
        "    # Apply the combined mask to the DataFrame.\n",
        "    # Using .copy() is crucial to prevent SettingWithCopyWarning downstream.\n",
        "    filtered_df = transactions_df[combined_mask].copy()\n",
        "\n",
        "    # Log the number of records after filtering.\n",
        "    final_record_count = len(filtered_df)\n",
        "    records_removed = initial_record_count - final_record_count\n",
        "    logging.info(\n",
        "        f\"Removed {records_removed:,} records due to date or invalid value. \"\n",
        "        f\"Remaining records: {final_record_count:,}\"\n",
        "    )\n",
        "\n",
        "    # Return the new, filtered DataFrame.\n",
        "    return filtered_df\n",
        "\n",
        "# =============================================================================\n",
        "# Task 2.2: Firm Entity Resolution\n",
        "# =============================================================================\n",
        "\n",
        "def _resolve_firm_entities(\n",
        "    transactions_df: pd.DataFrame,\n",
        "    firm_metadata_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Resolves firm IDs to ultimate owners and creates owner-country entities.\n",
        "\n",
        "    This function executes a critical transformation by moving from the raw\n",
        "    `firm_id` to a more analytically relevant \"owner-country entity\".\n",
        "    The process involves:\n",
        "    1. Mapping each transacting firm to its ultimate owner. If an owner is\n",
        "       not specified, the firm is treated as its own ultimate owner.\n",
        "    2. Augmenting the transaction log with the country of domicile for both\n",
        "       the seller's and buyer's ultimate owner.\n",
        "    3. Creating a composite string identifier ('owner_id_country_iso') for\n",
        "       both seller and buyer, which serves as the primary entity identifier\n",
        "       for the rest of the analysis.\n",
        "\n",
        "    Args:\n",
        "        transactions_df (pd.DataFrame): The transaction log DataFrame.\n",
        "        firm_metadata_df (pd.DataFrame): The firm metadata DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: An augmented DataFrame with resolved owner-country\n",
        "                      entities for both seller and buyer.\n",
        "\n",
        "    Raises:\n",
        "        DataValidationError: If any firm ID in the transaction log cannot be\n",
        "                             found in the firm metadata.\n",
        "    \"\"\"\n",
        "    # --- Step 2.2.1 & 2.2.2: Ultimate Owner Resolution ---\n",
        "    # Create a working copy of the metadata to avoid modifying the original.\n",
        "    metadata = firm_metadata_df.copy()\n",
        "\n",
        "    # Per the paper's methodology, fill missing ultimate owner IDs with the\n",
        "    # firm's own ID, treating them as self-owned.\n",
        "    metadata['ultimate_owner_id'] = metadata['ultimate_owner_id'].fillna(\n",
        "        metadata['firm_id']\n",
        "    )\n",
        "\n",
        "    # Create an efficient mapping from firm_id to ultimate_owner_id.\n",
        "    # A dictionary is faster for mapping than repeated merges.\n",
        "    owner_map = metadata.set_index('firm_id')['ultimate_owner_id'].to_dict()\n",
        "\n",
        "    # Create a working copy of the transactions DataFrame.\n",
        "    resolved_df = transactions_df.copy()\n",
        "\n",
        "    # Map seller and buyer firm IDs to their ultimate owner IDs.\n",
        "    resolved_df['seller_owner_id'] = resolved_df['seller_firm_id'].map(owner_map)\n",
        "    resolved_df['buyer_owner_id'] = resolved_df['buyer_firm_id'].map(owner_map)\n",
        "\n",
        "    # Validate that all mappings were successful.\n",
        "    if resolved_df[['seller_owner_id', 'buyer_owner_id']].isnull().any().any():\n",
        "        raise DataValidationError(\n",
        "            \"Entity resolution failed: Some firm IDs in the transaction log \"\n",
        "            \"could not be mapped to an ultimate owner. Check for missing \"\n",
        "            \"firms in the metadata.\"\n",
        "        )\n",
        "\n",
        "    # --- Step 2.2.3: Owner-Country Composite Entity Creation ---\n",
        "    # Create a minimal mapping from firm_id to country_domicile for merging.\n",
        "    # This is more memory-efficient than merging the full metadata table.\n",
        "    country_map_df = metadata[['firm_id', 'country_domicile']].drop_duplicates()\n",
        "\n",
        "    # Merge to get the seller's country of domicile.\n",
        "    resolved_df = pd.merge(\n",
        "        resolved_df,\n",
        "        country_map_df,\n",
        "        left_on='seller_firm_id',\n",
        "        right_on='firm_id',\n",
        "        how='left'\n",
        "    )\n",
        "    resolved_df.rename(columns={'country_domicile': 'seller_country'}, inplace=True)\n",
        "\n",
        "    # Merge to get the buyer's country of domicile.\n",
        "    resolved_df = pd.merge(\n",
        "        resolved_df,\n",
        "        country_map_df,\n",
        "        left_on='buyer_firm_id',\n",
        "        right_on='firm_id',\n",
        "        how='left',\n",
        "        suffixes=('_seller_map', '_buyer_map') # Suffixes not strictly needed here\n",
        "    )\n",
        "    resolved_df.rename(columns={'country_domicile': 'buyer_country'}, inplace=True)\n",
        "\n",
        "    # Validate that all country lookups were successful.\n",
        "    if resolved_df[['seller_country', 'buyer_country']].isnull().any().any():\n",
        "        raise DataValidationError(\n",
        "            \"Country resolution failed: Could not find country domicile for \"\n",
        "            \"all firms in the transaction log.\"\n",
        "        )\n",
        "\n",
        "    # Create the composite owner-country entity identifiers.\n",
        "    # Ensure IDs are strings for concatenation.\n",
        "    resolved_df['seller_owner_country_entity'] = (\n",
        "        resolved_df['seller_owner_id'].astype(str) + '_' +\n",
        "        resolved_df['seller_country'].astype(str)\n",
        "    )\n",
        "    resolved_df['buyer_owner_country_entity'] = (\n",
        "        resolved_df['buyer_owner_id'].astype(str) + '_' +\n",
        "        resolved_df['buyer_country'].astype(str)\n",
        "    )\n",
        "\n",
        "    # Clean up intermediate columns.\n",
        "    resolved_df.drop(\n",
        "        columns=[\n",
        "            'firm_id_seller_map', 'firm_id_buyer_map', 'firm_id'\n",
        "        ],\n",
        "        inplace=True,\n",
        "        errors='ignore' # Use ignore in case column names vary slightly\n",
        "    )\n",
        "\n",
        "    logging.info(\"Successfully resolved firm IDs to owner-country entities.\")\n",
        "    return resolved_df\n",
        "\n",
        "# =============================================================================\n",
        "# Task 2.3: Cross-Border and Multi-Product Firm Filtering\n",
        "# =============================================================================\n",
        "\n",
        "def _filter_by_transaction_type_and_firm_diversity(\n",
        "    resolved_df: pd.DataFrame,\n",
        "    params: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters for cross-border transactions and removes highly diversified firms.\n",
        "\n",
        "    This function applies the final two cleansing steps:\n",
        "    1. Cross-Border Filter: Removes all transactions where the seller and\n",
        "       buyer entities operate in the same country.\n",
        "    2. Multi-Product Firm Filter: Removes all transactions where the selling\n",
        "       entity exports products across a large number of distinct HS 2-digit\n",
        "       sections, as these are likely wholesalers or conglomerates not\n",
        "       specialized in production.\n",
        "\n",
        "    Args:\n",
        "        resolved_df (pd.DataFrame): The DataFrame with resolved owner-country\n",
        "                                    entities.\n",
        "        params (Dict[str, Any]): The 'parameters' sub-dictionary from the\n",
        "                                 replication manifest.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A new, fully cleansed DataFrame ready for analysis.\n",
        "    \"\"\"\n",
        "    # Log the record count before this filtering stage.\n",
        "    initial_record_count = len(resolved_df)\n",
        "\n",
        "    # --- Step 2.3.2: Cross-Border Transaction Filtering ---\n",
        "    # Create a boolean mask to identify cross-border transactions.\n",
        "    cross_border_mask = (\n",
        "        resolved_df['seller_country'] != resolved_df['buyer_country']\n",
        "    )\n",
        "    # Apply the mask.\n",
        "    cross_border_df = resolved_df[cross_border_mask].copy()\n",
        "\n",
        "    # Log the number of domestic transactions removed.\n",
        "    domestic_removed = initial_record_count - len(cross_border_df)\n",
        "    logging.info(\n",
        "        f\"Removed {domestic_removed:,} domestic transactions. \"\n",
        "        f\"Remaining records: {len(cross_border_df):,}\"\n",
        "    )\n",
        "\n",
        "    # --- Step 2.3.3: Multi-Product Firm Filtering ---\n",
        "    # Get the diversity threshold from the parameters.\n",
        "    threshold = params['network_inference']['multi_product_firm_hs2_threshold']\n",
        "\n",
        "    # Calculate the HS 2-digit section for each product.\n",
        "    # Integer division of the 4-digit code by 100 yields the 2-digit section.\n",
        "    cross_border_df['hs2_section'] = cross_border_df['product_hs_code'] // 100\n",
        "\n",
        "    # Group by the selling entity and count the number of unique HS2 sections.\n",
        "    seller_diversity = cross_border_df.groupby(\n",
        "        'seller_owner_country_entity'\n",
        "    )['hs2_section'].nunique()\n",
        "\n",
        "    # Identify the entities that exceed the diversity threshold.\n",
        "    diversified_sellers = seller_diversity[seller_diversity >= threshold].index\n",
        "\n",
        "    # Create a boolean mask to filter out transactions from these sellers.\n",
        "    # The .isin() method is highly efficient for this type of filtering.\n",
        "    multi_product_mask = ~cross_border_df['seller_owner_country_entity'].isin(\n",
        "        diversified_sellers\n",
        "    )\n",
        "\n",
        "    # Apply the mask to get the final cleansed DataFrame.\n",
        "    cleansed_df = cross_border_df[multi_product_mask].copy()\n",
        "\n",
        "    # Log the number of records removed due to firm diversity.\n",
        "    multi_product_removed = len(cross_border_df) - len(cleansed_df)\n",
        "    logging.info(\n",
        "        f\"Removed {len(diversified_sellers):,} highly diversified seller entities, \"\n",
        "        f\"resulting in the removal of {multi_product_removed:,} transactions.\"\n",
        "    )\n",
        "    logging.info(f\"Final cleansed transaction records: {len(cleansed_df):,}\")\n",
        "\n",
        "    # Return the final DataFrame, dropping the temporary hs2_section column.\n",
        "    return cleansed_df.drop(columns=['hs2_section'])\n",
        "\n",
        "# =============================================================================\n",
        "# Task 2: Orchestrator Function\n",
        "# =============================================================================\n",
        "\n",
        "def preprocess_and_cleanse_data(\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    firm_metadata_frame: pd.DataFrame,\n",
        "    replication_manifest: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end data preprocessing and cleansing pipeline.\n",
        "\n",
        "    This function executes the full sequence of cleansing and transformation\n",
        "    steps required to convert the raw, validated input data into an\n",
        "    analysis-ready format. It chains together the filtering, entity\n",
        "    resolution, and business rule application functions.\n",
        "\n",
        "    Args:\n",
        "        transactions_log_frame (pd.DataFrame): The validated raw transaction log.\n",
        "        firm_metadata_frame (pd.DataFrame): The validated firm metadata.\n",
        "        replication_manifest (Dict[str, Any]): The validated study parameters.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The fully preprocessed and cleansed transaction log,\n",
        "                      ready for network inference.\n",
        "    \"\"\"\n",
        "    # Log the start of the preprocessing task.\n",
        "    logging.info(\"Starting Task 2: Data Preprocessing and Cleansing...\")\n",
        "\n",
        "    # Extract the parameters sub-dictionary for use in helper functions.\n",
        "    params = replication_manifest['parameters']\n",
        "\n",
        "    # Step 2.1: Apply temporal and transaction value filters.\n",
        "    logging.info(\"Step 2.1: Applying temporal and value filters...\")\n",
        "    filtered_df = _filter_transactions_by_date_and_value(\n",
        "        transactions_df=transactions_log_frame,\n",
        "        params=params\n",
        "    )\n",
        "\n",
        "    # Step 2.2: Resolve firm IDs to ultimate owners and create composite entities.\n",
        "    logging.info(\"Step 2.2: Resolving firm entities...\")\n",
        "    resolved_df = _resolve_firm_entities(\n",
        "        transactions_df=filtered_df,\n",
        "        firm_metadata_df=firm_metadata_frame\n",
        "    )\n",
        "\n",
        "    # Step 2.3: Apply cross-border and multi-product firm filters.\n",
        "    logging.info(\"Step 2.3: Applying cross-border and firm diversity filters...\")\n",
        "    cleansed_df = _filter_by_transaction_type_and_firm_diversity(\n",
        "        resolved_df=resolved_df,\n",
        "        params=params\n",
        "    )\n",
        "\n",
        "    # Log the successful completion of the task.\n",
        "    logging.info(\"Task 2 successfully completed. Data is cleansed and ready for analysis.\")\n",
        "\n",
        "    # Return the final, analysis-ready DataFrame.\n",
        "    return cleansed_df\n"
      ],
      "metadata": {
        "id": "KrBSlzL3DgI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Firm Classification and Set Construction\n",
        "\n",
        "# =============================================================================\n",
        "# Task 3.1 & 3.2: Producer and Purchaser Set Identification\n",
        "# =============================================================================\n",
        "\n",
        "def _identify_significant_entities(\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    entity_type: str\n",
        ") -> Dict[int, Set[str]]:\n",
        "    \"\"\"\n",
        "    Identifies significant producers or purchasers for each product.\n",
        "\n",
        "    This function implements the core classification methodology from the paper.\n",
        "    An entity is classified as a \"significant\" producer (or purchaser) of a\n",
        "    product if its total sales (or purchase) value for that product exceeds\n",
        "    the average total sales (or purchase) value across all entities active\n",
        "    in that product market.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df (pd.DataFrame): The preprocessed and cleansed transaction log.\n",
        "        entity_type (str): The type of entity to identify. Must be either\n",
        "                           'producer' or 'purchaser'.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, Set[str]]: A dictionary mapping each product HS code (int) to\n",
        "                             a set of significant entity identifiers (str).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If entity_type is not 'producer' or 'purchaser'.\n",
        "    \"\"\"\n",
        "    # Validate the entity_type parameter to ensure correct column selection.\n",
        "    if entity_type == 'producer':\n",
        "        # For producers, the relevant entity is the seller.\n",
        "        entity_col = 'seller_owner_country_entity'\n",
        "    elif entity_type == 'purchaser':\n",
        "        # For purchasers, the relevant entity is the buyer.\n",
        "        entity_col = 'buyer_owner_country_entity'\n",
        "    else:\n",
        "        # Raise an error for invalid entity types.\n",
        "        raise ValueError(\"entity_type must be either 'producer' or 'purchaser'.\")\n",
        "\n",
        "    logging.info(f\"Identifying significant {entity_type}s...\")\n",
        "\n",
        "    # --- Step 1: Calculate total transaction value per (entity, product) pair.\n",
        "    # Group by the product and the relevant entity column, then sum the values.\n",
        "    entity_product_values = cleansed_df.groupby(\n",
        "        ['product_hs_code', entity_col]\n",
        "    )['transaction_value_usd'].sum().reset_index()\n",
        "    entity_product_values.rename(\n",
        "        columns={'transaction_value_usd': 'total_value'}, inplace=True\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Calculate the average total value threshold for each product.\n",
        "    # We use .transform('mean') which is highly efficient. It calculates the\n",
        "    # mean for each product group and broadcasts this value back to the original\n",
        "    # shape of the grouped data, avoiding a costly merge operation.\n",
        "    entity_product_values['avg_total_value'] = entity_product_values.groupby(\n",
        "        'product_hs_code'\n",
        "    )['total_value'].transform('mean')\n",
        "\n",
        "    # --- Step 3: Filter for entities that exceed the average threshold.\n",
        "    # This boolean mask identifies the significant entities for each product.\n",
        "    significant_mask = (\n",
        "        entity_product_values['total_value'] >\n",
        "        entity_product_values['avg_total_value']\n",
        "    )\n",
        "    significant_entities_df = entity_product_values[significant_mask]\n",
        "\n",
        "    # --- Step 4: Aggregate the entities into sets for each product.\n",
        "    # Group the filtered DataFrame by product and apply the `set` constructor\n",
        "    # to the entity column to collect all significant entities for that product.\n",
        "    entity_sets_series = significant_entities_df.groupby(\n",
        "        'product_hs_code'\n",
        "    )[entity_col].apply(set)\n",
        "\n",
        "    # Convert the resulting Series to a dictionary for fast lookups.\n",
        "    entity_sets_dict = entity_sets_series.to_dict()\n",
        "\n",
        "    logging.info(\n",
        "        f\"Identified significant {entity_type}s for \"\n",
        "        f\"{len(entity_sets_dict):,} products.\"\n",
        "    )\n",
        "\n",
        "    # Return the final dictionary of sets.\n",
        "    return entity_sets_dict\n",
        "\n",
        "# =============================================================================\n",
        "# Task 3.3: Intersection Set Construction and Value Aggregation\n",
        "# =============================================================================\n",
        "\n",
        "def _compute_intersection_metrics(\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    producer_sets: Dict[int, Set[str]],\n",
        "    purchaser_sets: Dict[int, Set[str]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes intersection set sizes and their total transaction values.\n",
        "\n",
        "    This function calculates the two key metrics for each potential\n",
        "    input-output product link (i, j) required for network inference:\n",
        "    1.  |S_i^j|: The number of firms that are both significant producers of\n",
        "        product j AND significant purchasers of product i.\n",
        "    2.  Value(S_i^j): The total monetary value of product i purchased by the\n",
        "        firms in the intersection set S_i^j.\n",
        "\n",
        "    It uses an efficient, vectorized approach by creating tables of all\n",
        "    products produced and purchased by each firm and then merging them.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df (pd.DataFrame): The preprocessed transaction log.\n",
        "        producer_sets (Dict[int, Set[str]]): A dictionary mapping product codes\n",
        "                                             to sets of producer entities.\n",
        "        purchaser_sets (Dict[int, Set[str]]): A dictionary mapping product codes\n",
        "                                              to sets of purchaser entities.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with a MultiIndex (input_product_i,\n",
        "                      output_product_j) and columns 'intersection_size' and\n",
        "                      'intersection_value'.\n",
        "    \"\"\"\n",
        "    logging.info(\"Computing intersection metrics for all product pairs...\")\n",
        "\n",
        "    # --- Step 1: Create long-format DataFrames of (entity, product) pairs ---\n",
        "    # This \"un-nests\" the dictionaries into a format suitable for merging.\n",
        "    producers_long = pd.DataFrame(\n",
        "        [\n",
        "            (entity, prod)\n",
        "            for prod, entities in producer_sets.items()\n",
        "            for entity in entities\n",
        "        ],\n",
        "        columns=['entity', 'output_product_j']\n",
        "    )\n",
        "\n",
        "    purchasers_long = pd.DataFrame(\n",
        "        [\n",
        "            (entity, prod)\n",
        "            for prod, entities in purchaser_sets.items()\n",
        "            for entity in entities\n",
        "        ],\n",
        "        columns=['entity', 'input_product_i']\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Merge to find entities in both producer and purchaser sets ---\n",
        "    # This self-merge on 'entity' is the vectorized equivalent of finding all\n",
        "    # intersection sets S_i^j. The result contains every (entity, i, j) triplet.\n",
        "    intersections_df = pd.merge(\n",
        "        purchasers_long, producers_long, on='entity'\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Calculate intersection sizes ---\n",
        "    # Group by the product pair (i, j) and count the number of unique entities.\n",
        "    # This directly calculates |S_i^j| for all pairs.\n",
        "    intersection_sizes = intersections_df.groupby(\n",
        "        ['input_product_i', 'output_product_j']\n",
        "    )['entity'].nunique().to_frame(name='intersection_size')\n",
        "\n",
        "    # --- Step 4: Calculate intersection transaction values ---\n",
        "    # To get the value, we need to link back to the original transactions.\n",
        "    # We merge the triplets with the transaction log.\n",
        "    # The merge keys are the buyer entity and the input product.\n",
        "    value_agg_df = pd.merge(\n",
        "        cleansed_df,\n",
        "        intersections_df,\n",
        "        left_on=['buyer_owner_country_entity', 'product_hs_code'],\n",
        "        right_on=['entity', 'input_product_i'],\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    # Now, group by the product pair (i, j) and sum the transaction values.\n",
        "    intersection_values = value_agg_df.groupby(\n",
        "        ['input_product_i', 'output_product_j']\n",
        "    )['transaction_value_usd'].sum().to_frame(name='intersection_value')\n",
        "\n",
        "    # --- Step 5: Combine size and value metrics ---\n",
        "    # Join the two resulting DataFrames to create the final output.\n",
        "    # We use an outer join to ensure we don't lose any pairs, though in\n",
        "    # practice they should have the same index. Fill any potential NaNs with 0.\n",
        "    final_metrics_df = intersection_sizes.join(\n",
        "        intersection_values, how='outer'\n",
        "    ).fillna(0)\n",
        "\n",
        "    logging.info(\n",
        "        f\"Computed metrics for {len(final_metrics_df):,} unique \"\n",
        "        f\"(input, output) product links.\"\n",
        "    )\n",
        "\n",
        "    return final_metrics_df\n",
        "\n",
        "# =============================================================================\n",
        "# Task 3: Orchestrator Function\n",
        "# =============================================================================\n",
        "\n",
        "def classify_firms_and_construct_sets(\n",
        "    cleansed_df: pd.DataFrame\n",
        ") -> Tuple[Dict[int, Set[str]], Dict[int, Set[str]], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the firm classification and set construction pipeline.\n",
        "\n",
        "    This function takes the cleansed transaction data and performs the\n",
        "    necessary aggregations and transformations to produce the core data\n",
        "    structures needed for the network inference algorithm in Task 4.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df (pd.DataFrame): The fully preprocessed and cleansed\n",
        "                                    transaction log from Task 2.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[int, Set[str]], Dict[int, Set[str]], pd.DataFrame]:\n",
        "        A tuple containing:\n",
        "        - producer_sets: A dictionary mapping product codes to sets of\n",
        "                         significant producer entities.\n",
        "        - purchaser_sets: A dictionary mapping product codes to sets of\n",
        "                          significant purchaser entities.\n",
        "        - intersection_metrics: A DataFrame containing the size and total\n",
        "                                transaction value for each product-pair\n",
        "                                intersection set.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting Task 3: Firm Classification and Set Construction...\")\n",
        "\n",
        "    # Step 3.1: Identify significant producer entities for each product.\n",
        "    producer_sets = _identify_significant_entities(\n",
        "        cleansed_df=cleansed_df,\n",
        "        entity_type='producer'\n",
        "    )\n",
        "\n",
        "    # Step 3.2: Identify significant purchaser entities for each product.\n",
        "    purchaser_sets = _identify_significant_entities(\n",
        "        cleansed_df=cleansed_df,\n",
        "        entity_type='purchaser'\n",
        "    )\n",
        "\n",
        "    # Step 3.3: Compute intersection metrics (size and value).\n",
        "    intersection_metrics = _compute_intersection_metrics(\n",
        "        cleansed_df=cleansed_df,\n",
        "        producer_sets=producer_sets,\n",
        "        purchaser_sets=purchaser_sets\n",
        "    )\n",
        "\n",
        "    logging.info(\"Task 3 successfully completed. All sets constructed.\")\n",
        "\n",
        "    # Return the three essential data structures for the next task.\n",
        "    return producer_sets, purchaser_sets, intersection_metrics\n"
      ],
      "metadata": {
        "id": "x8G6AHK9GE7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Network Inference Implementation\n",
        "\n",
        "# =============================================================================\n",
        "# Task 4.1: Adjacency Matrix Computation\n",
        "# =============================================================================\n",
        "\n",
        "def _compute_adjacency_weights(\n",
        "    producer_sets: Dict[int, Set[str]],\n",
        "    purchaser_sets: Dict[int, Set[str]],\n",
        "    intersection_metrics: pd.DataFrame,\n",
        "    total_unique_entities: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes the raw adjacency matrix weights (A_ij) using the core formula.\n",
        "\n",
        "    This function implements the central equation of the paper in a fully\n",
        "    vectorized manner:\n",
        "    A_ij = ( |S_i^j| / |S_j| ) / ( |S_i^†| / |S| )\n",
        "    where:\n",
        "    - |S_i^j|: Number of firms producing j and purchasing i.\n",
        "    - |S_j|: Total number of firms producing j.\n",
        "    - |S_i^†|: Total number of firms purchasing i.\n",
        "    - |S|: Total number of unique firms in the dataset.\n",
        "\n",
        "    Args:\n",
        "        producer_sets (Dict[int, Set[str]]): Map from product code to the set\n",
        "                                             of its producer entities.\n",
        "        purchaser_sets (Dict[int, Set[str]]): Map from product code to the set\n",
        "                                              of its purchaser entities.\n",
        "        intersection_metrics (pd.DataFrame): DataFrame with MultiIndex\n",
        "                                             (input_product_i, output_product_j)\n",
        "                                             and a column 'intersection_size'.\n",
        "        total_unique_entities (int): The total number of unique owner-country\n",
        "                                     entities in the cleansed dataset (|S|).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The intersection_metrics DataFrame augmented with a\n",
        "                      new column 'A_ij' containing the computed weights.\n",
        "    \"\"\"\n",
        "    logging.info(\"Computing raw adjacency matrix weights (A_ij)...\")\n",
        "\n",
        "    # Create a working copy to avoid modifying the original DataFrame.\n",
        "    adj_df = intersection_metrics.copy()\n",
        "\n",
        "    # --- Calculate each term of the formula as a pandas Series ---\n",
        "    # |S_j|: Cardinality of each producer set.\n",
        "    s_j_sizes = pd.Series(\n",
        "        {prod: len(entities) for prod, entities in producer_sets.items()},\n",
        "        name='s_j_size'\n",
        "    )\n",
        "\n",
        "    # |S_i^†|: Cardinality of each purchaser set.\n",
        "    s_i_dagger_sizes = pd.Series(\n",
        "        {prod: len(entities) for prod, entities in purchaser_sets.items()},\n",
        "        name='s_i_dagger_size'\n",
        "    )\n",
        "\n",
        "    # --- Align sizes with the intersection DataFrame using its index ---\n",
        "    # Map the producer set sizes to the 'output_product_j' level of the index.\n",
        "    adj_df['s_j_size'] = adj_df.index.get_level_values('output_product_j').map(s_j_sizes)\n",
        "\n",
        "    # Map the purchaser set sizes to the 'input_product_i' level of the index.\n",
        "    adj_df['s_i_dagger_size'] = adj_df.index.get_level_values('input_product_i').map(s_i_dagger_sizes)\n",
        "\n",
        "    # --- Compute the numerator and denominator of the formula ---\n",
        "    # Numerator: (|S_i^j| / |S_j|)\n",
        "    # This is the proportion of producers of j that also purchase i.\n",
        "    numerator = adj_df['intersection_size'] / adj_df['s_j_size']\n",
        "\n",
        "    # Denominator: (|S_i^†| / |S|)\n",
        "    # This is the baseline proportion of all firms that purchase i.\n",
        "    denominator = adj_df['s_i_dagger_size'] / total_unique_entities\n",
        "\n",
        "    # --- Compute A_ij and handle potential division by zero ---\n",
        "    # The division of the two proportions gives the final weight.\n",
        "    adj_df['A_ij'] = numerator / denominator\n",
        "\n",
        "    # Handle edge cases: division by zero can result in `inf` or `NaN`.\n",
        "    # These cases are not meaningful links, so their weight should be 0.\n",
        "    adj_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    adj_df['A_ij'].fillna(0, inplace=True)\n",
        "\n",
        "    logging.info(\"Successfully computed raw A_ij weights.\")\n",
        "\n",
        "    # Return the DataFrame with the new 'A_ij' column.\n",
        "    return adj_df.drop(columns=['s_j_size', 's_i_dagger_size'])\n",
        "\n",
        "# =============================================================================\n",
        "# Task 4.2: Network Sparsification\n",
        "# =============================================================================\n",
        "\n",
        "def _sparsify_network_edges(\n",
        "    weighted_edges_df: pd.DataFrame,\n",
        "    params: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies multiple filters to sparsify the network, retaining strong links.\n",
        "\n",
        "    Args:\n",
        "        weighted_edges_df (pd.DataFrame): DataFrame of all potential edges with\n",
        "                                          their computed A_ij weights and\n",
        "                                          intersection metrics.\n",
        "        params (Dict[str, Any]): The 'parameters' sub-dictionary from the\n",
        "                                 replication manifest.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A filtered DataFrame containing only the edges that\n",
        "                      meet all specified criteria.\n",
        "    \"\"\"\n",
        "    logging.info(\"Sparsifying network based on thresholds...\")\n",
        "    initial_edge_count = len(weighted_edges_df)\n",
        "\n",
        "    # --- Step 4.2.1: Apply firmcount threshold ---\n",
        "    # Filter based on the minimum number of firms supporting a link.\n",
        "    firmcount_threshold = params['network_inference']['primary_firmcount_threshold']\n",
        "    firmcount_mask = weighted_edges_df['intersection_size'] >= firmcount_threshold\n",
        "\n",
        "    # --- Step 4.2.2: Apply edge weight threshold ---\n",
        "    # Filter based on the minimum A_ij weight.\n",
        "    edge_weight_threshold = params['network_inference']['primary_edge_weight_threshold']\n",
        "    weight_mask = weighted_edges_df['A_ij'] >= edge_weight_threshold\n",
        "\n",
        "    # --- Step 4.2.3: Apply minimum transaction value filter ---\n",
        "    # Filter based on the minimum total monetary value of the link.\n",
        "    value_threshold = params['network_inference']['min_aggregated_link_value_usd']\n",
        "    value_mask = weighted_edges_df['intersection_value'] >= value_threshold\n",
        "\n",
        "    # Combine all masks into a single filter.\n",
        "    final_mask = firmcount_mask & weight_mask & value_mask\n",
        "\n",
        "    # Apply the final mask to get the sparsified set of edges.\n",
        "    sparsified_df = weighted_edges_df[final_mask].copy()\n",
        "\n",
        "    final_edge_count = len(sparsified_df)\n",
        "    edges_removed = initial_edge_count - final_edge_count\n",
        "    logging.info(\n",
        "        f\"Sparsification complete. Removed {edges_removed:,} edges. \"\n",
        "        f\"Final network has {final_edge_count:,} edges.\"\n",
        "    )\n",
        "\n",
        "    return sparsified_df\n",
        "\n",
        "# =============================================================================\n",
        "# Task 4.3: Network Graph Construction\n",
        "# =============================================================================\n",
        "\n",
        "def _construct_network_objects(\n",
        "    final_edges_df: pd.DataFrame,\n",
        "    all_products: List[int]\n",
        ") -> Tuple[csr_matrix, nx.DiGraph, Dict[int, int]]:\n",
        "    \"\"\"\n",
        "    Constructs the final network representations: a sparse matrix and a DiGraph.\n",
        "\n",
        "    Args:\n",
        "        final_edges_df (pd.DataFrame): The filtered DataFrame of network edges.\n",
        "        all_products (List[int]): A sorted list of all unique product codes\n",
        "                                  that will form the nodes of the network.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[csr_matrix, nx.DiGraph, Dict[int, int]]: A tuple containing:\n",
        "        - adj_matrix: The n x n weighted adjacency matrix in CSR format.\n",
        "        - graph: The networkx DiGraph object representing the network.\n",
        "        - product_to_idx: A dictionary mapping product HS codes to matrix indices.\n",
        "    \"\"\"\n",
        "    logging.info(\"Constructing final network objects (sparse matrix and graph)...\")\n",
        "\n",
        "    # --- Step 1: Create the product-to-index mapping ---\n",
        "    # This map is the single source of truth for node indexing.\n",
        "    n_products = len(all_products)\n",
        "    product_to_idx = {product: i for i, product in enumerate(all_products)}\n",
        "\n",
        "    # --- Step 2: Prepare edge data for matrix construction ---\n",
        "    # Reset index to access product codes as columns.\n",
        "    edges = final_edges_df.reset_index()\n",
        "\n",
        "    # Map product codes to their integer indices.\n",
        "    row_indices = edges['input_product_i'].map(product_to_idx)\n",
        "    col_indices = edges['output_product_j'].map(product_to_idx)\n",
        "\n",
        "    # The data for the sparse matrix is the final A_ij weight.\n",
        "    edge_weights = edges['A_ij']\n",
        "\n",
        "    # --- Step 3: Construct the sparse adjacency matrix ---\n",
        "    # Create the matrix in Coordinate (COO) format and then convert to\n",
        "    # Compressed Sparse Row (CSR) format, which is efficient for analysis.\n",
        "    adj_matrix = csr_matrix(\n",
        "        (edge_weights, (row_indices, col_indices)),\n",
        "        shape=(n_products, n_products)\n",
        "    )\n",
        "\n",
        "    # --- Step 4: Construct the networkx DiGraph object ---\n",
        "    # Create an empty directed graph.\n",
        "    graph = nx.DiGraph()\n",
        "\n",
        "    # Add all products as nodes to ensure nodes without edges are included.\n",
        "    graph.add_nodes_from(all_products)\n",
        "\n",
        "    # Add the weighted, directed edges from the filtered DataFrame.\n",
        "    # Create a list of tuples for efficient bulk edge addition.\n",
        "    edge_tuples = [\n",
        "        (row['input_product_i'], row['output_product_j'], {'weight': row['A_ij']})\n",
        "        for _, row in edges.iterrows()\n",
        "    ]\n",
        "    graph.add_edges_from(edge_tuples)\n",
        "\n",
        "    logging.info(\n",
        "        f\"Network construction complete. Graph has {graph.number_of_nodes()} \"\n",
        "        f\"nodes and {graph.number_of_edges()} edges.\"\n",
        "    )\n",
        "\n",
        "    return adj_matrix, graph, product_to_idx\n",
        "\n",
        "# =============================================================================\n",
        "# Task 4: Orchestrator Function\n",
        "# =============================================================================\n",
        "\n",
        "def infer_and_construct_network(\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    producer_sets: Dict[int, Set[str]],\n",
        "    purchaser_sets: Dict[int, Set[str]],\n",
        "    intersection_metrics: pd.DataFrame,\n",
        "    replication_manifest: Dict[str, Any]\n",
        ") -> Tuple[csr_matrix, nx.DiGraph, Dict[int, int]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the full network inference and construction pipeline.\n",
        "\n",
        "    This function takes the outputs of Task 3 and applies the core inference\n",
        "    algorithm and filtering rules from the paper to produce the final,\n",
        "    analysis-ready network representations.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df (pd.DataFrame): The cleansed transaction log.\n",
        "        producer_sets (Dict[int, Set[str]]): Map of product codes to producers.\n",
        "        purchaser_sets (Dict[int, Set[str]]): Map of product codes to purchasers.\n",
        "        intersection_metrics (pd.DataFrame): Pre-computed intersection metrics.\n",
        "        replication_manifest (Dict[str, Any]): The dictionary of study parameters.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[csr_matrix, nx.DiGraph, Dict[int, int]]: A tuple containing:\n",
        "        - adj_matrix: The final n x n weighted adjacency matrix (CSR format).\n",
        "        - graph: The final networkx DiGraph object.\n",
        "        - product_to_idx: The mapping from product HS codes to matrix/node indices.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting Task 4: Network Inference Implementation...\")\n",
        "\n",
        "    # Extract parameters for convenience.\n",
        "    params = replication_manifest['parameters']\n",
        "\n",
        "    # Determine the total number of unique entities (|S|).\n",
        "    total_unique_entities = len(\n",
        "        pd.unique(cleansed_df[['seller_owner_country_entity', 'buyer_owner_country_entity']].values.ravel('K'))\n",
        "    )\n",
        "\n",
        "    # Step 4.1: Compute the raw A_ij weights for all potential edges.\n",
        "    weighted_edges_df = _compute_adjacency_weights(\n",
        "        producer_sets=producer_sets,\n",
        "        purchaser_sets=purchaser_sets,\n",
        "        intersection_metrics=intersection_metrics,\n",
        "        total_unique_entities=total_unique_entities\n",
        "    )\n",
        "\n",
        "    # Step 4.2: Sparsify the network by applying all filters.\n",
        "    final_edges_df = _sparsify_network_edges(\n",
        "        weighted_edges_df=weighted_edges_df,\n",
        "        params=params\n",
        "    )\n",
        "\n",
        "    # Step 4.3: Construct the final network objects.\n",
        "    # Define the universe of nodes (all products present in sets).\n",
        "    all_products = sorted(\n",
        "        list(set(producer_sets.keys()) | set(purchaser_sets.keys()))\n",
        "    )\n",
        "\n",
        "    adj_matrix, graph, product_to_idx = _construct_network_objects(\n",
        "        final_edges_df=final_edges_df,\n",
        "        all_products=all_products\n",
        "    )\n",
        "\n",
        "    logging.info(\"Task 4 successfully completed. Network is constructed.\")\n",
        "\n",
        "    return adj_matrix, graph, product_to_idx\n"
      ],
      "metadata": {
        "id": "Xc9bN7XoG_1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Community Detection and Structural Analysis\n",
        "\n",
        "# =============================================================================\n",
        "# Task 5.1 & 5.2: Community Detection and Analysis\n",
        "# =============================================================================\n",
        "\n",
        "def _detect_communities_leiden(\n",
        "    graph: nx.DiGraph,\n",
        "    resolution_parameter: float = 0.1,\n",
        "    seed: Optional[int] = 42\n",
        ") -> Dict[int, int]:\n",
        "    \"\"\"\n",
        "    Detects communities in the network using the Leiden algorithm.\n",
        "\n",
        "    The Leiden algorithm is a state-of-the-art method for community detection\n",
        "    that improves upon the Louvain algorithm. It is used here as a robust\n",
        "    implementation for identifying meso-scale clusters in the directed,\n",
        "    weighted production network. The resolution_parameter is analogous to the\n",
        "    \"time scale\" in the original Stability algorithm: lower values tend to\n",
        "    produce fewer, larger communities.\n",
        "\n",
        "    Args:\n",
        "        graph (nx.DiGraph): The input networkx directed graph.\n",
        "        resolution_parameter (float): The resolution parameter for the Leiden\n",
        "                                      algorithm. Defaults to 0.1 to find a\n",
        "                                      small number of large communities.\n",
        "        seed (Optional[int]): A random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, int]: A dictionary mapping each node (product HS code) to\n",
        "                        its assigned community ID (integer).\n",
        "    \"\"\"\n",
        "    logging.info(\n",
        "        f\"Detecting communities with Leiden algorithm (resolution={resolution_parameter})...\"\n",
        "    )\n",
        "\n",
        "    # Convert the networkx graph to an igraph object for leidenalg.\n",
        "    # Note: node names (HS codes) are stored in the 'name' attribute.\n",
        "    igraph_graph = ig.Graph.from_networkx(graph, create_using=ig.Graph(directed=True))\n",
        "\n",
        "    # Find the partition using the Leiden algorithm.\n",
        "    # We use the RBConfigurationVertexPartition, which is suitable for directed graphs.\n",
        "    # Edge weights from the networkx graph are used to guide the partitioning.\n",
        "    partition = la.find_partition(\n",
        "        igraph_graph,\n",
        "        la.RBConfigurationVertexPartition,\n",
        "        weights='weight',\n",
        "        resolution_parameter=resolution_parameter,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Create a mapping from the node name (HS code) back to its community ID.\n",
        "    # The partition object groups nodes by their igraph index, so we must map back.\n",
        "    node_names = igraph_graph.vs['name']\n",
        "    community_map = {\n",
        "        node_name: membership\n",
        "        for node_name, membership in zip(node_names, partition.membership)\n",
        "    }\n",
        "\n",
        "    logging.info(\n",
        "        f\"Found {len(partition)} communities for {len(graph.nodes())} nodes.\"\n",
        "    )\n",
        "\n",
        "    return community_map\n",
        "\n",
        "def _characterize_communities(\n",
        "    community_map: Dict[int, int]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Assigns descriptive labels to detected communities.\n",
        "\n",
        "    This function provides a basic characterization of the communities based on\n",
        "    a simplified mapping of HS sections to industrial groups. This is a heuristic\n",
        "    to replicate the paper's qualitative findings (e.g., Machinery, Textiles).\n",
        "\n",
        "    Args:\n",
        "        community_map (Dict[int, int]): A map from product HS code to community ID.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series mapping each product HS code to a descriptive\n",
        "                   community label (e.g., \"Machinery & Metals\").\n",
        "    \"\"\"\n",
        "    # Create a DataFrame from the community map for easier analysis.\n",
        "    community_df = pd.DataFrame(\n",
        "        community_map.items(), columns=['product_hs_code', 'community_id']\n",
        "    )\n",
        "\n",
        "    # Define a heuristic mapping from HS 2-digit section to a broad category.\n",
        "    def hs_to_category(hs_code: int) -> str:\n",
        "        section = hs_code // 100\n",
        "        if 84 <= section <= 92 or 72 <= section <= 83:\n",
        "            return \"Machinery & Metals\"\n",
        "        elif 50 <= section <= 63:\n",
        "            return \"Textiles\"\n",
        "        elif 28 <= section <= 38 or 1 <= section <= 24:\n",
        "            return \"Chemicals & Food\"\n",
        "        else:\n",
        "            return \"Other\"\n",
        "\n",
        "    # Apply this mapping to each product.\n",
        "    community_df['category'] = community_df['product_hs_code'].apply(hs_to_category)\n",
        "\n",
        "    # Determine the dominant category for each community ID.\n",
        "    # This is done by finding the mode of the categories within each community.\n",
        "    community_labels = community_df.groupby('community_id')['category'].agg(\n",
        "        lambda x: x.mode()[0]\n",
        "    )\n",
        "\n",
        "    # Map the descriptive labels back to the original DataFrame.\n",
        "    community_df['community_label'] = community_df['community_id'].map(community_labels)\n",
        "\n",
        "    # Create the final Series for output.\n",
        "    final_labels = community_df.set_index('product_hs_code')['community_label']\n",
        "\n",
        "    logging.info(\"Characterized communities with descriptive labels.\")\n",
        "    logging.info(f\"Final community breakdown:\\n{final_labels.value_counts()}\")\n",
        "\n",
        "    return final_labels\n",
        "\n",
        "# =============================================================================\n",
        "# Task 5.3: Network Topology Validation\n",
        "# =============================================================================\n",
        "\n",
        "def _validate_network_topology(\n",
        "    graph: nx.DiGraph,\n",
        "    adj_matrix: csr_matrix,\n",
        "    product_to_idx: Dict[int, int]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs robust validation of the network's structural properties.\n",
        "\n",
        "    This function conducts two key validation checks on the inferred network:\n",
        "    1.  Basic Statistics: Reports fundamental network properties like node count,\n",
        "        edge count, and overall density to provide a high-level summary.\n",
        "    2.  HS Section Density: Validates the paper's finding that the network is\n",
        "        denser within broad industrial sectors (defined by 2-digit HS codes)\n",
        "        than between them. This is a test for sectoral homophily.\n",
        "\n",
        "    This implementation is specifically designed to be robust against edge-case\n",
        "    graph structures, such as those that may lack any intra- or inter-section\n",
        "    links, by handling empty density lists gracefully.\n",
        "\n",
        "    Args:\n",
        "        graph (nx.DiGraph): The networkx graph object representing the network.\n",
        "        adj_matrix (csr_matrix): The sparse adjacency matrix of the network.\n",
        "        product_to_idx (Dict[int, int]): A dictionary mapping product HS codes\n",
        "                                         to their corresponding matrix indices.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Calculate and log basic network statistics ---\n",
        "    # Retrieve the number of nodes (products) from the graph.\n",
        "    num_nodes = graph.number_of_nodes()\n",
        "    # Retrieve the number of edges (inferred links) from the graph.\n",
        "    num_edges = graph.number_of_edges()\n",
        "    # Calculate the network density for a directed graph.\n",
        "    # Formula: Density = E / (N * (N - 1)), where E is edges, N is nodes.\n",
        "    density = num_edges / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0.0\n",
        "\n",
        "    # Log the basic statistics for audit and review.\n",
        "    logging.info(\"--- Network Topology Validation ---\")\n",
        "    logging.info(f\"Node count: {num_nodes}\")\n",
        "    logging.info(f\"Edge count: {num_edges}\")\n",
        "    logging.info(f\"Network Density: {density:.6f}\")\n",
        "\n",
        "    # --- Step 2: Prepare HS section mappings for block density analysis ---\n",
        "    # Create a DataFrame from the product-to-index map for easy manipulation.\n",
        "    nodes_df = pd.DataFrame(\n",
        "        product_to_idx.items(), columns=['product_hs_code', 'matrix_idx']\n",
        "    )\n",
        "    # Compute the 2-digit HS section for each product via integer division.\n",
        "    nodes_df['hs2_section'] = nodes_df['product_hs_code'] // 100\n",
        "\n",
        "    # Group matrix indices by their HS2 section to define the blocks.\n",
        "    section_indices = nodes_df.groupby('hs2_section')['matrix_idx'].apply(list)\n",
        "\n",
        "    # --- Step 3: Calculate block densities for all section pairs ---\n",
        "    # Initialize empty lists to store the calculated densities.\n",
        "    intra_section_densities = []\n",
        "    inter_section_densities = []\n",
        "\n",
        "    # Iterate over all pairs of source and target sections.\n",
        "    for source_section, source_idxs in section_indices.items():\n",
        "        for target_section, target_idxs in section_indices.items():\n",
        "            # Calculate the number of potential edges in this block.\n",
        "            potential_edges = len(source_idxs) * len(target_idxs)\n",
        "\n",
        "            # Skip this block if no edges are possible (e.g., empty section).\n",
        "            if potential_edges == 0:\n",
        "                continue\n",
        "\n",
        "            # Extract the sub-matrix corresponding to the (source, target) block.\n",
        "            # This slicing is highly efficient with CSR matrices.\n",
        "            sub_matrix = adj_matrix[source_idxs, :][:, target_idxs]\n",
        "\n",
        "            # Calculate the density of the block.\n",
        "            block_density = sub_matrix.nnz / potential_edges\n",
        "\n",
        "            # Append the calculated density to the appropriate list.\n",
        "            if source_section == target_section:\n",
        "                intra_section_densities.append(block_density)\n",
        "            else:\n",
        "                inter_section_densities.append(block_density)\n",
        "\n",
        "    # --- Step 4: Robustly calculate average densities and validate ---\n",
        "    # This is the core remediation step to handle empty lists.\n",
        "\n",
        "    # Calculate average intra-section density.\n",
        "    if intra_section_densities:\n",
        "        # If the list is not empty, compute the mean.\n",
        "        avg_intra_density = np.mean(intra_section_densities)\n",
        "    else:\n",
        "        # If the list is empty, set the mean to 0.0 and log a warning.\n",
        "        avg_intra_density = 0.0\n",
        "        logging.warning(\n",
        "            \"No intra-section edges found. Average intra-section density is 0.\"\n",
        "        )\n",
        "\n",
        "    # Calculate average inter-section density.\n",
        "    if inter_section_densities:\n",
        "        # If the list is not empty, compute the mean.\n",
        "        avg_inter_density = np.mean(inter_section_densities)\n",
        "    else:\n",
        "        # If the list is empty, set the mean to 0.0 and log a warning.\n",
        "        avg_inter_density = 0.0\n",
        "        logging.warning(\n",
        "            \"No inter-section edges found. Average inter-section density is 0.\"\n",
        "        )\n",
        "\n",
        "    # Log the final calculated average densities.\n",
        "    logging.info(f\"Average intra-section density: {avg_intra_density:.6f}\")\n",
        "    logging.info(f\"Average inter-section density: {avg_inter_density:.6f}\")\n",
        "\n",
        "    # Perform the final validation check based on the paper's hypothesis.\n",
        "    if avg_intra_density > avg_inter_density:\n",
        "        # Log a success message if the validation passes.\n",
        "        logging.info(\n",
        "            \"Validation PASSED: Network is denser within HS sections than between them.\"\n",
        "        )\n",
        "    else:\n",
        "        # Log a warning if the validation fails.\n",
        "        logging.warning(\n",
        "            \"Validation FAILED: Network is not denser within HS sections.\"\n",
        "        )\n",
        "\n",
        "# =============================================================================\n",
        "# Task 5: Orchestrator Function\n",
        "# =============================================================================\n",
        "\n",
        "def analyze_network_structure(\n",
        "    graph: nx.DiGraph,\n",
        "    adj_matrix: csr_matrix,\n",
        "    product_to_idx: Dict[int, int],\n",
        "    replication_manifest: Dict[str, Any]\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Orchestrates the community detection and structural analysis of the network.\n",
        "\n",
        "    Args:\n",
        "        graph (nx.DiGraph): The final networkx DiGraph object from Task 4.\n",
        "        adj_matrix (csr_matrix): The final sparse adjacency matrix.\n",
        "        product_to_idx (Dict[int, int]): Mapping from product codes to indices.\n",
        "        replication_manifest (Dict[str, Any]): The dictionary of study parameters.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series mapping each product HS code to its assigned\n",
        "                   descriptive community label.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting Task 5: Community Detection and Structural Analysis...\")\n",
        "\n",
        "    # Step 5.1 & 5.2: Detect and characterize communities.\n",
        "    # We select a low resolution parameter to aim for the 3-cluster structure.\n",
        "    # This may require empirical tuning in a real application.\n",
        "    community_map = _detect_communities_leiden(graph=graph)\n",
        "\n",
        "    # Assign descriptive labels to the found communities.\n",
        "    community_labels = _characterize_communities(community_map=community_map)\n",
        "\n",
        "    # Step 5.3: Perform topological validation.\n",
        "    _validate_network_topology(\n",
        "        graph=graph,\n",
        "        adj_matrix=adj_matrix,\n",
        "        product_to_idx=product_to_idx\n",
        "    )\n",
        "\n",
        "    logging.info(\"Task 5 successfully completed. Network structure analyzed.\")\n",
        "\n",
        "    return community_labels\n"
      ],
      "metadata": {
        "id": "wH2WpfUZIEfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Centrality Calculations\n",
        "\n",
        "# =============================================================================\n",
        "# Task 6.1: Betweenness Centrality Computation\n",
        "# =============================================================================\n",
        "\n",
        "def _calculate_betweenness_centrality(\n",
        "    graph: nx.DiGraph\n",
        ") -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Calculates the weighted betweenness centrality for each node in the network.\n",
        "\n",
        "    Betweenness centrality identifies nodes that act as critical \"choke points\"\n",
        "    or bridges along the shortest paths in the network. For this calculation,\n",
        "    the edge weights (A_ij) must be inverted, as higher A_ij values represent\n",
        "    stronger (i.e., \"shorter\" or \"easier\") paths.\n",
        "\n",
        "    The function implements the mathematical definition:\n",
        "    BC(v) = Σ_{s ≠ t ≠ v} [σ_st(v) / σ_st]\n",
        "    where σ_st is the number of shortest paths between nodes s and t, and\n",
        "    σ_st(v) is the number of those paths that pass through node v.\n",
        "\n",
        "    Args:\n",
        "        graph (nx.DiGraph): The input networkx directed graph with an 'weight'\n",
        "                            attribute on each edge representing A_ij.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, float]: A dictionary mapping each node (product HS code) to\n",
        "                          its normalized betweenness centrality score.\n",
        "    \"\"\"\n",
        "    logging.info(\"Calculating weighted betweenness centrality...\")\n",
        "\n",
        "    # Create a working copy of the graph to avoid modifying the original object.\n",
        "    g_copy = graph.copy()\n",
        "\n",
        "    # --- Weight Inversion for Shortest Path Calculation ---\n",
        "    # The algorithm finds shortest paths by minimizing the sum of weights.\n",
        "    # Since our 'weight' (A_ij) signifies strength, a higher value should\n",
        "    # mean a shorter path. We achieve this by using the inverse of the weight.\n",
        "    for u, v, data in g_copy.edges(data=True):\n",
        "        # Access the original weight.\n",
        "        weight = data.get('weight', 0.0)\n",
        "        # Add a new 'distance' attribute. Add a small epsilon to avoid division by zero.\n",
        "        if weight > 0:\n",
        "            g_copy[u][v]['distance'] = 1.0 / weight\n",
        "        else:\n",
        "            # Assign a very large distance to edges with zero or negative weight.\n",
        "            g_copy[u][v]['distance'] = float('inf')\n",
        "\n",
        "    # --- Centrality Calculation ---\n",
        "    # Use networkx's highly optimized function to calculate centrality.\n",
        "    # 'weight=\"distance\"' tells the algorithm to use our inverted weights.\n",
        "    # 'normalized=True' scales the results to the range [0, 1].\n",
        "    betweenness_centrality = nx.betweenness_centrality(\n",
        "        g_copy,\n",
        "        weight='distance',\n",
        "        normalized=True\n",
        "    )\n",
        "\n",
        "    logging.info(\"Successfully calculated betweenness centrality.\")\n",
        "    return betweenness_centrality\n",
        "\n",
        "# =============================================================================\n",
        "# Task 6.2: Hub Score Calculation (HITS Algorithm)\n",
        "# =============================================================================\n",
        "\n",
        "def _calculate_hub_scores(\n",
        "    graph: nx.DiGraph\n",
        ") -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    Calculates the hub scores for each node using the HITS algorithm.\n",
        "\n",
        "    The HITS algorithm identifies two types of important nodes: \"hubs\" (nodes\n",
        "    that point to many important authorities) and \"authorities\" (nodes that\n",
        "    are pointed to by many important hubs). This function calculates and\n",
        "    returns the hub scores, which, in the context of the paper, identify\n",
        "    products embedded in dense, complex supply chains.\n",
        "\n",
        "    The algorithm iteratively solves:\n",
        "    h = A^T * a  (hub score is proportional to sum of authority scores it points to)\n",
        "    a = A * h    (authority score is proportional to sum of hub scores pointing to it)\n",
        "\n",
        "    Args:\n",
        "        graph (nx.DiGraph): The input networkx directed graph.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, float]: A dictionary mapping each node (product HS code) to\n",
        "                          its hub score.\n",
        "\n",
        "    Raises:\n",
        "        DataValidationError: If the HITS algorithm fails to converge.\n",
        "    \"\"\"\n",
        "    logging.info(\"Calculating hub scores using the HITS algorithm...\")\n",
        "\n",
        "    try:\n",
        "        # Use networkx's implementation of the HITS algorithm.\n",
        "        # The 'weight' attribute (A_ij) is used directly.\n",
        "        # The tolerance and max_iterations are set to standard, robust values.\n",
        "        hubs, authorities = nx.hits(\n",
        "            graph,\n",
        "            max_iter=1000,\n",
        "            tol=1.0e-6,\n",
        "            normalized=True\n",
        "        )\n",
        "    except nx.PowerIterationFailedConvergence as e:\n",
        "        # Catch convergence errors, which indicate potential issues with the\n",
        "        # network structure (e.g., disconnected components not handled).\n",
        "        raise DataValidationError(\n",
        "            \"HITS algorithm failed to converge. The network may be unsuitable \"\n",
        "            \"for this centrality measure.\"\n",
        "        ) from e\n",
        "\n",
        "    logging.info(\"Successfully calculated hub and authority scores.\")\n",
        "    return hubs\n",
        "\n",
        "# =============================================================================\n",
        "# Task 6.3: Centrality Aggregation and Validation\n",
        "# =============================================================================\n",
        "\n",
        "def _aggregate_and_validate_centralities(\n",
        "    betweenness_centrality: Dict[int, float],\n",
        "    hub_scores: Dict[int, float]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregates centrality scores into a single DataFrame and validates them.\n",
        "\n",
        "    This function consolidates the computed centrality metrics, ranks them,\n",
        "    and performs a correlation analysis to verify that they capture distinct\n",
        "    aspects of network structure, as expected.\n",
        "\n",
        "    Args:\n",
        "        betweenness_centrality (Dict[int, float]): Node-to-betweenness map.\n",
        "        hub_scores (Dict[int, float]): Node-to-hub-score map.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by product HS code with columns for\n",
        "                      centrality values and their ranks.\n",
        "    \"\"\"\n",
        "    logging.info(\"Aggregating and validating centrality measures...\")\n",
        "\n",
        "    # Create a DataFrame from the centrality dictionaries.\n",
        "    # Using pd.Series ensures alignment by node ID (HS code).\n",
        "    centrality_df = pd.DataFrame({\n",
        "        'betweenness': pd.Series(betweenness_centrality),\n",
        "        'hub_score': pd.Series(hub_scores)\n",
        "    })\n",
        "    centrality_df.index.name = 'product_hs_code'\n",
        "\n",
        "    # --- Rank the centrality scores ---\n",
        "    # 'ascending=False' ensures that higher scores get lower rank numbers (e.g., rank 1).\n",
        "    centrality_df['rank_betweenness'] = centrality_df['betweenness'].rank(\n",
        "        method='average', ascending=False\n",
        "    )\n",
        "    centrality_df['rank_hub_score'] = centrality_df['hub_score'].rank(\n",
        "        method='average', ascending=False\n",
        "    )\n",
        "\n",
        "    # --- Validate by checking correlation ---\n",
        "    # We expect a low-to-moderate correlation, indicating the measures\n",
        "    # capture different structural properties. Spearman is used for rank correlation.\n",
        "    correlation_matrix = centrality_df[['betweenness', 'hub_score']].corr(\n",
        "        method='spearman'\n",
        "    )\n",
        "    correlation = correlation_matrix.iloc[0, 1]\n",
        "    logging.info(\n",
        "        f\"Spearman rank correlation between Betweenness and Hub Score: {correlation:.4f}\"\n",
        "    )\n",
        "\n",
        "    return centrality_df\n",
        "\n",
        "# =============================================================================\n",
        "# Task 6: Orchestrator Function\n",
        "# =============================================================================\n",
        "\n",
        "def calculate_centralities(\n",
        "    graph: nx.DiGraph,\n",
        "    replication_manifest: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the calculation of all specified network centrality measures.\n",
        "\n",
        "    This function takes the final constructed network graph and computes the\n",
        "    key node-level importance metrics discussed in the paper: Betweenness\n",
        "    Centrality and Hub Scores. It returns a consolidated DataFrame containing\n",
        "    these metrics for every product in the network.\n",
        "\n",
        "    Args:\n",
        "        graph (nx.DiGraph): The final, weighted, directed network graph from Task 4.\n",
        "        replication_manifest (Dict[str, Any]): The dictionary of study parameters.\n",
        "                                               (Currently unused but included for\n",
        "                                               API consistency).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by product HS code, containing columns\n",
        "                      for each centrality measure and its corresponding rank.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting Task 6: Centrality Calculations...\")\n",
        "\n",
        "    # Step 6.1: Calculate weighted betweenness centrality.\n",
        "    betweenness_centrality = _calculate_betweenness_centrality(graph=graph)\n",
        "\n",
        "    # Step 6.2: Calculate hub scores using the HITS algorithm.\n",
        "    hub_scores = _calculate_hub_scores(graph=graph)\n",
        "\n",
        "    # Step 6.3: Aggregate, rank, and validate the centrality scores.\n",
        "    centrality_df = _aggregate_and_validate_centralities(\n",
        "        betweenness_centrality=betweenness_centrality,\n",
        "        hub_scores=hub_scores\n",
        "    )\n",
        "\n",
        "    logging.info(\"Task 6 successfully completed. Centrality metrics calculated.\")\n",
        "\n",
        "    return centrality_df\n"
      ],
      "metadata": {
        "id": "ZCHWam22JNS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Validation Procedures\n",
        "\n",
        "# =============================================================================\n",
        "# Task 7.1: External Network Comparison\n",
        "# =============================================================================\n",
        "\n",
        "def _compare_networks(\n",
        "    empirical_matrix: csr_matrix,\n",
        "    external_matrix: csr_matrix,\n",
        "    product_to_idx: Dict[int, int],\n",
        "    external_product_list: List[int]\n",
        ") -> Dict[str, Tuple[float, float]]:\n",
        "    \"\"\"\n",
        "    Compares the empirical network to an external network on multiple metrics.\n",
        "\n",
        "    This function aligns the external network to the node set of the empirical\n",
        "    network and then computes three correlation metrics:\n",
        "    1. Edge-level correlation: Pearson correlation of the flattened adjacency matrices.\n",
        "    2. In-degree correlation: Pearson correlation of the in-degree vectors.\n",
        "    3. Out-degree correlation: Pearson correlation of the out-degree vectors.\n",
        "\n",
        "    Args:\n",
        "        empirical_matrix (csr_matrix): The adjacency matrix of our inferred network.\n",
        "        external_matrix (csr_matrix): The adjacency matrix of the external network.\n",
        "        product_to_idx (Dict[int, int]): Mapping from product HS code to the\n",
        "                                         index in our empirical matrix.\n",
        "        external_product_list (List[int]): Ordered list of products corresponding\n",
        "                                           to the rows/cols of the external_matrix.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Tuple[float, float]]: A dictionary with correlation results,\n",
        "                                        where each value is a tuple of\n",
        "                                        (correlation_coefficient, p_value).\n",
        "    \"\"\"\n",
        "    logging.info(\"Aligning and comparing to an external network...\")\n",
        "\n",
        "    # --- Step 1: Align the external network to the empirical node set ---\n",
        "    # Create the inverse mapping for our network.\n",
        "    idx_to_product = {i: p for p, i in product_to_idx.items()}\n",
        "\n",
        "    # Create the product-to-index map for the external network.\n",
        "    external_product_to_idx = {p: i for i, p in enumerate(external_product_list)}\n",
        "\n",
        "    # Find the common products between both networks.\n",
        "    common_products = sorted(\n",
        "        list(set(product_to_idx.keys()) & set(external_product_to_idx.keys()))\n",
        "    )\n",
        "\n",
        "    # Get the indices corresponding to these common products for each matrix.\n",
        "    empirical_indices = [product_to_idx[p] for p in common_products]\n",
        "    external_indices = [external_product_to_idx[p] for p in common_products]\n",
        "\n",
        "    # Slice both matrices to create aligned versions with only common nodes.\n",
        "    aligned_empirical = empirical_matrix[empirical_indices, :][:, empirical_indices]\n",
        "    aligned_external = external_matrix[external_indices, :][:, external_indices]\n",
        "\n",
        "    # --- Step 2: Calculate correlation metrics ---\n",
        "    results = {}\n",
        "\n",
        "    # Edge-level correlation\n",
        "    # Flatten the matrices to 1D arrays for correlation.\n",
        "    empirical_flat = aligned_empirical.toarray().flatten()\n",
        "    external_flat = aligned_external.toarray().flatten()\n",
        "    results['edge_correlation'] = pearsonr(empirical_flat, external_flat)\n",
        "\n",
        "    # In-degree correlation\n",
        "    # Sum over columns (axis=0) to get in-degrees.\n",
        "    empirical_indegree = np.array(aligned_empirical.sum(axis=0)).flatten()\n",
        "    external_indegree = np.array(aligned_external.sum(axis=0)).flatten()\n",
        "    results['indegree_correlation'] = pearsonr(empirical_indegree, external_indegree)\n",
        "\n",
        "    # Out-degree correlation\n",
        "    # Sum over rows (axis=1) to get out-degrees.\n",
        "    empirical_outdegree = np.array(aligned_empirical.sum(axis=1)).flatten()\n",
        "    external_outdegree = np.array(aligned_external.sum(axis=1)).flatten()\n",
        "    results['outdegree_correlation'] = pearsonr(empirical_outdegree, external_outdegree)\n",
        "\n",
        "    logging.info(f\"Comparison Results: {results}\")\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# Task 7.2 & 7.3: Configuration Model and Modularity Validation\n",
        "# =============================================================================\n",
        "\n",
        "def _calculate_directed_modularity(\n",
        "    adj_matrix: csr_matrix,\n",
        "    subgraph_indices: List[int],\n",
        "    in_degrees: np.ndarray,\n",
        "    out_degrees: np.ndarray\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the directed modularity of a subgraph.\n",
        "\n",
        "    Implements the formula for the contribution of a single community (subgraph)\n",
        "    to the total network modularity:\n",
        "    M_G = (1/m) * Σ_{i,j ∈ G} [A_ij - (k_i^out * k_j^in) / m]\n",
        "    where A is the unweighted adjacency matrix.\n",
        "\n",
        "    Args:\n",
        "        adj_matrix (csr_matrix): The UNWEIGHTED adjacency matrix of the full graph.\n",
        "        subgraph_indices (List[int]): A list of integer indices for the nodes\n",
        "                                      in the subgraph.\n",
        "        in_degrees (np.ndarray): The in-degree sequence of the FULL graph.\n",
        "        out_degrees (np.ndarray): The out-degree sequence of the FULL graph.\n",
        "\n",
        "    Returns:\n",
        "        float: The modularity score of the specified subgraph.\n",
        "    \"\"\"\n",
        "    # Total number of edges in the full graph.\n",
        "    m = adj_matrix.nnz\n",
        "    if m == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Extract the part of the adjacency matrix corresponding to the subgraph.\n",
        "    subgraph_matrix = adj_matrix[subgraph_indices, :][:, subgraph_indices]\n",
        "\n",
        "    # The actual number of edges within the subgraph.\n",
        "    edges_within_subgraph = subgraph_matrix.nnz\n",
        "\n",
        "    # The expected number of edges within the subgraph under the null model.\n",
        "    subgraph_out_degrees = out_degrees[subgraph_indices]\n",
        "    subgraph_in_degrees = in_degrees[subgraph_indices]\n",
        "    expected_edges = (subgraph_out_degrees.sum() * subgraph_in_degrees.sum()) / m\n",
        "\n",
        "    # Calculate and return the modularity score.\n",
        "    modularity = (edges_within_subgraph - expected_edges) / m\n",
        "    return modularity\n",
        "\n",
        "def _modularity_simulation_worker(\n",
        "    args: Tuple[np.ndarray, np.ndarray, List[int], int]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    A worker function for parallel modularity simulation.\n",
        "\n",
        "    Generates one random graph using the configuration model and calculates\n",
        "    the modularity of the specified subgraph within it.\n",
        "\n",
        "    Args:\n",
        "        args (Tuple): A tuple containing (in_degrees, out_degrees,\n",
        "                      subgraph_indices, seed).\n",
        "\n",
        "    Returns:\n",
        "        float: The modularity score for the subgraph in one random graph.\n",
        "    \"\"\"\n",
        "    in_degrees, out_degrees, subgraph_indices, seed = args\n",
        "\n",
        "    # Generate a random directed graph with the same degree sequence.\n",
        "    random_graph = nx.directed_configuration_model(\n",
        "        in_degree_sequence=in_degrees,\n",
        "        out_degree_sequence=out_degrees,\n",
        "        seed=seed,\n",
        "        create_using=nx.DiGraph\n",
        "    )\n",
        "\n",
        "    # Convert to an unweighted adjacency matrix.\n",
        "    random_adj = nx.to_scipy_sparse_matrix(random_graph, format='csr')\n",
        "\n",
        "    # Calculate modularity for this random instance.\n",
        "    return _calculate_directed_modularity(\n",
        "        random_adj, subgraph_indices, in_degrees, out_degrees\n",
        "    )\n",
        "\n",
        "def _validate_subgraph_connectivity(\n",
        "    adj_matrix: csr_matrix,\n",
        "    product_to_idx: Dict[int, int],\n",
        "    supply_chain_products: List[int],\n",
        "    supply_chain_name: str,\n",
        "    num_simulations: int\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Tests if a subgraph's connectivity is statistically significant.\n",
        "\n",
        "    This function performs a Monte Carlo simulation to determine if the\n",
        "    observed modularity of a predefined subgraph (e.g., a known supply chain)\n",
        "    is significantly higher than what would be expected by chance in a random\n",
        "    network with the same degree distribution (the configuration model).\n",
        "\n",
        "    Args:\n",
        "        adj_matrix (csr_matrix): The adjacency matrix of the empirical network.\n",
        "        product_to_idx (Dict[int, int]): Mapping from product HS code to index.\n",
        "        supply_chain_products (List[int]): List of HS codes in the subgraph.\n",
        "        supply_chain_name (str): The name of the supply chain for logging.\n",
        "        num_simulations (int): The number of random graphs to generate.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: A tuple of (empirical_modularity, p_value).\n",
        "    \"\"\"\n",
        "    logging.info(f\"--- Validating connectivity for '{supply_chain_name}' subgraph ---\")\n",
        "\n",
        "    # --- Step 1: Calculate empirical modularity ---\n",
        "    # Create an unweighted version of the adjacency matrix (0s and 1s).\n",
        "    unweighted_adj = (adj_matrix > 0).astype(int)\n",
        "\n",
        "    # Get the degree sequences of the full graph.\n",
        "    out_degrees = np.array(unweighted_adj.sum(axis=1)).flatten()\n",
        "    in_degrees = np.array(unweighted_adj.sum(axis=0)).flatten()\n",
        "\n",
        "    # Get the matrix indices for the products in this supply chain.\n",
        "    subgraph_indices = [\n",
        "        product_to_idx[p] for p in supply_chain_products if p in product_to_idx\n",
        "    ]\n",
        "    if not subgraph_indices:\n",
        "        logging.warning(f\"No products from '{supply_chain_name}' found in the network.\")\n",
        "        return 0.0, 1.0\n",
        "\n",
        "    # Calculate the modularity of the subgraph in our actual network.\n",
        "    empirical_modularity = _calculate_directed_modularity(\n",
        "        unweighted_adj, subgraph_indices, in_degrees, out_degrees\n",
        "    )\n",
        "    logging.info(f\"Empirical modularity for '{supply_chain_name}': {empirical_modularity:.6f}\")\n",
        "\n",
        "    # --- Step 2: Run Monte Carlo simulation ---\n",
        "    logging.info(f\"Running {num_simulations:,} simulations for the null distribution...\")\n",
        "\n",
        "    # Prepare arguments for the parallel worker function.\n",
        "    # We generate unique seeds for each worker for true parallelism.\n",
        "    worker_args = [\n",
        "        (in_degrees, out_degrees, subgraph_indices, seed)\n",
        "        for seed in range(num_simulations)\n",
        "    ]\n",
        "\n",
        "    # Use a multiprocessing pool to parallelize the simulations.\n",
        "    with Pool(processes=max(1, cpu_count() - 1)) as pool:\n",
        "        null_distribution = pool.map(_modularity_simulation_worker, worker_args)\n",
        "\n",
        "    # --- Step 3: Calculate the p-value ---\n",
        "    # Count how many times the random modularity was >= the empirical one.\n",
        "    null_distribution = np.array(null_distribution)\n",
        "    count_extreme = np.sum(null_distribution >= empirical_modularity)\n",
        "\n",
        "    # Calculate the p-value with a +1 correction to avoid p=0.\n",
        "    p_value = (count_extreme + 1) / (num_simulations + 1)\n",
        "    logging.info(f\"P-value for '{supply_chain_name}': {p_value:.6f}\")\n",
        "\n",
        "    return empirical_modularity, p_value\n",
        "\n",
        "# =============================================================================\n",
        "# Task 7: Orchestrator Function\n",
        "# =============================================================================\n",
        "\n",
        "def run_validation_procedures(\n",
        "    adj_matrix: csr_matrix,\n",
        "    product_to_idx: Dict[int, int],\n",
        "    supply_chains_definitions: Dict[str, List[int]],\n",
        "    replication_manifest: Dict[str, Any],\n",
        "    # Placeholder for external networks, which would be loaded here.\n",
        "    external_networks: Dict[str, Any] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates all network validation procedures.\n",
        "\n",
        "    Args:\n",
        "        adj_matrix (csr_matrix): The inferred adjacency matrix.\n",
        "        product_to_idx (Dict[int, int]): Mapping from product HS code to index.\n",
        "        supply_chains_definitions (Dict[str, List[int]]): Definitions of\n",
        "                                                          manual supply chains.\n",
        "        replication_manifest (Dict[str, Any]): The study parameters.\n",
        "        external_networks (Dict[str, Any], optional): A dictionary containing\n",
        "            loaded external networks for comparison. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing all validation results.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting Task 7: Validation Procedures...\")\n",
        "    validation_results = {}\n",
        "\n",
        "    # --- Task 7.1: External Network Comparison ---\n",
        "    # This part is conditional on providing external network data.\n",
        "    if external_networks:\n",
        "        logging.info(\"--- Task 7.1: External Network Comparison ---\")\n",
        "        validation_results['external_comparison'] = {}\n",
        "        for name, data in external_networks.items():\n",
        "            # Assuming data is a dict with 'matrix' and 'product_list'\n",
        "            comparison = _compare_networks(\n",
        "                empirical_matrix=adj_matrix,\n",
        "                external_matrix=data['matrix'],\n",
        "                product_to_idx=product_to_idx,\n",
        "                external_product_list=data['product_list']\n",
        "            )\n",
        "            validation_results['external_comparison'][name] = comparison\n",
        "    else:\n",
        "        logging.warning(\"Skipping external network comparison: No data provided.\")\n",
        "\n",
        "    # --- Task 7.2 & 7.3: Manual Supply Chain Validation ---\n",
        "    logging.info(\"--- Task 7.2 & 7.3: Manual Supply Chain Validation ---\")\n",
        "    validation_results['subgraph_validation'] = {}\n",
        "    num_sims = replication_manifest['parameters']['network_analysis']['modularity_validation_simulations']\n",
        "\n",
        "    for name, products in supply_chains_definitions.items():\n",
        "        modularity, p_value = _validate_subgraph_connectivity(\n",
        "            adj_matrix=adj_matrix,\n",
        "            product_to_idx=product_to_idx,\n",
        "            supply_chain_products=products,\n",
        "            supply_chain_name=name,\n",
        "            num_simulations=num_sims\n",
        "        )\n",
        "        validation_results['subgraph_validation'][name] = {\n",
        "            'empirical_modularity': modularity,\n",
        "            'p_value': p_value\n",
        "        }\n",
        "\n",
        "    logging.info(\"Task 7 successfully completed. Validation results generated.\")\n",
        "    return validation_results\n"
      ],
      "metadata": {
        "id": "NpPMaEu0J58e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Economic Feature Engineering\n",
        "\n",
        "# =============================================================================\n",
        "# Task 8.1: Country-Product Presence Matrix Construction\n",
        "# =============================================================================\n",
        "\n",
        "def _calculate_rpop(\n",
        "    comtrade_df: pd.DataFrame,\n",
        "    country_df: pd.DataFrame,\n",
        "    years: List[int]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates Revealed Population-Adjusted Comparative Advantage (Rpop).\n",
        "\n",
        "    Implements the formula:\n",
        "    Rpop_p,c = (E_p,c / pop_c) / (E_p / pop_world)\n",
        "    where E_p,c is exports of product p by country c.\n",
        "\n",
        "    Args:\n",
        "        comtrade_df (pd.DataFrame): DataFrame of country-product exports.\n",
        "        country_df (pd.DataFrame): DataFrame of country populations.\n",
        "        years (List[int]): The years for which to calculate Rpop.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with a MultiIndex (year, reporter_iso,\n",
        "                      product_hs_code) and a column for Rpop.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Calculating Rpop for years: {years}...\")\n",
        "\n",
        "    # Filter data for the relevant years to reduce memory footprint.\n",
        "    exports = comtrade_df[comtrade_df['year'].isin(years)].copy()\n",
        "    population = country_df[country_df['year'].isin(years)].copy()\n",
        "\n",
        "    # Calculate total world exports per product, per year (E_p).\n",
        "    world_exports = exports.groupby(['year', 'product_hs_code'])['export_value_usd'].sum().rename('world_export')\n",
        "\n",
        "    # Calculate total world population per year (pop_world).\n",
        "    world_population = population.groupby('year')['population'].sum().rename('world_population')\n",
        "\n",
        "    # Merge exports with population data.\n",
        "    merged_df = pd.merge(\n",
        "        exports, population, on=['year', 'reporter_iso'], how='left'\n",
        "    )\n",
        "\n",
        "    # Merge with world totals.\n",
        "    merged_df = pd.merge(\n",
        "        merged_df, world_exports, on=['year', 'product_hs_code'], how='left'\n",
        "    )\n",
        "    merged_df = pd.merge(\n",
        "        merged_df, world_population, on='year', how='left'\n",
        "    )\n",
        "\n",
        "    # Handle cases where population or world data is missing.\n",
        "    if merged_df[['population', 'world_export', 'world_population']].isnull().any().any():\n",
        "        logging.warning(\"Missing population or world export data for some records. Rpop will be NaN.\")\n",
        "\n",
        "    # Calculate per capita values.\n",
        "    merged_df['per_capita_export_country'] = merged_df['export_value_usd'] / merged_df['population']\n",
        "    merged_df['per_capita_export_world'] = merged_df['world_export'] / merged_df['world_population']\n",
        "\n",
        "    # Calculate Rpop, handling division by zero.\n",
        "    merged_df['rpop'] = merged_df['per_capita_export_country'] / merged_df['per_capita_export_world']\n",
        "    merged_df['rpop'].fillna(0, inplace=True)\n",
        "    merged_df.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "\n",
        "    # Return a clean DataFrame with the required index and column.\n",
        "    return merged_df.set_index(['year', 'reporter_iso', 'product_hs_code'])[['rpop']]\n",
        "\n",
        "\n",
        "def _construct_diversification_outcome(\n",
        "    rpop_df: pd.DataFrame,\n",
        "    params: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs the binary diversification outcome variable.\n",
        "\n",
        "    A diversification event occurs if a country-product pair was \"absent\" in\n",
        "    the start year and \"present\" in the end year, based on Rpop thresholds.\n",
        "\n",
        "    Args:\n",
        "        rpop_df (pd.DataFrame): DataFrame of Rpop values.\n",
        "        params (Dict[str, Any]): The 'econometric_analysis' parameters.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by (reporter_iso, product_hs_code)\n",
        "                      with columns for presence and the diversification outcome.\n",
        "    \"\"\"\n",
        "    logging.info(\"Constructing diversification outcome variable...\")\n",
        "\n",
        "    # Unstack the Rpop data to have years as columns.\n",
        "    rpop_wide = rpop_df.unstack(level='year')\n",
        "    rpop_wide.columns = rpop_wide.columns.droplevel(0) # Drop 'rpop' level\n",
        "\n",
        "    start_year = params['start_year_for_diversification']\n",
        "    end_year = params['end_year_for_diversification']\n",
        "\n",
        "    # Fill NaNs with 0, as missing Rpop implies absence.\n",
        "    rpop_wide.fillna(0, inplace=True)\n",
        "\n",
        "    # Create binary presence indicators based on the specified thresholds.\n",
        "    rpop_wide[f'presence_{start_year}'] = (\n",
        "        rpop_wide[start_year] >= params['rpop_absence_threshold']\n",
        "    )\n",
        "    rpop_wide[f'presence_{end_year}'] = (\n",
        "        rpop_wide[end_year] >= params['rpop_presence_threshold']\n",
        "    )\n",
        "\n",
        "    # Define the diversification outcome variable (R_p,c).\n",
        "    # R_p,c = 1 if (M_2016 = 0) AND (M_2021 = 1) else 0\n",
        "    rpop_wide['diversification_outcome'] = (\n",
        "        (~rpop_wide[f'presence_{start_year}']) & (rpop_wide[f'presence_{end_year}'])\n",
        "    ).astype(int)\n",
        "\n",
        "    logging.info(\n",
        "        f\"Identified {rpop_wide['diversification_outcome'].sum():,} \"\n",
        "        \"diversification events.\"\n",
        "    )\n",
        "\n",
        "    return rpop_wide[[f'presence_{start_year}', 'diversification_outcome']]\n",
        "\n",
        "# =============================================================================\n",
        "# Task 8.2: Network Density Metric Calculation\n",
        "# =============================================================================\n",
        "\n",
        "def _calculate_network_density(\n",
        "    adj_matrix: csr_matrix,\n",
        "    presence_matrix: pd.DataFrame,\n",
        "    product_to_idx: Dict[int, int],\n",
        "    density_type: str,\n",
        "    top_k: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculates upstream or downstream network density for each country-product.\n",
        "\n",
        "    This function uses efficient sparse matrix algebra to compute the density\n",
        "    metric, which measures the proportion of a product's top-k suppliers\n",
        "    (upstream) or buyers (downstream) that a country already exports.\n",
        "\n",
        "    Args:\n",
        "        adj_matrix (csr_matrix): The n x n adjacency matrix of the network.\n",
        "        presence_matrix (pd.DataFrame): A (products x countries) binary matrix\n",
        "                                        indicating export presence in the base year.\n",
        "        product_to_idx (Dict[int, int]): Mapping from product HS code to matrix index.\n",
        "        density_type (str): Either 'upstream' or 'downstream'.\n",
        "        top_k (int): The number of top partners to consider for the density calculation.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A (products x countries) DataFrame of density scores.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Calculating {density_type} density (top_k={top_k})...\")\n",
        "\n",
        "    # Determine which matrix to use based on density type.\n",
        "    # Downstream density: uses transpose to find outputs for a given input.\n",
        "    # Upstream density: uses the original matrix to find inputs for a given output.\n",
        "    matrix = adj_matrix.T if density_type == 'downstream' else adj_matrix\n",
        "\n",
        "    # --- Create the binary top-k weight matrix (W) ---\n",
        "    n_products = matrix.shape[0]\n",
        "    W = csr_matrix((n_products, n_products), dtype=np.float32)\n",
        "\n",
        "    # Iterate through each product (row) to find its top-k partners.\n",
        "    for i in range(n_products):\n",
        "        # Get the data and indices for the current row.\n",
        "        row_start = matrix.indptr[i]\n",
        "        row_end = matrix.indptr[i+1]\n",
        "        if row_start == row_end:\n",
        "            continue # Skip if no partners\n",
        "\n",
        "        row_data = matrix.data[row_start:row_end]\n",
        "        row_indices = matrix.indices[row_start:row_end]\n",
        "\n",
        "        # Find the indices of the top-k values.\n",
        "        # `np.argpartition` is faster than a full sort for finding top-k.\n",
        "        k = min(top_k, len(row_data))\n",
        "        top_k_local_indices = np.argpartition(row_data, -k)[-k:]\n",
        "        top_k_global_indices = row_indices[top_k_local_indices]\n",
        "\n",
        "        # Set the corresponding entries in W to 1.\n",
        "        W[i, top_k_global_indices] = 1\n",
        "\n",
        "    # --- Calculate Density via Matrix Multiplication ---\n",
        "    # Align the presence matrix with the adjacency matrix's indexing.\n",
        "    aligned_presence = presence_matrix.reindex(\n",
        "        index=list(product_to_idx.keys())\n",
        "    ).fillna(0)\n",
        "\n",
        "    # The core calculation: D = W * M\n",
        "    # This multiplies the top-k links by the country presence.\n",
        "    density_matrix_raw = W.dot(aligned_presence.values)\n",
        "\n",
        "    # Normalize by the number of partners for each product.\n",
        "    # The normalizer is the sum of each row in W.\n",
        "    normalizer = np.array(W.sum(axis=1)).flatten()\n",
        "    # Avoid division by zero for products with no partners.\n",
        "    normalizer[normalizer == 0] = 1\n",
        "\n",
        "    # Apply normalization.\n",
        "    density_matrix_normalized = density_matrix_raw / normalizer[:, np.newaxis]\n",
        "\n",
        "    # Convert the result back to a DataFrame.\n",
        "    density_df = pd.DataFrame(\n",
        "        density_matrix_normalized,\n",
        "        index=aligned_presence.index,\n",
        "        columns=aligned_presence.columns\n",
        "    )\n",
        "\n",
        "    return density_df\n",
        "\n",
        "# =============================================================================\n",
        "# Task 8.3: Econometric Dataset Preparation\n",
        "# =============================================================================\n",
        "\n",
        "def _prepare_econometric_dataset(\n",
        "    diversification_df: pd.DataFrame,\n",
        "    downstream_density: pd.DataFrame,\n",
        "    upstream_density: pd.DataFrame,\n",
        "    comtrade_df: pd.DataFrame,\n",
        "    params: Dict[str, Any],\n",
        "    network_products: List[int]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Assembles and filters the final dataset for econometric analysis.\n",
        "\n",
        "    Args:\n",
        "        diversification_df (pd.DataFrame): DataFrame with diversification outcomes.\n",
        "        downstream_density (pd.DataFrame): Matrix of downstream density scores.\n",
        "        upstream_density (pd.DataFrame): Matrix of upstream density scores.\n",
        "        comtrade_df (pd.DataFrame): Raw Comtrade data for filtering.\n",
        "        params (Dict[str, Any]): The 'econometric_analysis' parameters.\n",
        "        network_products (List[int]): List of products in the network's LCC.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A long-format DataFrame ready for Probit regression.\n",
        "    \"\"\"\n",
        "    logging.info(\"Assembling and filtering the final econometric dataset...\")\n",
        "\n",
        "    # --- Step 8.3.1: Filter products ---\n",
        "    # Filter by global trade value in the start year.\n",
        "    start_year = params['start_year_for_diversification']\n",
        "    trade_filter = comtrade_df[comtrade_df['year'] == start_year]\n",
        "    product_trade = trade_filter.groupby('product_hs_code')['export_value_usd'].sum()\n",
        "\n",
        "    products_above_threshold = product_trade[\n",
        "        product_trade >= params['min_global_trade_for_product_inclusion_usd']\n",
        "    ].index\n",
        "\n",
        "    # Final product set is the intersection of all criteria.\n",
        "    final_product_set = set(network_products) & set(products_above_threshold)\n",
        "    logging.info(f\"Final analysis includes {len(final_product_set)} products.\")\n",
        "\n",
        "    # --- Step 8.3.2 & 8.3.3: Melt, Merge, and Filter Sample ---\n",
        "    # Melt all DataFrames to long format for merging.\n",
        "    presence_col = f\"presence_{start_year}\"\n",
        "    base_df = diversification_df.reset_index()\n",
        "\n",
        "    downstream_long = downstream_density.melt(\n",
        "        var_name='reporter_iso', value_name='downstream_density', ignore_index=False\n",
        "    ).reset_index()\n",
        "\n",
        "    upstream_long = upstream_density.melt(\n",
        "        var_name='reporter_iso', value_name='upstream_density', ignore_index=False\n",
        "    ).reset_index()\n",
        "\n",
        "    # Merge all components together.\n",
        "    merged = pd.merge(\n",
        "        base_df, downstream_long, on=['reporter_iso', 'product_hs_code']\n",
        "    )\n",
        "    merged = pd.merge(\n",
        "        merged, upstream_long, on=['reporter_iso', 'product_hs_code']\n",
        "    )\n",
        "\n",
        "    # Apply product filter.\n",
        "    final_df = merged[merged['product_hs_code'].isin(final_product_set)]\n",
        "\n",
        "    # Apply the final sample filter: only include observations that were\n",
        "    # NOT present in the start year.\n",
        "    final_df = final_df[~final_df[presence_col]].copy()\n",
        "\n",
        "    logging.info(f\"Final regression sample contains {len(final_df):,} observations.\")\n",
        "\n",
        "    return final_df.drop(columns=[presence_col])\n",
        "\n",
        "# =============================================================================\n",
        "# Task 8: Orchestrator Function\n",
        "# =============================================================================\n",
        "\n",
        "def engineer_economic_features(\n",
        "    comtrade_exports_frame: pd.DataFrame,\n",
        "    country_data_frame: pd.DataFrame,\n",
        "    adj_matrix: csr_matrix,\n",
        "    product_to_idx: Dict[int, int],\n",
        "    replication_manifest: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the full feature engineering pipeline for econometric analysis.\n",
        "\n",
        "    Args:\n",
        "        comtrade_exports_frame (pd.DataFrame): Country-product export data.\n",
        "        country_data_frame (pd.DataFrame): Country population data.\n",
        "        adj_matrix (csr_matrix): The inferred network adjacency matrix.\n",
        "        product_to_idx (Dict[int, int]): Mapping from product HS code to index.\n",
        "        replication_manifest (Dict[str, Any]): The dictionary of study parameters.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The final, long-format dataset ready for regression.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting Task 8: Economic Feature Engineering...\")\n",
        "    params = replication_manifest['parameters']['econometric_analysis']\n",
        "\n",
        "    # Step 8.1: Construct country-product presence and diversification outcome.\n",
        "    rpop_df = _calculate_rpop(\n",
        "        comtrade_df=comtrade_exports_frame,\n",
        "        country_df=country_data_frame,\n",
        "        years=[params['start_year_for_diversification'], params['end_year_for_diversification']]\n",
        "    )\n",
        "    diversification_df = _construct_diversification_outcome(rpop_df, params)\n",
        "\n",
        "    # Step 8.2: Calculate network density metrics.\n",
        "    # Create the base year presence matrix (products x countries).\n",
        "    presence_matrix_2016 = diversification_df[[f\"presence_{params['start_year_for_diversification']}\"]].unstack(level='reporter_iso')\n",
        "    presence_matrix_2016.columns = presence_matrix_2016.columns.droplevel(0)\n",
        "\n",
        "    downstream_density = _calculate_network_density(\n",
        "        adj_matrix, presence_matrix_2016, product_to_idx, 'downstream', params['density_metric_top_k_edges']\n",
        "    )\n",
        "    upstream_density = _calculate_network_density(\n",
        "        adj_matrix, presence_matrix_2016, product_to_idx, 'upstream', params['density_metric_top_k_edges']\n",
        "    )\n",
        "\n",
        "    # Step 8.3: Assemble the final econometric dataset.\n",
        "    network_products = list(product_to_idx.keys())\n",
        "    econometric_df = _prepare_econometric_dataset(\n",
        "        diversification_df, downstream_density, upstream_density,\n",
        "        comtrade_exports_frame, params, network_products\n",
        "    )\n",
        "\n",
        "    logging.info(\"Task 8 successfully completed. Economic features engineered.\")\n",
        "    return econometric_df\n"
      ],
      "metadata": {
        "id": "wQTbGU8bLLed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Econometric Analysis\n",
        "\n",
        "# =============================================================================\n",
        "# Task 9.1: Probit Model Specification and Estimation\n",
        "# =============================================================================\n",
        "\n",
        "def _estimate_probit_model(\n",
        "    econometric_df: pd.DataFrame,\n",
        "    density_variable: str,\n",
        "    include_fixed_effects: bool = True\n",
        ") -> ProbitResults:\n",
        "    \"\"\"\n",
        "    Estimates a Probit model for the diversification outcome.\n",
        "\n",
        "    This function specifies and fits a Probit model according to the paper's\n",
        "    methodology. The dependent variable is 'diversification_outcome', and the\n",
        "    primary independent variable is a specified network density metric.\n",
        "    The model can optionally include country and product fixed effects.\n",
        "\n",
        "    The estimated model is:\n",
        "    R_p,c = Φ(α + β * density_p,c + γ_p + η_c)\n",
        "\n",
        "    Args:\n",
        "        econometric_df (pd.DataFrame): The final, long-format dataset.\n",
        "        density_variable (str): The name of the density column to use as the\n",
        "                                predictor ('downstream_density' or 'upstream_density').\n",
        "        include_fixed_effects (bool): If True, includes country and product\n",
        "                                      fixed effects in the model.\n",
        "\n",
        "    Returns:\n",
        "        ProbitResults: The fitted model results object from statsmodels.\n",
        "\n",
        "    Raises:\n",
        "        DataValidationError: If the specified density_variable is not in the DataFrame.\n",
        "    \"\"\"\n",
        "    # Validate that the required density variable exists.\n",
        "    if density_variable not in econometric_df.columns:\n",
        "        raise DataValidationError(f\"Density variable '{density_variable}' not found in the dataset.\")\n",
        "\n",
        "    # Construct the model formula using the R-style formula API.\n",
        "    if include_fixed_effects:\n",
        "        # Formula with country (reporter_iso) and product fixed effects.\n",
        "        # C() treats the variables as categorical for dummy creation.\n",
        "        formula = (\n",
        "            f\"diversification_outcome ~ {density_variable} + \"\n",
        "            \"C(reporter_iso) + C(product_hs_code)\"\n",
        "        )\n",
        "        logging.info(f\"Estimating Probit model with fixed effects: {formula}\")\n",
        "    else:\n",
        "        # Formula without fixed effects for baseline comparison.\n",
        "        formula = f\"diversification_outcome ~ {density_variable}\"\n",
        "        logging.info(f\"Estimating Probit model without fixed effects: {formula}\")\n",
        "\n",
        "    # Instantiate the Probit model.\n",
        "    model = smf.probit(formula=formula, data=econometric_df)\n",
        "\n",
        "    # Fit the model using Maximum Likelihood Estimation.\n",
        "    # Note: The paper does not specify clustered errors, but it's a robust\n",
        "    # choice. For strict replication, we will use standard errors first,\n",
        "    # but the code structure allows for easy extension to clustered errors.\n",
        "    # To use clustered errors:\n",
        "    # groups = econometric_df['reporter_iso']\n",
        "    # results = model.fit(cov_type='cluster', cov_kwds={'groups': groups})\n",
        "    try:\n",
        "        results = model.fit(disp=0) # disp=0 suppresses convergence messages\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Probit model estimation failed for '{density_variable}': {e}\")\n",
        "        raise DataValidationError(\"Probit model failed to converge.\") from e\n",
        "\n",
        "    logging.info(f\"Successfully estimated Probit model for '{density_variable}'.\")\n",
        "    return results\n",
        "\n",
        "# =============================================================================\n",
        "# Task 9.2: Model Performance Evaluation\n",
        "# =============================================================================\n",
        "\n",
        "def _evaluate_model_performance(\n",
        "    results: ProbitResults,\n",
        "    econometric_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluates the predictive performance of a fitted Probit model.\n",
        "\n",
        "    This function calculates key performance metrics, including the Area Under\n",
        "    the ROC Curve (AUC), and generates the data for plotting the ROC curve.\n",
        "\n",
        "    Args:\n",
        "        results (ProbitResults): The fitted model results object.\n",
        "        econometric_df (pd.DataFrame): The dataset used for fitting.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing performance metrics, including\n",
        "                        'auc', 'roc_curve_data', and other model diagnostics.\n",
        "    \"\"\"\n",
        "    # Get the true outcomes (dependent variable).\n",
        "    y_true = econometric_df[results.model.endog_names]\n",
        "\n",
        "    # Get the predicted probabilities from the model.\n",
        "    y_pred_prob = results.predict(econometric_df)\n",
        "\n",
        "    # --- Step 9.2.1: Calculate AUC ---\n",
        "    auc = roc_auc_score(y_true, y_pred_prob)\n",
        "\n",
        "    # --- Step 9.2.2: Generate ROC Curve data ---\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
        "\n",
        "    # --- Step 9.2.3: Compute additional model diagnostics ---\n",
        "    diagnostics = {\n",
        "        'auc': auc,\n",
        "        'roc_curve_data': (fpr, tpr),\n",
        "        'log_likelihood': results.llf,\n",
        "        'pseudo_r_squared': results.prsquared,\n",
        "        'aic': results.aic,\n",
        "        'bic': results.bic\n",
        "    }\n",
        "\n",
        "    logging.info(f\"Model Performance: AUC = {auc:.4f}, Pseudo R^2 = {results.prsquared:.4f}\")\n",
        "    return diagnostics\n",
        "\n",
        "# =============================================================================\n",
        "# Task 9.3: Results Interpretation and Validation\n",
        "# =============================================================================\n",
        "\n",
        "def _interpret_and_summarize_results(\n",
        "    results: ProbitResults,\n",
        "    density_variable: str\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Extracts, interprets, and summarizes key econometric results.\n",
        "\n",
        "    This function extracts the coefficient, standard error, p-value, and\n",
        "    calculates the Average Marginal Effect (AME) for the primary variable of\n",
        "    interest (the density metric).\n",
        "\n",
        "    Args:\n",
        "        results (ProbitResults): The fitted model results object.\n",
        "        density_variable (str): The name of the primary predictor variable.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the key statistical results\n",
        "                        and their interpretation.\n",
        "    \"\"\"\n",
        "    # --- Step 9.3.1: Extract coefficient estimates ---\n",
        "    summary = {\n",
        "        'coefficient': results.params.get(density_variable),\n",
        "        'std_error': results.bse.get(density_variable),\n",
        "        'p_value': results.pvalues.get(density_variable),\n",
        "        'conf_int': results.conf_int().loc[density_variable].tolist()\n",
        "    }\n",
        "\n",
        "    # --- Step 9.3.2: Calculate Average Marginal Effects (AME) ---\n",
        "    # This is the most meaningful way to interpret the magnitude of the effect.\n",
        "    marginal_effects = results.get_margeff()\n",
        "    summary['average_marginal_effect'] = marginal_effects.summary().tables[1].data[1][1]\n",
        "\n",
        "    logging.info(f\"--- Results for '{density_variable}' ---\")\n",
        "    logging.info(f\"  Coefficient: {summary['coefficient']:.4f}\")\n",
        "    logging.info(f\"  P-value: {summary['p_value']:.4f}\")\n",
        "    logging.info(f\"  Average Marginal Effect: {summary['average_marginal_effect']:.4f}\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "# =============================================================================\n",
        "# Task 9: Orchestrator Function\n",
        "# =============================================================================\n",
        "\n",
        "def run_econometric_analysis(\n",
        "    econometric_df: pd.DataFrame,\n",
        "    replication_manifest: Dict[str, Any]\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end econometric analysis pipeline.\n",
        "\n",
        "    This function runs the Probit regressions for both upstream and downstream\n",
        "    density metrics, evaluates their performance, and summarizes the results\n",
        "    in a structured format, replicating the core analysis of the paper.\n",
        "\n",
        "    Args:\n",
        "        econometric_df (pd.DataFrame): The final, analysis-ready dataset from Task 8.\n",
        "        replication_manifest (Dict[str, Any]): The dictionary of study parameters.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Dict[str, Any]]: A nested dictionary containing the full\n",
        "                                   results for both the 'downstream' and\n",
        "                                   'upstream' models.\n",
        "    \"\"\"\n",
        "    logging.info(\"Starting Task 9: Econometric Analysis...\")\n",
        "\n",
        "    final_results = {}\n",
        "    density_variables = ['downstream_density', 'upstream_density']\n",
        "\n",
        "    for density_var in density_variables:\n",
        "        model_name = density_var.replace('_density', '')\n",
        "        logging.info(f\"\\n===== Running Analysis for {model_name.upper()} Model =====\")\n",
        "\n",
        "        # --- Step 9.1: Estimate the Probit model ---\n",
        "        # The paper's main results include fixed effects.\n",
        "        model_results = _estimate_probit_model(\n",
        "            econometric_df=econometric_df,\n",
        "            density_variable=density_var,\n",
        "            include_fixed_effects=True\n",
        "        )\n",
        "\n",
        "        # --- Step 9.2: Evaluate model performance ---\n",
        "        performance_metrics = _evaluate_model_performance(\n",
        "            results=model_results,\n",
        "            econometric_df=econometric_df\n",
        "        )\n",
        "\n",
        "        # --- Step 9.3: Interpret and summarize results ---\n",
        "        interpretation = _interpret_and_summarize_results(\n",
        "            results=model_results,\n",
        "            density_variable=density_var\n",
        "        )\n",
        "\n",
        "        # Consolidate all results for this model.\n",
        "        final_results[model_name] = {\n",
        "            'model_summary': interpretation,\n",
        "            'performance': performance_metrics,\n",
        "            'statsmodels_results': model_results # For full inspection\n",
        "        }\n",
        "\n",
        "    logging.info(\"\\nTask 9 successfully completed. Econometric analysis finished.\")\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "ANTwJtVxL6-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Orchestrator Function Creation\n",
        "\n",
        "# =============================================================================\n",
        "# Task 10: Orchestrator Function Creation\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "def run_end_to_end_pipeline(\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    firm_metadata_frame: pd.DataFrame,\n",
        "    comtrade_exports_frame: pd.DataFrame,\n",
        "    country_data_frame: pd.DataFrame,\n",
        "    supply_chains_definitions_dict: Dict[str, Any],\n",
        "    replication_manifest: Dict[str, Any],\n",
        "    external_networks: Dict[str, Any] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the full, end-to-end research pipeline from data validation to\n",
        "    final econometric analysis.\n",
        "\n",
        "    This master orchestrator function serves as the single entry point for\n",
        "    replicating the study \"Deciphering the global production network from\n",
        "    cross-border firm transactions\". It sequentially executes each of the nine\n",
        "    major tasks, handling the flow of data between them and aggregating the\n",
        "    final results into a comprehensive output dictionary.\n",
        "\n",
        "    Args:\n",
        "        transactions_log_frame (pd.DataFrame): Raw log of firm transactions.\n",
        "        firm_metadata_frame (pd.DataFrame): Raw firm ownership and location data.\n",
        "        comtrade_exports_frame (pd.DataFrame): Raw country-product export data.\n",
        "        country_data_frame (pd.DataFrame): Raw country population data.\n",
        "        supply_chains_definitions_dict (Dict[str, Any]): Definitions of\n",
        "            manual supply chains for validation.\n",
        "        replication_manifest (Dict[str, Any]): Dictionary of all study parameters.\n",
        "        external_networks (Dict[str, Any], optional): A dictionary containing\n",
        "            loaded external networks for comparison. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing all key outputs of the\n",
        "                        pipeline, including the final network objects, analysis\n",
        "                        results, validation metrics, and econometric findings.\n",
        "\n",
        "    Raises:\n",
        "        DataValidationError: If any critical validation or processing step fails.\n",
        "        Exception: For any other unexpected runtime errors.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Set up a comprehensive logger for the entire pipeline run.\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - [Task Orchestrator] - %(message)s'\n",
        "    )\n",
        "\n",
        "    # Initialize a dictionary to store all final results.\n",
        "    pipeline_results = {}\n",
        "\n",
        "    logging.info(\"===== STARTING END-TO-END RESEARCH PIPELINE =====\")\n",
        "\n",
        "    try:\n",
        "        # --- Task 1: Data Validation and Quality Assurance ---\n",
        "        # This step validates all inputs. It returns None on success or raises\n",
        "        # a DataValidationError on failure, halting the pipeline early.\n",
        "        validate_and_assess_inputs(\n",
        "            transactions_log_frame=transactions_log_frame,\n",
        "            firm_metadata_frame=firm_metadata_frame,\n",
        "            comtrade_exports_frame=comtrade_exports_frame,\n",
        "            country_data_frame=country_data_frame,\n",
        "            replication_manifest=replication_manifest\n",
        "        )\n",
        "\n",
        "        # --- Task 2: Data Preprocessing and Cleansing ---\n",
        "        # This step cleans the raw transaction data.\n",
        "        cleansed_df = preprocess_and_cleanse_data(\n",
        "            transactions_log_frame=transactions_log_frame,\n",
        "            firm_metadata_frame=firm_metadata_frame,\n",
        "            replication_manifest=replication_manifest\n",
        "        )\n",
        "\n",
        "        # --- Task 3: Firm Classification and Set Construction ---\n",
        "        # This step creates the fundamental sets for network inference.\n",
        "        producer_sets, purchaser_sets, intersection_metrics = classify_firms_and_construct_sets(\n",
        "            cleansed_df=cleansed_df\n",
        "        )\n",
        "\n",
        "        # --- Task 4: Network Inference Implementation ---\n",
        "        # This is the core step where the network is built.\n",
        "        adj_matrix, graph, product_to_idx = infer_and_construct_network(\n",
        "            cleansed_df=cleansed_df,\n",
        "            producer_sets=producer_sets,\n",
        "            purchaser_sets=purchaser_sets,\n",
        "            intersection_metrics=intersection_metrics,\n",
        "            replication_manifest=replication_manifest\n",
        "        )\n",
        "        # Store the primary network artifacts.\n",
        "        pipeline_results['network_objects'] = {\n",
        "            'adjacency_matrix': adj_matrix,\n",
        "            'graph': graph,\n",
        "            'product_to_idx_map': product_to_idx\n",
        "        }\n",
        "\n",
        "        # --- Task 5: Community Detection and Structural Analysis ---\n",
        "        # This step analyzes the network's meso-scale structure.\n",
        "        community_labels = analyze_network_structure(\n",
        "            graph=graph,\n",
        "            adj_matrix=adj_matrix,\n",
        "            product_to_idx=product_to_idx,\n",
        "            replication_manifest=replication_manifest\n",
        "        )\n",
        "        pipeline_results['network_analysis'] = {'community_labels': community_labels}\n",
        "\n",
        "        # --- Task 6: Centrality Calculations ---\n",
        "        # This step calculates node-level importance metrics.\n",
        "        centrality_df = calculate_centralities(\n",
        "            graph=graph,\n",
        "            replication_manifest=replication_manifest\n",
        "        )\n",
        "        pipeline_results['network_analysis']['centralities'] = centrality_df\n",
        "\n",
        "        # --- Task 7: Validation Procedures ---\n",
        "        # This step validates the network against statistical null models.\n",
        "        validation_results = run_validation_procedures(\n",
        "            adj_matrix=adj_matrix,\n",
        "            product_to_idx=product_to_idx,\n",
        "            supply_chains_definitions=supply_chains_definitions_dict,\n",
        "            replication_manifest=replication_manifest,\n",
        "            external_networks=external_networks\n",
        "        )\n",
        "        pipeline_results['validation_results'] = validation_results\n",
        "\n",
        "        # --- Task 8: Economic Feature Engineering ---\n",
        "        # This step prepares the data for the final econometric analysis.\n",
        "        econometric_df = engineer_economic_features(\n",
        "            comtrade_exports_frame=comtrade_exports_frame,\n",
        "            country_data_frame=country_data_frame,\n",
        "            adj_matrix=adj_matrix,\n",
        "            product_to_idx=product_to_idx,\n",
        "            replication_manifest=replication_manifest\n",
        "        )\n",
        "        pipeline_results['econometric_dataset'] = econometric_df\n",
        "\n",
        "        # --- Task 9: Econometric Analysis ---\n",
        "        # This step runs the final Probit models.\n",
        "        econometric_results = run_econometric_analysis(\n",
        "            econometric_df=econometric_df,\n",
        "            replication_manifest=replication_manifest\n",
        "        )\n",
        "        pipeline_results['econometric_results'] = econometric_results\n",
        "\n",
        "        logging.info(\"===== END-TO-END RESEARCH PIPELINE COMPLETED SUCCESSFULLY =====\")\n",
        "\n",
        "    except DataValidationError as e:\n",
        "        # Catch specific validation errors and log them before re-raising.\n",
        "        logging.error(f\"A critical data validation error occurred: {e}\")\n",
        "        logging.error(\"PIPELINE HALTED.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        # Catch any other unexpected errors.\n",
        "        logging.error(f\"An unexpected error occurred during pipeline execution: {e}\", exc_info=True)\n",
        "        logging.error(\"PIPELINE HALTED.\")\n",
        "        raise\n",
        "\n",
        "    # Return the final, aggregated results.\n",
        "    return pipeline_results\n"
      ],
      "metadata": {
        "id": "ngUYJrFgWCUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Robustness Analysis\n",
        "\n",
        "# =============================================================================\n",
        "# Task 11.1: Parameter Sensitivity Analysis\n",
        "# =============================================================================\n",
        "\n",
        "def _update_nested_dict(\n",
        "    d: Dict[str, Any],\n",
        "    path: Tuple[str, ...],\n",
        "    value: Any\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    A robust helper utility to set a value in a nested dictionary using a path tuple.\n",
        "\n",
        "    This function traverses a nested dictionary according to the keys provided\n",
        "    in the path tuple and sets the value at the final destination. It includes\n",
        "    error handling to ensure the path is valid.\n",
        "\n",
        "    Args:\n",
        "        d (Dict[str, Any]): The nested dictionary to update.\n",
        "        path (Tuple[str, ...]): A tuple of keys representing the path to the\n",
        "                                value to be updated.\n",
        "        value (Any): The new value to set at the specified path.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If any key in the path (except the last one) does not exist\n",
        "                  or does not lead to a sub-dictionary.\n",
        "    \"\"\"\n",
        "    # Start the traversal from the top level of the dictionary.\n",
        "    current_level = d\n",
        "    # Iterate through the path until the second-to-last key.\n",
        "    for key in path[:-1]:\n",
        "        # Check if the key exists and leads to another dictionary.\n",
        "        if key in current_level and isinstance(current_level[key], dict):\n",
        "            # Move one level deeper into the dictionary.\n",
        "            current_level = current_level[key]\n",
        "        else:\n",
        "            # If the path is invalid, raise a specific KeyError.\n",
        "            raise KeyError(f\"Invalid path in manifest: key '{key}' not found or not a dict.\")\n",
        "\n",
        "    # Get the final key in the path.\n",
        "    final_key = path[-1]\n",
        "    # Check if the final key exists at the target level.\n",
        "    if final_key not in current_level:\n",
        "        raise KeyError(f\"Invalid path in manifest: final key '{final_key}' not found.\")\n",
        "\n",
        "    # Set the value of the final key.\n",
        "    current_level[final_key] = value\n",
        "\n",
        "\n",
        "def _run_single_pipeline_iteration(\n",
        "    params_combination: Dict[Tuple[str, ...], Any],\n",
        "    base_manifest: Dict[str, Any],\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    firm_metadata_frame: pd.DataFrame,\n",
        "    comtrade_exports_frame: pd.DataFrame,\n",
        "    country_data_frame: pd.DataFrame,\n",
        "    supply_chains_definitions_dict: Dict[str, Any],\n",
        "    external_networks: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Worker function to execute one full pipeline run with a specific, robustly-\n",
        "    defined parameter set.\n",
        "\n",
        "    This function is designed for general-purpose sensitivity analysis.\n",
        "    It accepts a dictionary of parameters where keys are tuples representing the\n",
        "    path to the parameter in the nested manifest. It updates a deep copy of the\n",
        "    base manifest, executes the end-to-end pipeline, and extracts key results.\n",
        "\n",
        "    Args:\n",
        "        params_combination (Dict[Tuple[str, ...], Any]): A dictionary where keys\n",
        "            are path tuples (e.g., ('network_inference', 'primary_firmcount_threshold'))\n",
        "            and values are the specific settings for this iteration.\n",
        "        base_manifest (Dict[str, Any]): The original, unmodified replication manifest.\n",
        "        **kwargs: All the raw data inputs required by the main pipeline.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the parameters for this run\n",
        "                        (with flattened, dot-notation keys) and the key extracted\n",
        "                        results. Returns a dictionary with an 'error' key if the\n",
        "                        run fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a deep copy of the manifest to ensure run-to-run isolation.\n",
        "        run_manifest = deepcopy(base_manifest)\n",
        "\n",
        "        # Iterate through the parameter combination and update the manifest copy.\n",
        "        for path, value in params_combination.items():\n",
        "            # Use the robust helper function to update the nested dictionary.\n",
        "            _update_nested_dict(run_manifest, path, value)\n",
        "\n",
        "        # Execute the full end-to-end pipeline with the modified manifest.\n",
        "        pipeline_results = run_end_to_end_pipeline(\n",
        "            transactions_log_frame=transactions_log_frame,\n",
        "            firm_metadata_frame=firm_metadata_frame,\n",
        "            comtrade_exports_frame=comtrade_exports_frame,\n",
        "            country_data_frame=country_data_frame,\n",
        "            supply_chains_definitions_dict=supply_chains_definitions_dict,\n",
        "            replication_manifest=run_manifest,\n",
        "            external_networks=external_networks\n",
        "        )\n",
        "\n",
        "        # Extract the key results for this run.\n",
        "        downstream_auc = pipeline_results['econometric_results']['downstream']['performance']['auc']\n",
        "        upstream_auc = pipeline_results['econometric_results']['upstream']['performance']['auc']\n",
        "        edge_count = pipeline_results['network_objects']['graph'].number_of_edges()\n",
        "\n",
        "        # Flatten the parameter path tuples into dot-notation strings for the output.\n",
        "        flattened_params = {'.'.join(path): value for path, value in params_combination.items()}\n",
        "\n",
        "        # Combine the flattened parameters and results into a single record.\n",
        "        result_record = {\n",
        "            **flattened_params,\n",
        "            'downstream_auc': downstream_auc,\n",
        "            'upstream_auc': upstream_auc,\n",
        "            'edge_count': edge_count,\n",
        "            'error': None\n",
        "        }\n",
        "        return result_record\n",
        "\n",
        "    except Exception as e:\n",
        "        # If any part of the pipeline fails, log the error and return a record\n",
        "        # indicating failure.\n",
        "        flattened_params = {'.'.join(path): value for path, value in params_combination.items()}\n",
        "        logging.error(f\"Pipeline run failed for parameters {flattened_params}: {e}\")\n",
        "        return {**flattened_params, 'error': str(e)}\n",
        "\n",
        "\n",
        "def run_parameter_sensitivity_analysis(\n",
        "    parameter_grid: Dict[str, List[Any]],\n",
        "    base_manifest: Dict[str, Any],\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    firm_metadata_frame: pd.DataFrame,\n",
        "    comtrade_exports_frame: pd.DataFrame,\n",
        "    country_data_frame: pd.DataFrame,\n",
        "    supply_chains_definitions_dict: Dict[str, Any],\n",
        "    external_networks: Dict[str, Any] = None,\n",
        "    n_jobs: int = -1\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a sensitivity analysis by running the full pipeline across a grid\n",
        "    of specified parameter values.\n",
        "\n",
        "    This function systematically varies key methodological parameters (e.g.,\n",
        "    network sparsification thresholds), executes the entire research pipeline\n",
        "    for each combination, and collects key performance indicators (like model\n",
        "    AUC and network edge count) to assess the robustness of the findings.\n",
        "\n",
        "    Args:\n",
        "        parameter_grid (Dict[str, List[Any]]): A dictionary defining the\n",
        "            parameter sweep. Keys are parameter names (e.g.,\n",
        "            'primary_firmcount_threshold'), and values are lists of values to test.\n",
        "        base_manifest (Dict[str, Any]): The base replication manifest with\n",
        "                                        default parameter settings.\n",
        "        **kwargs: All the raw data inputs required by the main pipeline.\n",
        "        n_jobs (int): The number of CPU cores to use for parallel execution.\n",
        "                      -1 means using all available cores. 1 means no parallelism.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A tidy DataFrame where each row corresponds to one full\n",
        "                      pipeline run, with columns for the parameters used and\n",
        "                      the resulting performance metrics.\n",
        "    \"\"\"\n",
        "    logging.info(\"===== STARTING TASK 11.1: PARAMETER SENSITIVITY ANALYSIS =====\")\n",
        "\n",
        "    # --- Step 1: Generate all parameter combinations from the grid ---\n",
        "    param_names = parameter_grid.keys()\n",
        "    param_values = parameter_grid.values()\n",
        "\n",
        "    # itertools.product creates the Cartesian product of the parameter values.\n",
        "    all_combinations = [\n",
        "        dict(zip(param_names, combo)) for combo in itertools.product(*param_values)\n",
        "    ]\n",
        "    logging.info(f\"Generated {len(all_combinations)} parameter combinations to test.\")\n",
        "\n",
        "    # --- Step 2: Execute the pipeline for each combination in parallel ---\n",
        "    # Use joblib.Parallel for robust and easy-to-use parallel processing.\n",
        "    # `delayed` is a wrapper that makes the worker function picklable.\n",
        "    if n_jobs == 1:\n",
        "        logging.info(\"Running iterations sequentially...\")\n",
        "        results_list = [\n",
        "            _run_single_pipeline_iteration(\n",
        "                params, base_manifest, transactions_log_frame, firm_metadata_frame,\n",
        "                comtrade_exports_frame, country_data_frame,\n",
        "                supply_chains_definitions_dict, external_networks\n",
        "            ) for params in all_combinations\n",
        "        ]\n",
        "    else:\n",
        "        logging.info(f\"Running iterations in parallel on {n_jobs if n_jobs > 0 else cpu_count()} cores...\")\n",
        "        results_list = Parallel(n_jobs=n_jobs)(\n",
        "            delayed(_run_single_pipeline_iteration)(\n",
        "                params, base_manifest, transactions_log_frame, firm_metadata_frame,\n",
        "                comtrade_exports_frame, country_data_frame,\n",
        "                supply_chains_definitions_dict, external_networks\n",
        "            ) for params in all_combinations\n",
        "        )\n",
        "\n",
        "    # --- Step 3: Aggregate results into a final DataFrame ---\n",
        "    # Filter out any runs that may have failed completely (though the worker\n",
        "    # function is designed to return an error message instead).\n",
        "    successful_results = [r for r in results_list if r is not None]\n",
        "\n",
        "    if not successful_results:\n",
        "        logging.error(\"All pipeline runs failed during sensitivity analysis.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    results_df = pd.DataFrame(successful_results)\n",
        "\n",
        "    logging.info(\"===== PARAMETER SENSITIVITY ANALYSIS COMPLETED =====\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Task 11.2: Temporal Window Robustness Testing\n",
        "# =============================================================================\n",
        "\n",
        "def _extract_key_econometric_results(\n",
        "    pipeline_results: Dict[str, Any]\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    A helper function to extract key econometric metrics from a pipeline result.\n",
        "\n",
        "    Args:\n",
        "        pipeline_results (Dict[str, Any]): The output dictionary from a full\n",
        "                                           pipeline run.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A flattened dictionary with key performance and\n",
        "                          model summary statistics.\n",
        "    \"\"\"\n",
        "    # Initialize an empty dictionary to store the extracted results.\n",
        "    extracted = {}\n",
        "\n",
        "    # Check if the expected results are present.\n",
        "    if 'econometric_results' in pipeline_results:\n",
        "        # Iterate through both the 'downstream' and 'upstream' models.\n",
        "        for model_name, results in pipeline_results['econometric_results'].items():\n",
        "            # Extract performance metrics, providing None as a default.\n",
        "            performance = results.get('performance', {})\n",
        "            extracted[f'{model_name}_auc'] = performance.get('auc')\n",
        "\n",
        "            # Extract model summary statistics, providing None as a default.\n",
        "            summary = results.get('model_summary', {})\n",
        "            extracted[f'{model_name}_coeff'] = summary.get('coefficient')\n",
        "            extracted[f'{model_name}_p_value'] = summary.get('p_value')\n",
        "            extracted[f'{model_name}_ame'] = summary.get('average_marginal_effect')\n",
        "\n",
        "        # Extract the final sample size for this econometric run.\n",
        "        if 'econometric_dataset' in pipeline_results:\n",
        "            extracted['sample_size'] = len(pipeline_results['econometric_dataset'])\n",
        "\n",
        "    return extracted\n",
        "\n",
        "\n",
        "def run_temporal_robustness_analysis(\n",
        "    base_manifest: Dict[str, Any],\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    firm_metadata_frame: pd.DataFrame,\n",
        "    comtrade_exports_frame: pd.DataFrame,\n",
        "    country_data_frame: pd.DataFrame,\n",
        "    supply_chains_definitions_dict: Dict[str, Any],\n",
        "    external_networks: Dict[str, Any] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a temporal robustness analysis by comparing the baseline results\n",
        "    to results from an alternative, longer time window.\n",
        "\n",
        "    This function executes the full end-to-end pipeline twice:\n",
        "    1.  With the baseline econometric time window (e.g., 2016-2021).\n",
        "    2.  With an alternative, longer time window (e.g., 2011-2021).\n",
        "\n",
        "    It then extracts key econometric findings (AUC, coefficients, p-values)\n",
        "    from both runs and presents them in a comparative DataFrame to assess the\n",
        "    robustness of the conclusions to the choice of time period.\n",
        "\n",
        "    Args:\n",
        "        base_manifest (Dict[str, Any]): The base replication manifest with\n",
        "                                        default parameter settings.\n",
        "        **kwargs: All the raw data inputs required by the main pipeline.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame comparing the key econometric results\n",
        "                      across the two temporal windows.\n",
        "\n",
        "    Raises:\n",
        "        DataValidationError: If any of the pipeline runs fail critically.\n",
        "    \"\"\"\n",
        "    logging.info(\"===== STARTING TASK 11.2: TEMPORAL ROBUSTNESS ANALYSIS =====\")\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    # --- Run 1: Baseline Analysis (5-Year Window) ---\n",
        "    logging.info(\"\\n--- Running Baseline Analysis (5-Year Window: 2016-2021) ---\")\n",
        "    try:\n",
        "        # Execute the pipeline with the original, unmodified manifest.\n",
        "        baseline_results = run_end_to_end_pipeline(\n",
        "            transactions_log_frame=transactions_log_frame,\n",
        "            firm_metadata_frame=firm_metadata_frame,\n",
        "            comtrade_exports_frame=comtrade_exports_frame,\n",
        "            country_data_frame=country_data_frame,\n",
        "            supply_chains_definitions_dict=supply_chains_definitions_dict,\n",
        "            replication_manifest=base_manifest,\n",
        "            external_networks=external_networks\n",
        "        )\n",
        "        # Extract and store the key results.\n",
        "        all_results['Baseline_5_Year_Window'] = _extract_key_econometric_results(baseline_results)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Baseline pipeline run failed: {e}\")\n",
        "        # If the baseline fails, we cannot proceed.\n",
        "        raise\n",
        "\n",
        "    # --- Run 2: Alternative Analysis (10-Year Window) ---\n",
        "    logging.info(\"\\n--- Running Robustness Analysis (10-Year Window: 2011-2021) ---\")\n",
        "    try:\n",
        "        # Create a deep copy of the manifest to modify for the alternative run.\n",
        "        robustness_manifest = deepcopy(base_manifest)\n",
        "\n",
        "        # Modify the econometric analysis years.\n",
        "        robustness_manifest['parameters']['econometric_analysis']['start_year_for_diversification'] = 2011\n",
        "        robustness_manifest['parameters']['econometric_analysis']['end_year_for_diversification'] = 2021\n",
        "\n",
        "        # Execute the pipeline with the modified manifest.\n",
        "        robustness_results = run_end_to_end_pipeline(\n",
        "            transactions_log_frame=transactions_log_frame,\n",
        "            firm_metadata_frame=firm_metadata_frame,\n",
        "            comtrade_exports_frame=comtrade_exports_frame,\n",
        "            country_data_frame=country_data_frame,\n",
        "            supply_chains_definitions_dict=supply_chains_definitions_dict,\n",
        "            replication_manifest=robustness_manifest,\n",
        "            external_networks=external_networks\n",
        "        )\n",
        "        # Extract and store the key results.\n",
        "        all_results['Robustness_10_Year_Window'] = _extract_key_econometric_results(robustness_results)\n",
        "    except Exception as e:\n",
        "        # If the robustness run fails, log it but don't halt. We can still\n",
        "        # report the baseline results.\n",
        "        logging.error(f\"Temporal robustness pipeline run failed: {e}\")\n",
        "        all_results['Robustness_10_Year_Window'] = {}\n",
        "\n",
        "    # --- Step 3: Aggregate results into a final comparative DataFrame ---\n",
        "    # Convert the nested dictionary of results into a DataFrame for easy comparison.\n",
        "    results_df = pd.DataFrame(all_results).T # Transpose to have scenarios as rows.\n",
        "\n",
        "    logging.info(\"\\n===== TEMPORAL ROBUSTNESS ANALYSIS COMPLETED =====\")\n",
        "\n",
        "    # Display the comparative results.\n",
        "    print(\"\\n--- Comparative Temporal Robustness Results ---\")\n",
        "    print(results_df)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# =============================================================================\n",
        "# Task 11.3: Alternative Network Construction Robustness\n",
        "# =============================================================================\n",
        "\n",
        "def _identify_significant_entities_refactored(\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    entity_type: str,\n",
        "    method: Any = 'mean'\n",
        ") -> Dict[int, Set[str]]:\n",
        "    \"\"\"\n",
        "    Identifies significant producers or purchasers for each product using a\n",
        "    configurable statistical method. (REFACTORED for Task 11.3)\n",
        "\n",
        "    This function implements the core classification methodology from the paper.\n",
        "    An entity (a firm's ultimate owner in a specific country) is classified as\n",
        "    a \"significant\" producer or purchaser of a product if its total sales or\n",
        "    purchase value for that product exceeds a statistical threshold. This\n",
        "    refactored version allows the threshold to be defined as the mean, median,\n",
        "    or a specific quantile of the total values across all entities active in\n",
        "    that product market.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df (pd.DataFrame): The preprocessed and cleansed transaction log,\n",
        "                                    containing resolved owner-country entities.\n",
        "        entity_type (str): The type of entity to identify. Must be either\n",
        "                           'producer' or 'purchaser'.\n",
        "        method (Any): The statistical method for the threshold. Can be the\n",
        "                      string 'mean', the string 'median', or a float between\n",
        "                      0 and 1 for a specific quantile (e.g., 0.75 for the\n",
        "                      75th percentile). Defaults to 'mean'.\n",
        "\n",
        "    Returns:\n",
        "        Dict[int, Set[str]]: A dictionary mapping each product HS code (int) to\n",
        "                             a set of significant entity identifiers (str).\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If `entity_type` or `method` are invalid.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Input Validation and Column Selection ---\n",
        "    # Validate the entity_type parameter to ensure correct column selection.\n",
        "    if entity_type == 'producer':\n",
        "        # For producers, the relevant entity is the seller.\n",
        "        entity_col = 'seller_owner_country_entity'\n",
        "    elif entity_type == 'purchaser':\n",
        "        # For purchasers, the relevant entity is the buyer.\n",
        "        entity_col = 'buyer_owner_country_entity'\n",
        "    else:\n",
        "        # Raise an error for invalid entity types.\n",
        "        raise ValueError(\"`entity_type` must be either 'producer' or 'purchaser'.\")\n",
        "\n",
        "    # Log the start of the process with the specified method.\n",
        "    logging.info(f\"Identifying significant {entity_type}s using '{method}' threshold...\")\n",
        "\n",
        "    # --- Step 2: Calculate total transaction value per (entity, product) pair ---\n",
        "    # Group by the product and the relevant entity column, then sum the values.\n",
        "    # This creates a summary of total activity for each entity in each product.\n",
        "    entity_product_values = cleansed_df.groupby(\n",
        "        ['product_hs_code', entity_col]\n",
        "    )['transaction_value_usd'].sum().reset_index()\n",
        "\n",
        "    # Rename the aggregated column for clarity.\n",
        "    entity_product_values.rename(\n",
        "        columns={'transaction_value_usd': 'total_value'}, inplace=True\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Determine and apply the statistical threshold function ---\n",
        "    # This block allows for flexible thresholding methods based on the `method` param.\n",
        "    if method == 'mean':\n",
        "        # Use the string 'mean' for the highly optimized transform function.\n",
        "        transform_func = 'mean'\n",
        "    elif method == 'median':\n",
        "        # Use the string 'median' for the highly optimized transform function.\n",
        "        transform_func = 'median'\n",
        "    elif isinstance(method, float) and 0 < method < 1:\n",
        "        # For a float, use a lambda function to calculate the specified quantile.\n",
        "        transform_func = lambda x: x.quantile(method)\n",
        "    else:\n",
        "        # If the method is not recognized, raise a descriptive ValueError.\n",
        "        raise ValueError(f\"Invalid classification method: '{method}'. Must be \"\n",
        "                         \"'mean', 'median', or a float between 0 and 1.\")\n",
        "\n",
        "    # Calculate the threshold for each product using the selected function.\n",
        "    # `.transform()` is highly efficient as it computes the group-wise statistic\n",
        "    # and broadcasts the result back to the original shape, avoiding a merge.\n",
        "    entity_product_values['threshold'] = entity_product_values.groupby(\n",
        "        'product_hs_code'\n",
        "    )['total_value'].transform(transform_func)\n",
        "\n",
        "    # --- Step 4: Filter for entities that exceed the calculated threshold ---\n",
        "    # Create a boolean mask to identify the significant entities for each product.\n",
        "    significant_mask = entity_product_values['total_value'] > entity_product_values['threshold']\n",
        "\n",
        "    # Apply the mask to get the final DataFrame of significant entities.\n",
        "    significant_entities_df = entity_product_values[significant_mask]\n",
        "\n",
        "    # --- Step 5: Aggregate the significant entities into sets for each product ---\n",
        "    # Group the filtered DataFrame by product code.\n",
        "    # Apply the `set` constructor to the entity column for each group. This\n",
        "    # efficiently collects all significant entities for a given product.\n",
        "    entity_sets_series = significant_entities_df.groupby(\n",
        "        'product_hs_code'\n",
        "    )[entity_col].apply(set)\n",
        "\n",
        "    # Convert the resulting pandas Series to a dictionary for fast lookups later.\n",
        "    entity_sets_dict = entity_sets_series.to_dict()\n",
        "\n",
        "    # Log a summary of the operation's result.\n",
        "    logging.info(\n",
        "        f\"Identified significant {entity_type}s for \"\n",
        "        f\"{len(entity_sets_dict):,} products using method='{method}'.\"\n",
        "    )\n",
        "\n",
        "    # Return the final dictionary of sets.\n",
        "    return entity_sets_dict\n",
        "\n",
        "def classify_firms_and_construct_sets_variant(\n",
        "    cleansed_df: pd.DataFrame,\n",
        "    classification_method: Any\n",
        ") -> Tuple[Dict[int, Set[str]], Dict[int, Set[str]], pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates firm classification using a specified statistical method.\n",
        "    (VARIANT for Task 11.3)\n",
        "\n",
        "    This function serves as a variant of the main Task 3 orchestrator,\n",
        "    allowing the method for identifying significant entities (e.g., 'mean',\n",
        "    'median', quantile) to be passed as a parameter.\n",
        "\n",
        "    Args:\n",
        "        cleansed_df (pd.DataFrame): The fully preprocessed and cleansed\n",
        "                                    transaction log from Task 2.\n",
        "        classification_method (Any): The statistical method for the threshold.\n",
        "            Can be 'mean', 'median', or a float for a quantile.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[Dict[int, Set[str]], Dict[int, Set[str]], pd.DataFrame]:\n",
        "        A tuple containing producer sets, purchaser sets, and intersection metrics.\n",
        "    \"\"\"\n",
        "    # Log the start of the task with the specified method.\n",
        "    logging.info(\n",
        "        f\"Starting Task 3 (Variant): Firm Classification with method='{classification_method}'...\"\n",
        "    )\n",
        "\n",
        "    # Step 3.1 (Variant): Identify significant producers with the specified method.\n",
        "    producer_sets = _identify_significant_entities_refactored(\n",
        "        cleansed_df=cleansed_df,\n",
        "        entity_type='producer',\n",
        "        method=classification_method\n",
        "    )\n",
        "\n",
        "    # Step 3.2 (Variant): Identify significant purchasers with the specified method.\n",
        "    purchaser_sets = _identify_significant_entities_refactored(\n",
        "        cleansed_df=cleansed_df,\n",
        "        entity_type='purchaser',\n",
        "        method=classification_method\n",
        "    )\n",
        "\n",
        "    # Step 3.3 (Standard): Compute intersection metrics based on the new sets.\n",
        "    intersection_metrics = _compute_intersection_metrics(\n",
        "        cleansed_df=cleansed_df,\n",
        "        producer_sets=producer_sets,\n",
        "        purchaser_sets=purchaser_sets\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Task 3 (Variant) successfully completed for method='{classification_method}'.\")\n",
        "\n",
        "    # Return the three essential data structures.\n",
        "    return producer_sets, purchaser_sets, intersection_metrics\n",
        "\n",
        "def run_end_to_end_pipeline_variant(\n",
        "    classification_method: Any,\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    firm_metadata_frame: pd.DataFrame,\n",
        "    comtrade_exports_frame: pd.DataFrame,\n",
        "    country_data_frame: pd.DataFrame,\n",
        "    supply_chains_definitions_dict: Dict[str, Any],\n",
        "    replication_manifest: Dict[str, Any],\n",
        "    external_networks: Dict[str, Any] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the full research pipeline with a parameterizable firm\n",
        "    classification method. (VARIANT for Task 11.3)\n",
        "\n",
        "    This master orchestrator is a fully functional equivalent of the main\n",
        "    `run_end_to_end_pipeline`. It is specifically designed for the construction\n",
        "    robustness analysis by accepting a `classification_method` parameter. This\n",
        "    parameter is injected into the Task 3 stage of the workflow, allowing the\n",
        "    entire network and subsequent analyses to be rebuilt based on different\n",
        "    foundational assumptions about what constitutes a significant firm.\n",
        "\n",
        "    Args:\n",
        "        classification_method (Any): The method ('mean', 'median', quantile)\n",
        "            to be used in the firm classification stage (Task 3).\n",
        "        transactions_log_frame (pd.DataFrame): Raw log of firm transactions.\n",
        "        firm_metadata_frame (pd.DataFrame): Raw firm ownership and location data.\n",
        "        comtrade_exports_frame (pd.DataFrame): Raw country-product export data.\n",
        "        country_data_frame (pd.DataFrame): Raw country population data.\n",
        "        supply_chains_definitions_dict (Dict[str, Any]): Definitions of\n",
        "            manual supply chains for validation.\n",
        "        replication_manifest (Dict[str, Any]): Dictionary of all study parameters.\n",
        "        external_networks (Dict[str, Any], optional): Loaded external networks\n",
        "            for comparison. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing all key outputs of the\n",
        "                        pipeline for this specific, variant run.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store all final results for this run.\n",
        "    pipeline_results = {}\n",
        "\n",
        "    # Log the start of the variant pipeline run, specifying the method.\n",
        "    logging.info(f\"===== STARTING END-TO-END PIPELINE (VARIANT: method='{classification_method}') =====\")\n",
        "\n",
        "    # --- Task 1: Data Validation and Quality Assurance ---\n",
        "    # This step is independent of the classification method and runs as standard.\n",
        "    validate_and_assess_inputs(\n",
        "        transactions_log_frame, firm_metadata_frame, comtrade_exports_frame,\n",
        "        country_data_frame, replication_manifest\n",
        "    )\n",
        "\n",
        "    # --- Task 2: Data Preprocessing and Cleansing ---\n",
        "    # This step is also independent of the classification method.\n",
        "    cleansed_df = preprocess_and_cleanse_data(\n",
        "        transactions_log_frame, firm_metadata_frame, replication_manifest\n",
        "    )\n",
        "\n",
        "    # --- Task 3 (Variant Call) ---\n",
        "    # This is the critical modification: call the new variant orchestrator for Task 3,\n",
        "    # passing the specified classification method.\n",
        "    producer_sets, purchaser_sets, intersection_metrics = classify_firms_and_construct_sets_variant(\n",
        "        cleansed_df=cleansed_df,\n",
        "        classification_method=classification_method\n",
        "    )\n",
        "\n",
        "    # --- Task 4: Network Inference Implementation ---\n",
        "    # All subsequent tasks use the outputs from the variant Task 3.\n",
        "    adj_matrix, graph, product_to_idx = infer_and_construct_network(\n",
        "        cleansed_df, producer_sets, purchaser_sets, intersection_metrics, replication_manifest\n",
        "    )\n",
        "    pipeline_results['network_objects'] = {'adjacency_matrix': adj_matrix, 'graph': graph, 'product_to_idx_map': product_to_idx}\n",
        "\n",
        "    # --- Task 5: Community Detection and Structural Analysis ---\n",
        "    community_labels = analyze_network_structure(\n",
        "        graph, adj_matrix, product_to_idx, replication_manifest\n",
        "    )\n",
        "\n",
        "    # --- Task 6: Centrality Calculations ---\n",
        "    centrality_df = calculate_centralities(graph, replication_manifest)\n",
        "    pipeline_results['network_analysis'] = {'community_labels': community_labels, 'centralities': centrality_df}\n",
        "\n",
        "    # --- Task 7: Validation Procedures ---\n",
        "    validation_results = run_validation_procedures(\n",
        "        adj_matrix, product_to_idx, supply_chains_definitions_dict, replication_manifest, external_networks\n",
        "    )\n",
        "    pipeline_results['validation_results'] = validation_results\n",
        "\n",
        "    # --- Task 8: Economic Feature Engineering ---\n",
        "    econometric_df = engineer_economic_features(\n",
        "        comtrade_exports_frame, country_data_frame, adj_matrix, product_to_idx, replication_manifest\n",
        "    )\n",
        "    pipeline_results['econometric_dataset'] = econometric_df\n",
        "\n",
        "    # --- Task 9: Econometric Analysis ---\n",
        "    econometric_results = run_econometric_analysis(\n",
        "        econometric_df, replication_manifest\n",
        "    )\n",
        "    pipeline_results['econometric_results'] = econometric_results\n",
        "\n",
        "    # Log the successful completion of this variant run.\n",
        "    logging.info(f\"===== END-TO-END PIPELINE (VARIANT: method='{classification_method}') COMPLETED SUCCESSFULLY =====\")\n",
        "\n",
        "    # Return the comprehensive results dictionary.\n",
        "    return pipeline_results\n",
        "\n",
        "def run_construction_robustness_analysis_revised(\n",
        "    methods_to_test: List[Any],\n",
        "    base_manifest: Dict[str, Any],\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    firm_metadata_frame: pd.DataFrame,\n",
        "    comtrade_exports_frame: pd.DataFrame,\n",
        "    country_data_frame: pd.DataFrame,\n",
        "    supply_chains_definitions_dict: Dict[str, Any],\n",
        "    external_networks: Dict[str, Any] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Performs a robustness analysis by reconstructing the network using\n",
        "    alternative firm classification methods. (REVISED for Task 11.3)\n",
        "\n",
        "    This function iterates through a list of methods (e.g., 'mean', 'median'),\n",
        "    invokes a variant of the end-to-end pipeline for each, and produces a\n",
        "    comparative summary of the key econometric findings.\n",
        "\n",
        "    Args:\n",
        "        methods_to_test (List[Any]): A list of classification methods to test.\n",
        "        base_manifest (Dict[str, Any]): The base replication manifest.\n",
        "        **kwargs: All the raw data inputs required by the main pipeline.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame comparing key results across the different\n",
        "                      network construction methods.\n",
        "    \"\"\"\n",
        "    logging.info(\"===== STARTING TASK 11.3: CONSTRUCTION ROBUSTNESS ANALYSIS (REVISED) =====\")\n",
        "\n",
        "    all_results = {}\n",
        "\n",
        "    # Iterate through each specified classification method.\n",
        "    for method in methods_to_test:\n",
        "        # Create a descriptive name for the run.\n",
        "        method_name = f\"Method_{method}\"\n",
        "        logging.info(f\"\\n--- Running Analysis for Construction Method: '{method}' ---\")\n",
        "\n",
        "        try:\n",
        "            # Call the pipeline variant, passing the current method.\n",
        "            pipeline_results = run_end_to_end_pipeline_variant(\n",
        "                classification_method=method,\n",
        "                transactions_log_frame=transactions_log_frame,\n",
        "                firm_metadata_frame=firm_metadata_frame,\n",
        "                comtrade_exports_frame=comtrade_exports_frame,\n",
        "                country_data_frame=country_data_frame,\n",
        "                supply_chains_definitions_dict=supply_chains_definitions_dict,\n",
        "                replication_manifest=base_manifest,\n",
        "                external_networks=external_networks\n",
        "            )\n",
        "\n",
        "            # Extract and store the key results for this method.\n",
        "            run_summary = _extract_key_econometric_results(pipeline_results)\n",
        "            # Also capture how the network structure changes.\n",
        "            run_summary['edge_count'] = pipeline_results['network_objects']['graph'].number_of_edges()\n",
        "            all_results[method_name] = run_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            # Log any failure for a specific method and continue.\n",
        "            logging.error(f\"Pipeline run failed for construction method '{method}': {e}\")\n",
        "            all_results[method_name] = {'error': str(e)}\n",
        "\n",
        "    # Aggregate results into a final comparative DataFrame.\n",
        "    results_df = pd.DataFrame(all_results).T\n",
        "\n",
        "    logging.info(\"\\n===== CONSTRUCTION ROBUSTNESS ANALYSIS COMPLETED =====\")\n",
        "\n",
        "    # Display the comparative results.\n",
        "    print(\"\\n--- Comparative Construction Robustness Results ---\")\n",
        "    print(results_df)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return results_df\n",
        "\n",
        "# =============================================================================\n",
        "# Robustness Analysis Orchestrator Function\n",
        "# =============================================================================\n",
        "\n",
        "def run_full_robustness_analysis(\n",
        "    # All raw data inputs required by the core pipeline\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    firm_metadata_frame: pd.DataFrame,\n",
        "    comtrade_exports_frame: pd.DataFrame,\n",
        "    country_data_frame: pd.DataFrame,\n",
        "    supply_chains_definitions_dict: Dict[str, Any],\n",
        "    base_manifest: Dict[str, Any],\n",
        "    external_networks: Optional[Dict[str, Any]] = None,\n",
        "    # Configuration for the specific robustness checks\n",
        "    parameter_grid: Optional[Dict[str, List[Any]]] = None,\n",
        "    methods_to_test: Optional[List[Any]] = None,\n",
        "    n_jobs: int = -1\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of a comprehensive suite of robustness analyses.\n",
        "\n",
        "    This master function serves as the single entry point for Task 11. It\n",
        "    sequentially invokes the dedicated orchestrators for each type of\n",
        "    robustness check:\n",
        "    1.  Parameter Sensitivity: Varies key network inference parameters.\n",
        "    2.  Temporal Window: Re-runs the analysis over a different time period.\n",
        "    3.  Construction Method: Re-builds the network using alternative firm\n",
        "        classification rules.\n",
        "\n",
        "    It aggregates the results from each check into a final, structured\n",
        "    dictionary, providing a complete picture of the stability and reliability\n",
        "    of the study's findings.\n",
        "\n",
        "    Args:\n",
        "        transactions_log_frame (pd.DataFrame): Raw log of firm transactions.\n",
        "        firm_metadata_frame (pd.DataFrame): Raw firm ownership and location data.\n",
        "        comtrade_exports_frame (pd.DataFrame): Raw country-product export data.\n",
        "        country_data_frame (pd.DataFrame): Raw country population data.\n",
        "        supply_chains_definitions_dict (Dict[str, Any]): Definitions of\n",
        "            manual supply chains for validation.\n",
        "        base_manifest (Dict[str, Any]): The base replication manifest with\n",
        "                                        default parameter settings.\n",
        "        external_networks (Optional[Dict[str, Any]]): Loaded external networks\n",
        "            for comparison. Defaults to None.\n",
        "        parameter_grid (Optional[Dict[str, List[Any]]]): Configuration for the\n",
        "            parameter sensitivity analysis. If None, this check is skipped.\n",
        "        methods_to_test (Optional[List[Any]]): Configuration for the construction\n",
        "            robustness analysis. If None, this check is skipped.\n",
        "        n_jobs (int): The number of CPU cores to use for parallelizable tasks.\n",
        "                      -1 means using all available cores.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary where keys are the names of the\n",
        "                                 robustness checks and values are the\n",
        "                                 corresponding result DataFrames.\n",
        "    \"\"\"\n",
        "    # Set up a comprehensive logger for the entire robustness suite.\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - [Robustness Orchestrator] - %(message)s'\n",
        "    )\n",
        "\n",
        "    # Initialize a dictionary to store all robustness results.\n",
        "    robustness_results = {}\n",
        "\n",
        "    logging.info(\"===== STARTING FULL ROBUSTNESS ANALYSIS SUITE =====\")\n",
        "\n",
        "    # --- Task 11.1: Parameter Sensitivity Analysis ---\n",
        "    # This check is run only if a parameter grid is provided.\n",
        "    if parameter_grid:\n",
        "        try:\n",
        "            # Execute the parameter sensitivity analysis.\n",
        "            sensitivity_results_df = run_parameter_sensitivity_analysis(\n",
        "                parameter_grid=parameter_grid,\n",
        "                base_manifest=base_manifest,\n",
        "                transactions_log_frame=transactions_log_frame,\n",
        "                firm_metadata_frame=firm_metadata_frame,\n",
        "                comtrade_exports_frame=comtrade_exports_frame,\n",
        "                country_data_frame=country_data_frame,\n",
        "                supply_chains_definitions_dict=supply_chains_definitions_dict,\n",
        "                external_networks=external_networks,\n",
        "                n_jobs=n_jobs\n",
        "            )\n",
        "            # Store the resulting DataFrame.\n",
        "            robustness_results['parameter_sensitivity'] = sensitivity_results_df\n",
        "        except Exception as e:\n",
        "            # Log a failure in this specific check but do not halt the entire suite.\n",
        "            logging.error(f\"Task 11.1 (Parameter Sensitivity) failed: {e}\", exc_info=True)\n",
        "            robustness_results['parameter_sensitivity'] = pd.DataFrame({'error': [str(e)]})\n",
        "\n",
        "    # --- Task 11.2: Temporal Window Robustness Testing ---\n",
        "    try:\n",
        "        # Execute the temporal robustness analysis.\n",
        "        temporal_results_df = run_temporal_robustness_analysis(\n",
        "            base_manifest=base_manifest,\n",
        "            transactions_log_frame=transactions_log_frame,\n",
        "            firm_metadata_frame=firm_metadata_frame,\n",
        "            comtrade_exports_frame=comtrade_exports_frame,\n",
        "            country_data_frame=country_data_frame,\n",
        "            supply_chains_definitions_dict=supply_chains_definitions_dict,\n",
        "            external_networks=external_networks\n",
        "        )\n",
        "        # Store the resulting comparative DataFrame.\n",
        "        robustness_results['temporal_robustness'] = temporal_results_df\n",
        "    except Exception as e:\n",
        "        # Log a failure in this specific check.\n",
        "        logging.error(f\"Task 11.2 (Temporal Robustness) failed: {e}\", exc_info=True)\n",
        "        robustness_results['temporal_robustness'] = pd.DataFrame({'error': [str(e)]})\n",
        "\n",
        "    # --- Task 11.3: Alternative Network Construction Robustness ---\n",
        "    # This check is run only if a list of alternative methods is provided.\n",
        "    if methods_to_test:\n",
        "        try:\n",
        "            # Execute the construction robustness analysis.\n",
        "            construction_results_df = run_construction_robustness_analysis_revised(\n",
        "                methods_to_test=methods_to_test,\n",
        "                base_manifest=base_manifest,\n",
        "                transactions_log_frame=transactions_log_frame,\n",
        "                firm_metadata_frame=firm_metadata_frame,\n",
        "                comtrade_exports_frame=comtrade_exports_frame,\n",
        "                country_data_frame=country_data_frame,\n",
        "                supply_chains_definitions_dict=supply_chains_definitions_dict,\n",
        "                external_networks=external_networks\n",
        "            )\n",
        "            # Store the resulting comparative DataFrame.\n",
        "            robustness_results['construction_robustness'] = construction_results_df\n",
        "        except Exception as e:\n",
        "            # Log a failure in this specific check.\n",
        "            logging.error(f\"Task 11.3 (Construction Robustness) failed: {e}\", exc_info=True)\n",
        "            robustness_results['construction_robustness'] = pd.DataFrame({'error': [str(e)]})\n",
        "\n",
        "    # Log the completion of the entire suite.\n",
        "    logging.info(\"===== FULL ROBUSTNESS ANALYSIS SUITE COMPLETED =====\")\n",
        "\n",
        "    # Return the final, aggregated robustness results.\n",
        "    return robustness_results\n"
      ],
      "metadata": {
        "id": "0QkjhZC3Wu_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Master Orchestrator Callable\n",
        "\n",
        "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
        "# Fused High-Fidelity Constellation of Callables for Replicating the Study:\n",
        "# \"Deciphering the global production network from cross-border firm transactions\"\n",
        "#\n",
        "# This script provides a complete, end-to-end implementation of the research pipeline from the paper.\n",
        "# It is designed with modularity, rigor, and performance as primary objectives.\n",
        "#\n",
        "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\n",
        "\n",
        "\n",
        "def main_analysis_orchestrator(\n",
        "    # All raw data inputs\n",
        "    transactions_log_frame: pd.DataFrame,\n",
        "    firm_metadata_frame: pd.DataFrame,\n",
        "    comtrade_exports_frame: pd.DataFrame,\n",
        "    country_data_frame: pd.DataFrame,\n",
        "    supply_chains_definitions_dict: Dict[str, Any],\n",
        "    # Configuration inputs\n",
        "    base_manifest: Dict[str, Any],\n",
        "    # Optional inputs for robustness checks\n",
        "    run_robustness_checks: bool = True,\n",
        "    parameter_grid: Optional[Dict[str, List[Any]]] = None,\n",
        "    methods_to_test: Optional[List[Any]] = None,\n",
        "    external_networks: Optional[Dict[str, Any]] = None,\n",
        "    n_jobs: int = -1\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Serves as the top-level entry point for the entire research project,\n",
        "    orchestrating both the baseline replication and the full suite of\n",
        "    robustness analyses.\n",
        "\n",
        "    This function provides a single, powerful interface to a complex and\n",
        "    rigorous research pipeline. It first executes the end-to-end replication\n",
        "    of the study's main findings. Then, if enabled, it proceeds to run a\n",
        "    comprehensive set of robustness checks to assess the stability of those\n",
        "    findings with respect to key methodological choices.\n",
        "\n",
        "    Args:\n",
        "        transactions_log_frame (pd.DataFrame): Raw log of firm transactions.\n",
        "        firm_metadata_frame (pd.DataFrame): Raw firm ownership and location data.\n",
        "        comtrade_exports_frame (pd.DataFrame): Raw country-product export data.\n",
        "        country_data_frame (pd.DataFrame): Raw country population data.\n",
        "        supply_chains_definitions_dict (Dict[str, Any]): Definitions of\n",
        "            manual supply chains for validation.\n",
        "        base_manifest (Dict[str, Any]): The base replication manifest with\n",
        "                                        default parameter settings.\n",
        "        run_robustness_checks (bool): A flag to enable or disable the entire\n",
        "                                      robustness analysis suite. Defaults to True.\n",
        "        parameter_grid (Optional[Dict[str, List[Any]]]): Configuration for the\n",
        "            parameter sensitivity analysis. Required if robustness checks are run.\n",
        "        methods_to_test (Optional[List[Any]]): Configuration for the construction\n",
        "            robustness analysis. Required if robustness checks are run.\n",
        "        external_networks (Optional[Dict[str, Any]]): Loaded external networks\n",
        "            for comparison. Defaults to None.\n",
        "        n_jobs (int): The number of CPU cores to use for parallelizable tasks.\n",
        "                      -1 means using all available cores.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A master dictionary containing two top-level keys:\n",
        "                        'baseline_results': The full output of the main study\n",
        "                                            replication.\n",
        "                        'robustness_results': A dictionary with the results of\n",
        "                                              each completed robustness check.\n",
        "    \"\"\"\n",
        "    # --- Initialization ---\n",
        "    # Set up a global logger for the entire session.\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - [Main Orchestrator] - %(message)s',\n",
        "        force=True # Force re-configuration if logger is already set\n",
        "    )\n",
        "\n",
        "    # Initialize the master dictionary to store all results.\n",
        "    master_results = {\n",
        "        'baseline_results': None,\n",
        "        'robustness_results': None\n",
        "    }\n",
        "\n",
        "    logging.info(\"||||| COMMENCING FULL ANALYSIS AND ROBUSTNESS PIPELINE |||||\")\n",
        "\n",
        "    # --- Baseline Replication Run ---\n",
        "    try:\n",
        "        logging.info(\"\\n\" + \"=\"*80 + \"\\n\" + \"||\" + \" \"*25 + \"EXECUTING BASELINE REPLICATION\" + \" \"*25 + \"||\\n\" + \"=\"*80)\n",
        "\n",
        "        # Execute the primary end-to-end pipeline.\n",
        "        baseline_pipeline_results = run_end_to_end_pipeline(\n",
        "            transactions_log_frame=transactions_log_frame,\n",
        "            firm_metadata_frame=firm_metadata_frame,\n",
        "            comtrade_exports_frame=comtrade_exports_frame,\n",
        "            country_data_frame=country_data_frame,\n",
        "            supply_chains_definitions_dict=supply_chains_definitions_dict,\n",
        "            replication_manifest=base_manifest,\n",
        "            external_networks=external_networks\n",
        "        )\n",
        "        # Store the complete set of results from the baseline run.\n",
        "        master_results['baseline_results'] = baseline_pipeline_results\n",
        "\n",
        "        logging.info(\"||\" + \" \"*24 + \"BASELINE REPLICATION COMPLETED SUCCESSFULLY\" + \" \"*23 + \"||\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # If the baseline run fails, it's a critical error. Log and terminate.\n",
        "        logging.critical(f\"The baseline pipeline run failed critically: {e}\", exc_info=True)\n",
        "        logging.critical(\"Cannot proceed to robustness checks. Terminating analysis.\")\n",
        "        # Return the partial results dictionary with the error.\n",
        "        master_results['error'] = f\"Baseline pipeline failed: {e}\"\n",
        "        return master_results\n",
        "\n",
        "    # --- Full Robustness Analysis Suite ---\n",
        "    # Proceed only if the baseline was successful and checks are enabled.\n",
        "    if run_robustness_checks:\n",
        "        try:\n",
        "            logging.info(\"\\n\" + \"=\"*80 + \"\\n\" + \"||\" + \" \"*24 + \"EXECUTING ROBUSTNESS ANALYSIS SUITE\" + \" \"*23 + \"||\\n\" + \"=\"*80)\n",
        "\n",
        "            # Execute the master orchestrator for all robustness checks.\n",
        "            robustness_suite_results = run_full_robustness_analysis(\n",
        "                transactions_log_frame=transactions_log_frame,\n",
        "                firm_metadata_frame=firm_metadata_frame,\n",
        "                comtrade_exports_frame=comtrade_exports_frame,\n",
        "                country_data_frame=country_data_frame,\n",
        "                supply_chains_definitions_dict=supply_chains_definitions_dict,\n",
        "                base_manifest=base_manifest,\n",
        "                external_networks=external_networks,\n",
        "                parameter_grid=parameter_grid,\n",
        "                methods_to_test=methods_to_test,\n",
        "                n_jobs=n_jobs\n",
        "            )\n",
        "            # Store the complete set of results from the robustness suite.\n",
        "            master_results['robustness_results'] = robustness_suite_results\n",
        "\n",
        "            logging.info(\"||\" + \" \"*23 + \"ROBUSTNESS ANALYSIS SUITE COMPLETED SUCCESSFULLY\" + \" \"*22 + \"||\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # If the robustness suite fails, log the error but do not crash.\n",
        "            # The baseline results are still valuable.\n",
        "            logging.error(f\"The robustness analysis suite encountered an error: {e}\", exc_info=True)\n",
        "            master_results['robustness_results'] = {'error': f\"Robustness suite failed: {e}\"}\n",
        "    else:\n",
        "        # Log if robustness checks were intentionally skipped.\n",
        "        logging.info(\"Robustness checks were disabled. Skipping Task 11.\")\n",
        "\n",
        "    # Log the successful completion of the entire analysis.\n",
        "    logging.info(\"\\n||||| FULL ANALYSIS AND ROBUSTNESS PIPELINE CONCLUDED |||||\")\n",
        "\n",
        "    # Return the final, comprehensive results.\n",
        "    return master_results\n"
      ],
      "metadata": {
        "id": "2wfqF1y-krET"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}